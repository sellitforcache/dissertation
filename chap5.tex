\chapter{Conclusions and Future Work}
\label{chap:conclusions}

The ultimate purpose for developing WARP was to accelerate accurate continuous energy neutron transport simulations in general 3D geometries by using GPUs.  By that metric it has been successful in its mission.  It can produce results within fractions of a percent compared to qualified and benchmarked production codes like MCNP and Serpent.  It can produce these results 11-80 faster, depending on problem parameters and hardware.  Along the way, useful information regarding the importance of thread divergence, neutron termination, geometric acceleration structures, and dataset size has been collected and analyzed.

WARP's secondary goals of using standard nuclear data files, running in both fixed source and criticality source modes, and producing neutron spectra have also been achieved.  Despite these milestones, WARP can only handle a single tally volume, fixed source mode runs only marginally faster than Serpent and MCNP and produces spectra with inaccurate source values, and loading data from many isotopes has not been tested.  It also does not have routines to handle thermal scattering data or unresolved resonance tables. There is still much work to be done in testing, stabilizing WARP's execution, adding physics and features, as well as continuing to optimize algorithms for efficient GPU execution.

This initial development phase of WARP will hopefully be the first step in creating a full featured reactor simulation program that runs on GPUs.  Such an effort was needed to ensure the nuclear engineering community's codebase keeps up with modern programming techniques and hardware.  WARP is by no means a mature code ready to be used in everyday nuclear reactor calculations, but rather it is a good starting point for more advanced development.  

Good for multiplication factor parameters searches, like perturbing densities/temperatures, not good for depletion since can only do a single tally at the moment.


\section{Conclusions}

In developing WARP, a few important conclusions can be made.  The first, and arguably most important, is that running with large datasets and therefore a large number of threads is important for good performance.  It was seen that in some cases moving from $10^5$ to $10^6$ neutrons per batch in criticality mode increase performance by a factor of four or more.  It is important to keep the GPU saturated with threads to it can effectively pipeline data loads.  Criticality is a global quantity and global algorithms are used to calculate the multiplication factor of systems.  WARP uses global operations to act on a large dataset of neutron histories, and there is no state continuity between kernel launches.  Therefore data must be stored in and loaded from global memory.  This makes it  important to launch as many threads as possible so the GPU can hide the large data access latencies though pipelining, especially since data access is random and uncoalesced, exacerbating the need for pipelining.  In addition to being able to hide latencies better with more active threads, using large datasets also pays for kernel launch overhead by making the computational payload large and therefore the factional cost of launching a kernel small.  This also why it is important to have most of the data loads requested immediately at the beginning of the kernel; the compute rate mainly depends on the rate at which data is fetched, so fetching it as soon as possible will provide the best performance.

The second conclusion is that remapping threads to active data is an effective way of raising the processing rate when the number of active neutrons becomes small as well as reducing thread divergence in reaction kernels.  Using a radix sort to do this is effective since it segregates reactions into contiguous blocks, efficient since it can be done in place and in $O(kN)$ time, and can eliminate completed data from being accessed if slight modifications to the standard reaction number encodings are made.  Most of the performance gain in remapping data references comes from being able to launch grids that are sized for only the active data rather than the entire dataset for both global and reaction kernels.  A non-remapping algorithm does not keep track of where active data is, and therefore must launch a grid that covers the entire dataset.  When the number of active neutrons drops below about 30\% of the initial number, the overhead and memory bandwidth cost of launching these extra threads, which only load a ``done'' bit and return, is more than the cost of performing the radix sort and edge detection.  The majority of the transport iterations occur  while there are less than 30\% of the initial neutrons left, and remapping references is usually very worthwhile.

Using the NVIDIA OptiX ray tracing framework was also shown to be an effectively way to handle the geometry representation in WARP.  OptiX is flexible, and allows attachment of material and cell number to individual geometric primitives, can do perform surface detection with a randomly distributed and directed dataset, can incorporate the remapping vector created by a radix sort, and can do so fast enough to be used in WARP.  The acceleration structures that OptiX can automatically build over the scene geometry was the initial reason for using it, and it was determined that the BVH builder and traverser provided the best performance as well as using mesh primitive instancing rather than a transform node approach.  The number of objects present in the scenes is small compared to some rending scenes, and the SBVH acceleration structure actually had worse performance, presumably due to some addition overhead in traversing it which is not offset when few numbers (less than a few hundred thousand) of objects are present in the scene.  Primitive instance provides better performance since using transform nodes requires traversing a deeper geometry tree which also has more (redundant) data associated to it.

\section{Future Work}

As this is the very first 

- replace geom, parallel routines with handwritten, specific ones, if geom could fit in shared memory, there would be great performance increase

- on the fly temperature treatments

- automatical scnece episonl placement for coinceident surfaces

- reduce the memory needed per history so a maximum number can be launched, this was no optimized at all in the initial development

- use a SM-based algorithm instead of global.  history info could be stored in shared, but would need to rendezvous like CPU codes.

- newer libraries, CUB, optix prime, rayforce?

- legendre instead of tabular angular?

- better geom performance with woodcock maybe

- material processing schemes , xsbench \cite{openmc}

- dynamic parallelism can be implemented and kernel launch overhead minimized.  

- change fixed source pop to be more like criticality, pop into next generation instead of this generation?

-----  might be the same as using a stack-pop and task based with syncthreads?  The only routines used in the transport loop are the geometry and the sort.   pop would replace the sort?  and pass off reactions if total coherence is wanted.  still need to communicate the fissions?  this could be eliminated by a query-able analytic representation of a converged fission source.  FFT, something else.  Might not be fast as simply using the previous cycle points but would scale well?  Previous points could be used for refinement...  might be a neat consideration?

Releasing it as open source software is in progress, and is pending University approval.  Releasing the source openly has benefits of providing potential users with a convenient and transparent way of obtaining the software, as well as allowing for valuable contributions from the community.