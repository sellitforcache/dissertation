\chapter{GPU Implementations}
\label{chap:imp}

In this chapter all GPU implementations will be explained in detail. 
  First, the exploratory studies done in preparation for developing WARP will be discussed.  These studies show the algorithmic benefits of a very important feature of WARP - remapping the data references.  This chapter also covers how OptiX execution is optimized for best performance in reactor-like geometries.  After the preliminary studies, the data layout for cross sections is explained and its similarities and differences from Serpent pointed out.  The last topic discussed is the CUDA kernels written for WARP.  These routines process the neutrons as they travel through the problem geometry and provide the ``glue'' to connect all the important tasks that WARP requires to processing the neutron data.

\section{Preliminary Studies}
\label{sec:prelim}

Before any serious coding efforts were undertaken, a pair of smaller preliminary studies were done to determine the feasibility of doing Monte Carlo neutron transport on GPUs.  The first study was attempting to perform a simple, 2D, mono-energetic scattering game on a GPU.  The second study was a test of the OptiX framework to see if it could handle randomized ray tracing while maintaining acceptable performance. In both of the following studies and in WARP itself, CUDA 5.0 and OptiX 3.0.1 were used.

\subsection{2D Scattering Game}

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/prelim_geom.eps}
     \caption{The 2D geometry of the scattering test. \label{prelim_geom} }
\end{figure}

The goal of the 2D, mono-energetic scattering game was to determine whether a sorted or an unsorted event-based algorithm was best for controlling thread divergence on a GPU, to the see the relative performance of the history- and event-based GPU implementations to an identical serial CPU version, and to determine if the performance of the CUDPP (CUDA Parallel Primitives) library is adequate for use in WARP.   A set of three GPU implementations were written using CUDA C, and a single CPU implementation was written using C.  

In the simulation, only two reaction types are possible - scattering and absorption.  Scattering is treated isotropically and, since it is mono-energetic, scattering only serves to change the direction of a particle.  The geometry of the game is kept simple in order to highlight how the reaction divergence is handled rather than how geometry routines effect performance.   The geometry used in the game is shown in Figure \ref{prelim_geom}.  There are five different cells, all of which are square, and particles can only move in the $x$-$y$ plane.  Each cell has different reaction cross sections, and cell 0 extends to infinity but has a nonzero absorption cross section so particles cannot scatter forever.  Particles are initialized with a uniform, random distribution in cell 1, which has no absorption cross section so that particles must cross a cell boundary at least once.  Since the cells are few and square, the ``where am I?'' operation that determines the cell number (and therefore the cross sections) each particle is in can be done with simple, hard-coded logic comparisons.

As particles scatter, they start to be absorbed and their histories are terminated.  This means they are no longer transported, and their data should no longer be accessed.  Here is where the GPU implementations start to differ from one another.  The task-based implementation performs the transport from a one-particle-per-thread standpoint.  When the transport kernel is launched, each thread contains a while-loop that transports a particle until it terminates and the thread returns.  If more histories are requested than the maximum thread number, transport is performed in batches.  The diagram of the simple task-based algorithm used is shown in Figure \ref{prelim_alg_task}.  The CPU version of the code also uses this algorithm.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.7\textwidth]{graphics/prelim_alg_task.eps}
     \caption{The task-based algorithm used in the 2D scattering study. \label{prelim_alg_task} }
\end{figure}

An event-based algorithm performs the same operations on the data as the task-based algorithm, but the operations are instead carried out over all the particle data simultaneously in parallel.  Now, each kernel does \emph{not} contain a while loop.  The while loop is on the CPU host, the drawbacks of which are discussed later.  Each kernel performs a single, simple task on the entire particle dataset in one step.  Figure \ref{prelim_batch} shows an illustration of how the nearest surface distance and interaction distance comparison is done in a data-parallel way.  
The leftmost diagram in the figure shows a box containing blue dots, which represent the positions of a set of neutrons.  These positions are all loaded from memory in a single step.  The next diagram shows the directions are then loaded in a single step.  The third diagram shows the sampled interaction distances for each of the neutrons, represented by green dots.  
%
\begin{figure}[h!] 
  \centering
    \includegraphics[width=\textwidth]{graphics/prelim_batch.pdf}
     \caption{Ray tracing done in a data-parallel way. \label{prelim_batch} }
\end{figure}
%
After the interaction distance is know, the distance to the nearest surface, shown as red dots, is calculated for all the neutrons.  Once the surface distance is known, the next step would be to update the neutron position to the smaller of interaction distance or the surface distance, at which point the process repeats.  Each step of a data-parallel algorithm acts this way; it performs identical operations on the entire set of neutrons.
% First give a concrete example of what this means in transport - e.g. the distance to interaction is sampled for all threads at the same time, then...  good I hope?

  An analogy can be made with homework grading.  A task-based algorithm is like grading a single student's homework in its entirety, then moving on to the next student's.  An event-based approach is like grading a single \emph{problem} for \emph{every student}, then moving on to the next problem.  This makes the transport loop \emph{data parallel}, which GPUs need to keep warps coherent.  Figure \ref{prelim_alg_event} shows the event-based transport loop used in the scattering game.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.7\textwidth]{graphics/prelim_alg_event.eps}
     \caption{The event-based algorithm used in the 2D scattering study. \label{prelim_alg_event} }
\end{figure}% you don't refer to this figure...  no referenced

Two different implementations were made using an event-based algorithm, one that uses the CUDPP compaction to remap threads to non-terminated (active) data, and one that does not.  In the non-remapping version, threads simply return if they access data belonging to a terminated particle.  In the remapping version, the number of threads launched is equal to the number of active particles left in the dataset, not the size of the dataset itself.  The threads access a remapping vector, which transforms their thread ID to be one that still contains active data.  Figure \ref{remapping} shows how the remap vector transforms the initial thread ID to an active ID.  

The CUDPP compaction function does just this - it takes an input vector and a valid flag vector and returns a new vector containing only the valid elements, preserving the order they are in.  For this test, the input vector is the thread ID vector (tid[i]=i), and the flag vector contains a 1 if a particle is unterminated and a 0 if it is not.  The compact function then returns a remapping vector, which is as long as the number of unterminated particles and contains the indices of the unterminated data.  Even though the neutron data access will not be coalesced, accessing the remap vector should be coalesced (adjacent threads access adjacent data) and should therefore be fast to load.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/remapping_horiz.eps}
     \caption{Mapping thread IDs to active data through a remapping vector \label{remapping} }
\end{figure}

% I think it makes more sense to talk about this up where you're talking about the methods you implemented...
%The studies done by Liu, Henderson, and Xu all employ a particle-popping algorithm to do...  %at some point, which is not considered in this study 
% in past work you don't discuss work by Yu or Xu, unless Yu is the Chinese university work, but you don't mention the name. 
% Come to think of it, you should also add a discussion about the work of Xu.  ---->  Xu is the chinese (not RPI) and I added this detail to chap2, Yu is a mistake (should be Liu from RPI)  these have been fixed now.
%This algorithm is task-based, but each thread holds a bank of particle data so when its initial data terminates it can immediately start transporting a new particle \cite{tianyu, henderson, qixu}.  This is a clever way of doing things and \emph{might} be the faster overall. This particle-popping algorithm is not, however, investigated in this work because GPU-specific library use and task-based algorithms are incompatible. 
%
%%%%  seems like more headache than its worth mentioning.
%

NVIDIA gives each GPU card a ``compute capability'' number which summarizes the features available on the card.  For example, dynamic parallelism is a feature only supported on cards of compute capability 3.5 and above.  When this preliminary test was first written, Fermi cards were the newest available, and the cards used were of compute capability 2.0.  For these cards, there is a maximum grid dimension size of 65,536 blocks \cite{cuda}, and this limit was hardcoded into the applications.  In other words, if the total number of particles was greater than what could be transported with 65,536 blocks, the particles were divided into separate batches.  This limit was lifted in devices with compute capability of 3.0 and up (which includes the K20), where the maximum 1D grid size was increased to ($10^{27}-1$) \cite{cuda}. The hardcoded block limit was used to keep compatibility with Fermi cards of compute capability 2.0. % I'd clarify the above paragraph and then add this comment here somewhere since you're already talking about 2.0/3.0   DONE

Libraries currently available for the GPU carry out a single task on a dataset; they are not coded for single-thread operation and must be launched as their own kernels.  Having threads in different states wouldn't allow the library routines to be dropped in and used consistently across all the active particle data.  Further, much of the performance from optimized algorithms in the libraries comes from thread blocks cooperating, and using a task-based algorithm would be like treating each SM as a separate CPU (using a stack-popping method).  The CUB library can perform sorts and scans in a per-block or per-warp fashion, but OptiX would be incompatible with a non-global transport scheme \cite{optix}.

Figure \ref{prelim_speedup_01} shows the speedup factors, $F_s=t_\mathrm{CPU}/t_\mathrm{GPU}$, of the GPU implementations over the CPU implementation versus the number of particles run.  This benchmark was run on a server with an 8-core AMD Opteron 6128 CPU clocked at 2.0GHz and a Tesla K20 card.  The task-parallel implementation always performs better than the event-based with the batched approach. The event-parallel with remapping implementation starts to overtake the batched version at around 20,000 particles and overtakes the task version around 100,000 particles.  No more that $10^8$ histories could be run because of memory constraints of the event-based implementations.  At this point, the speedups of the task and batched implementations appear to be saturated around 5.9x and 4x, respectively.  The remapping implementation does not seem to be saturated, but reaches a maximum speedup of 13x over the serial CPU implementation at the limit of this study.  Why there is a noticeable jump in the remapping speedup at $10^7$ particles instead of a smooth transition is as of yet unexplained.
%what happens at 10^7 particles that makes the behavior jump like that in the remap version?  I have no idea, and I'll say so.

The batched approach sees a visible performance decrease at around $10^7$ particles, the point where transport has to be broken into two batches ($128\times65,536=8,388,608$ threads in a single kernel launch possible).  The finite address space affects the batched implementation with 512 threads per block at larger dataset sizes, specifically %? later when?  ok rephrased
 at $512\times65,536=33,554,432$ particles. 
 A similar test was done with a Tesla C2075 (Fermi) card where this performance hit is more accentuated.  The results of the test and speculation as to why it exhibits different behavior compared to the K20 is %how/why?  it is in the appendix!
 is shown in Appendix \ref{app:A} .  
 The performance degradation of the batched implementation of the K20 is only slight at $10^7$ particles.   The task-based implementation also needs to break transport into two batches at the same boundaries as the batched implementation, but is seemingly unaffected by it.  The effect is likely unnoticeable since it only implies a second kernel launch in the task-based implementation instead of the hundreds more in the batched.

It should be noted that compiling the tests with compute capacity 2.0 (-arch sm\_20 compiler flag) improved performance about 20\% over compiling with compute capacity 3.0. 

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/prelim_speedup_001_k20.eps}
     \caption{Speedup factors of the GPU implementations over the CPU implementation on a Tesla K20. \label{prelim_speedup_01} }
\end{figure}

The most likely reasons that the task-based implementation outperforms the event-based for small particles datasets is because of the simplicity of the problem and the communication and setup overhead associated with many kernels being launched from the host.  The task-based implementation only launches a single kernel that houses the transportation loop whereas the event-based implementations must launch kernels for every interaction \emph{within} the transport loop.  The event-based method therefore launches hundreds of kernels in every loop compared to the single kernel of the task-based implementation.  

In addition, having only two reactions means threads only diverge when they terminate.  WARP will use real data and have many reactions types to deal with, so divergence will be a greater problem.  Only having two reaction channels also allows threads in a block to almost always be in the same step of the transport algorithm.  The only real problem in this study is the idle threads left in the block after they terminate.  

The batched implementation is really just an intermediate case between the task-based and remapping implementations.  It has all the drawbacks of the event-based algorithm, namely high kernel launch costs, but none of the benefits of breaking the transport up into coherent steps since any threads are left idle in blocks once their particle terminates.  

The remapping implementation shows the real benefits of an event-based algorithm in that a sort can be inserted into the transport algorithm to convert a ``particle per thread'' into ``X particles shared by Y threads.''  The remapping sweeps out references to terminated particles and allows the blocks to remain full of active data.  This eliminates the cost of processing blocks with stale data, i.e.\ makes each block processed worth more on average.   

The speedup curve highlights an important feature - that speedup plateaus at a large number of histories.  This is because of the GPU's ability to hide memory latencies through pipelining when there are many many active threads.  It also may be related to the kernel launch overhead in that running large datasets spreads the overhead cost over many particles, reducing its cost per particle.  Other than effecting the maximum number of threads resident on the card, the number of threads per block seems to make little difference between the implementations. Having fewer threads per block slightly outperforms having more for all cases.  The reason for this could be the registers spilling to local memory (which is more time consuming) since there are more threads resident in a multiprocessor and more variables need to be stored in the registers.  This is speculation, profiler statistics about register spilling were not gathered.  % Do you have evidence of this, or is it speculation? Just curious

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/prelim_speedup_01_k20.eps}
     \caption{Speedup factors of the GPU implementations over the CPU implementation on a Tesla K20 with $\Sigma_a$ increased to 0.1 in cell 0. \label{prelim_speedup_1} }
\end{figure}

Figure \ref{prelim_speedup_1} shows the same test, but with the absorption cross section in cell 0 increased from 0.01 to 0.1.  This means the potential difference in the number of scatters a particle undergoes before being absorbed is greatly reduced and, therefore, impact of thread divergence is also reduced.  Again, no more that $10^8$ histories could be run because of memory constraints of the event-based implementations.  At saturation, the speedups of the task and batched implementations appear to be around 6.2x and 4.5x, respectively.  The remapping implementation does not seem to be saturated, but reaches a maximum speedup of 14x over the serial CPU implementation.  Again, there is a noticeable jump in the remapping speedup at $10^7$ particles; the cause of which is not known. %again with the jump at 10^7.  don't know
 The curves have very similar trends as in the case with $\Sigma_a=0.01$ in cell 0, but saturate at slightly higher values, most likely due to the fact that divergence is intrinsically decreased by the material parameters.

Figure \ref{prelim_active} shows the number of active histories per transport iteration for the batches and remapping implementations.  Since the remapping implementation eliminates all references to terminated particles, the data loaded into the threads blocks is all active and no threads return without doing work.  The figure clearly shows that the blocks are kept full until the end of the simulation when the remaining number of particles becomes less than the maximum number of threads.  It also shows how quickly particles are depleted from the blocks.  

In this simple case, by far most interactions happen in cell 0, and particles are absorbed almost identically to \eqref{depleted}, where $i$ is the iteration number and $\Sigma_a/\Sigma_t$ is the absorption probability in cell 0.  This is expected behavior. % You might want to explicitly state this is consistent with expectation.  yep, done.
  Figure \ref{prelim_active} does not reflect the time taken by each iteration, however.  The iterations with more particles take longer than those with fewer for both implementations.  More particles are transported per time in full iterations, however, as evidenced by the greater speedups of the remapping algorithm in Figures \ref{prelim_speedup_1} and \ref{prelim_speedup_01}.   The cycle time of a completely full iteration is approximately 0.2 seconds, whereas cycles times of iterations contain less than 5000 particles are around 0.01 seconds.  The corresponding processing rates are $4.2\times10^7$ particles per second for full iterations and $5\times10^6$ particle per second for small iterations, however. % Can you provide some sense of magnitude for how similar or different these are?  sure.

\begin{equation}
\label{depleted}
\frac{N}{N_0}=\exp \left(-\frac{\Sigma_a}{\Sigma_t} i \right)
\end{equation}

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/prelim_active.eps}
     \caption{The number of active threads for the event-based GPU implementations. \label{prelim_active} }
\end{figure}

Actively remapping the data also reduces control flow divergence, as shown by Figure \ref{prelim_divergence}, the effect of which  only manifests itself in this study as keeping threads full of active data.  Despite this, remapping will allow the blocks in WARP to remain coherent (undergoing identical interactions) as well as full, which is another benefit of remapping.  The figure shows the amount of divergence in a kernel launch, so the task-based algorithm only reports an average value since it only launches a single kernel.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/prelim_divergence.eps}
     \caption{The percent of control flow divergence in blocks of the task- and event-based GPU implementations. \label{prelim_divergence} }
\end{figure}

From this preliminary, 2D, mono-energetic scattering study it can be concluded that using an event-based algorithm with a compaction/sort algorithm to eliminate terminated particles from being accessed by thread blocks drastically reduces control flow divergence and keeps warps coherent.  There is a cost for adopting this algorithm, however, namely the overhead of kernel launches.  How these factors compete in WARP will be shown in the results in Chapter \ref{chap:results}.  WARP will adopt the event-based algorithm in hopes that thread coherency and its benefits (maximal load coalescing and little warp serialization) will outweigh launch overhead when real data is used, as well as to allow library usage in complicated parallel tasks.

\subsection{Ray Tracing with OptiX}

Another preliminary study was preformed to investigate how OptiX performs when ray tracing is done from randomized points and to find the optimal OptiX configuration for WARP.  In rendering, rays are initialized in a uniform array, but OptiX also allows for arbitrary starting points and directions to be used.  This flexibility comes from being able to write custom ray-generation programs in OptiX.  OptiX processes rays concurrently, so the ray tracing in WARP will be done in a batched way, i.e.\ the traces will be done for all neutrons in a single step.

There are two types of scaling in OptiX that need to be characterized in order to ensure optimal performance -- scaling with regard to the number of concurrent rays traced and scaling with regard to the number of geometrical objects in the scene.  Determining how OptiX scales with the number of concurrent rays is important in knowing how large neutron batches need to be in WARP for the geometry routines to execute efficiently.  Nuclear reactor simulations can contain thousands of material zones, and knowing how OptiX scales with the number of geometrical objects is important for choosing a configuration that will allow WARP to perform well neutron transport in complex geometries.  

The first scaling study focuses on scaling with regard to the number of concurrent rays. Three different geometries were used in the test -- an assembly-like hexagonal array of cylinders, an assembly of the same size but with two interleaved arrays, and a much larger version of the assembly-like geometry.  The assembly-like configuration has 631 instances of a single primitive type, the interleaved array has 381 instances of three different primitives types, and the large array consists of 1801 instances of a single primitive type.  These cases were chosen to determine if there are any large differences in the scaling when the number and types of objects are changed.  Each array resides in a large hexagonal prism, which is in turn nested in a large cube that defines the outer limit of the scene.

The second scaling study focuses on how OptiX scales with regard to the number of geometrical objects in the scene using two different primitive instancing methods.  Instancing refers to how individual geometric objects are created in the OptiX scene.  The geometry used in this study is a hexagonal lattice of cylinders identical to the assembly-like geometry of the first scaling study.  The volume, pitch to diameter ratio, and aspect ratios of the lattice is kept constant, but the cylinder radii are halved while doubling the number of elements on an edge.  The smallest scene only has a single primitive in it, and the most complex has 42,843 primitives in it.  

\subsubsection{Point-in-Polygon / Material Query}

\begin{figure}[h!] 
  \centering
    \includegraphics[width=1.0\textwidth]{graphics/whereami.eps}
     \caption{The point-in-polygon-like algorithm for determining the entering cell number by using ray tracing \label{whereami} }
\end{figure}

An important job for the geometry routine in a Monte Carlo neutron transport code is to determine the cell and material IDs of a particle based only on its coordinates.  In Woodcock tracking surface intersections are not calculated and the material query is the only place where geometric information enters into the simulation. In WARP and other ray-tracing codes, material information is only updated when a sampled interaction distance is greater than the near surface distance.  The neutron is then placed on the boundary, the material information is updated for the material the neutron will enter, and the interaction distance is sampled again using the same direction of flight as before.  WARP will use an algorithm to determine the entering cell number by using ray tracing, since all the geometric information is stored in the OptiX context.  This also means the material query will be able to take advantage of the OptiX acceleration structures and should scale well (logarithmic).

The material query algorithm is shown in Figure \ref{whereami}.  An ordered list of surface intersections is generated by iteratively ray tracing and adding the closest surface number to the hit list.  Tracing is terminated when a predefined outer cell that contains all other cells is intersected.  Since all surfaces are closed, the ray will intersect any cell surface twice that it isn't nested in.  When the list is made, the double entries are removed, which yields a list of cells the neutron is nested in.  The first entry will be its current cell and the second entry will be the cell it is entering into.  

An issue with ray tracing is that mathematical cell descriptions are exact, but the numbers representing them are not. If these numbers are treated as exact, a situation can occur where a neutron is placed at a boundary but is actually slightly behind the boundary because of floating-point roundoff.  When the next trace is started, the ray intersects the boundary it has already intersected with instead of tracing into the next cell.  This situation can be prevented by using the scene epsilon, which determines the minimum intersection distance possible, i.e.\ the distance away from the source point at which intersections are allowed to occur.  Giving OptiX a scene epsilon helps to guarantee that a trace starts after the very near boundary, and accurate results are calculated.  It is important to make the scene epsilon an appropriate value for the geometry if this algorithm is to be used effectively. 

Another problem that can occur that is due to the scene epsilon is an inaccurate material query.  This can occur when intersecting the very corner of a box, for example. If the thickness of the object is less than the scene epsilon, the material query algorithm may only count a single intersection of the distant object and think the material into which a particle may be traveling is the distant box's material instead of the correct material.  In many cases this can be avoided by performing the surface intersection in the neutron direction and then performing the material query in the $z$-direction only.  Currently, the objects in WARP are all various kinds of $z$-aligned prisms (or spheres) and corner cases like this will not happen if the material query is done in the $z$-direction only because the ray will only encounter planes perpendicular to it.  %why is that? is that a rule, or just reactor models?  Neither, its just how warp is right now.

The last problem that can result from this algorithm happens when cells have coincident surfaces.  In this case, which cell is actually intersected is undefined and the next trace iteration will skip the intersection of the coincident cell since it will be smaller than the scene epsilon value.  A way to avoid this would be to ensure the desired coincident surfaces are more than a scene epsilon away from each other.  In some cases, the neutron mean free path is larger than the introduced gap, and this approximation will not change the results, but it could introduce errors cases where the mean free path is smaller than the gap (e.g. in cells next to strong absorbers) % really?  I guess maybe not... will say so.
 Doing this by hand would be tedious, however, so an automatic way of doing this may be an area of future development in WARP.
 % There is research out there about better ways to handle this...we can talk about that later if you end up continuing to work on this :)

This algorithm is very similar to the ray casting point-in-polygon algorithm, which determines whether a point is inside or outside of an arbitrary polygon by counting the number of times it crosses a surface.  An even number means it is outside and an odd number means it is inside.  This algorithm is almost the same, but keeps track of cell numbers instead of binary logic for one surface.  This way, the nesting of a neutron can be determined and the entering cell can be found.  

%Both of these algorithms take advantage of the cell being closed, and in this sense are similar to Gauss's Law or the divergence theorem, which states the flux integrated around a surface will be nonzero only if the surface contains a source.  This material query algorithm is like a discrete, single field line version where the neutron's position is the source point and the cell boundaries are the integrating surfaces.
% Is there a reason to tell us this? Is this relating the paper by Brian Mirtich? If so - you need some back story and explanation and probably some equations...

An important consideration in the type of geometrical representation used in WARP is that the volume of a cell is \emph{always} the spatial intersection of the space inside of the cell with space outside any cells nested inside it.  For example, if two cube cells were specified to be centered at the origin with cube 2 completely encompassed by cube 1, the space in-between cube 1 and cube 2 would belong to cell 1 while the space inside cube 2 would all belong to cube 2.
% This is true, but I'm not sure why you're telling us now? I couldn't figure out a better place...

\subsubsection{Instancing}

Nuclear reactors can have very complicated geometries, but many rely on simple shapes that are repeated in arrays.  There are a few ways in which identical cells can be instanced in OptiX.  The first and most convenient is to define a single primitive in its own coordinate system, then use a transform node (shown in Figure \ref{node_graph}) in the OptiX node graph to transform the primitive to its actual position via an affine transformation matrix.  The resulting node graph is shown in Figure \ref{transform_instancing} for a scene that has three boxes in it.  Note, a GeometryInstance object ties a hit program to the spatial geometry data in the geometry primitive object, acceleration objects are attached to all group objects, and transform objects can only have a \emph{single child} that is a Group or GeometryGroup object.  This is why each GeometryInstance must have its own GeometryGroup.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/transform_instancing.eps}
     \caption{The OptiX node graph using transform instancing \label{transform_instancing} }
\end{figure}

Transform instancing is convenient since only a single primitive needs to be defined.  If another instance is needed, one can simply apply a transform matrix to it and all the work is done by OptiX.  This scheme has a lot of overhead, however, since each instance has its own group and its own acceleration object, producing a deeper node graph than necessary.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.4\textwidth]{graphics/primitive_instancing.eps}
     \caption{The OptiX node graph using mesh primitive instancing. \label{primitive_instancing} }
\end{figure}

An alternative instancing method uses mesh-based primitives.  In this scheme, there is still a single geometry primitive, but now the primitive is attached to an array of spatial data (and OptiX buffer) that contains the dimensions of each individual primitive.  The transform node is no longer used since it would transform the whole group instead of a single primitive.   The transforms must be applied to the data beforehand and the results are written into the OptiX buffer as separate primitives.  

This method produces a shallower node graph with only two acceleration objects and a single GeometryGroup for \emph{all boxes}.  These numbers would not change if there more boxes in the scene, only the number of data elements on the bottom of the graph would change.  This is called mesh instancing since the data structure was envisioned for the many triangular surfaces in meshes of complex objects rather than for individual instancing of separate, simple objects.

%One goal of this study is to determine which of these instancing methods has better performance and will ultimately be used in WARP.

\subsubsection{Test Geometries}

Figure \ref{raster_images} shows the geometry for the interleaved assembly and the large assembly cases used in the first scaling study.  These figures were created by OptiX by performing a cell number query as prescribed in the previous section.  The interleaved assembly consists of three hexagonal arrays, two cylinders, and one hexagonal prism.  The arrays are seven elements on a side, which corresponds to 127 elements each for a total of 383 elements (including the large hex cell around the arrays and the outer box cell).  The large assembly has 25 cylinders on a side, for a total of 1803 elements.  The smaller assembly is 15 cylinders on a side, for a total of 633 elements.

\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{graphics/prelim/interleaved_assembly.png}
  \caption{The interleaved assembly}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}x
  \centering
  \includegraphics[width=.9\linewidth]{graphics/prelim/large_assembly.png}
  \caption{The large assembly}
  \label{fig:sub2}
\end{subfigure}
\caption{$x$-$y$ cross sections of the geometry created by using the PIP cell/material query algorithm \label{raster_images}}
\label{fig:test}
\end{figure}

The cell numbers are mapped to colors in Figure \ref{raster_images}. The images appear correct upon visual inspection and no tracking errors were generated when creating these images, it therefore appears the routine is working and the algorithm can calculate the cell numbers.  A helpful feature of OptiX is that variables can be attached to each geometry primitive.  In addition to the cell ID number, the materiel ID is also attached, so the material query can be done directly by OptiX rather than by determining the cell number and then having to do an additional lookup or hash.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=1.0\textwidth]{graphics/prelim/prelim_scaling_geom.png}
     \caption{The geometries used in the scaling study from least number of objects on the left to most on the right. \label{prelim_scaling_geom} }
\end{figure}

Figure \ref{prelim_scaling_geom} shows the geometries used in the second scaling study.  The hexagonal cell has the same dimensions as the assembly-like geometry case in the first scaling study, but each successive case has smaller and more numerous cylinders in a hexagonal array inside the hexagonal cell.  The most sparse scene only has a single primitive in it, shown on the left side of Figure \ref{prelim_scaling_geom}, and the most complex scene, shown on the right side of the figure, has 42,843 primitives in it.

\subsubsection{Results}

%%%%%%%%%%
%    first study
%%%%%%%%%%

Both of the scaling studies were run on an NVIDIA Tesla K20 card.  Cell queries were done with the PIP algorithm from a uniformly random and isotropic distribution of source points in addition to finding the closest intersection point.  Figure \ref{prelim_optix_k20} shows the ray trace rates versus number particle starting points for the three test geometries. There are cases for each geometry (assembly, large, and interleaved), whether the trace uses primitive or transform instancing (prim or xfrm), and whether it uses a split bounding volume hierarchy or regular bounding volume hierarchy (SBVH or BVH).  K-d trees are not used since they require a mesh vertex buffer be provided by the user, which implies that they only work for triangularly-meshed objects, not ones instanced by simple geometric primitives like spheres and cubes.  Handling geometries that are meshed in such a way may be a future area of development in WARP.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/prelim_optix_k20.eps}
     \caption{Trace rates of an NVIDIA Tesla K20 performing cell queries with the PIP algorithm. `\label{prelim_optix_k20} }
\end{figure}

The trace rates are fairly constant after $10^6$ particles, but primitive instancing is always faster than transform instancing, and using BVH acceleration is always faster than SBVH.  The only time this isn't the case is for the interleaved assembly, where transform instancing performs slightly better than primitive instancing.  The interleaved assembly is also the geometry with the fewest objects.  The final data point at $10^8$ starting points is missing for the transform instancing because the card ran out of memory and these cases would not run.


%, so these rates are representative of the actual rates in a Monte Carlo Simulation where not only the first intersection must be found.  % what does this mean?  not a lot, leaving out.
% After noticing on this plot, it would be nice to have the actual data points on all of the plots instead of lines only. This helps show where there's real data and where it's just a line between data points. It's okay if it's too much of a pain to change this at this point.   Done.
% seems like this should go here, unless it doesn't apply to all tests?  yeah I agree, thanks!
% all your labels say prim...?  ADDED XFRM.

%%%%%%%%%%
%    second study
%%%%%%%%%%

Teh seconda 
The number of source points was set to $8\times10^7$ for every trace in order to ensure being on the trace rate plateau shown in in Figure \ref{prelim_optix_k20}.  A BVH acceleration structure was used since it showed the best performance.

Figure \ref{prelim_optix_scaling} shows the results of the scaling test. It can be seen that the ray trace rates for both primitive and transform instancing plateau after about $10^3$ objects in the scene. % It sort of plateaus after 20 or 30 doesn't it? 
 Primitive instancing still outperforms transform instancing, especially at very large numbers of objects.% it looks like they stay about the same amount apart and get closer towards the end. 
   It should be noted that the trace rates for the two dataset sizes start quite far apart for one object then converge to equal performance at large object numbers where the trace becomes more dependent on acceleration traversal rather than output buffer access (where smaller size makes a difference when there are few objects). %They're not that different - and they come together pretty quickly.
  %Traces were also done in a cell containing no objects, and rates were on the order of ten times faster, which is apparent in the scaling figure.%?

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/prelim_optix_scaling.eps}
     \caption{Trace rate scaling on an NVIDIA Tesla K20 performing cell queries with the PIP algorithm. \label{prelim_optix_scaling} }
\end{figure}
%What do the 10^6 and 10^8 mean? You don't need to label them all as BVH since they all are.

Using transform instancing did not work for cell numbers between 2613 and 10,623 cells for the $10^8$ dataset size because this requires too much memory so the tracking process cannot execute % you may want to explain this more clearly. 
This is why the transform instancing trace stops at 2613 cells in Figure \ref{prelim_optix_scaling}.  This is also likely the reason why performance of the transform instancing method drops at the same point for the $10^6$ set of rays.  This is the point where OptiX started paging GPU memory to the host memory, and this cost degraded performance significantly.  Structure construction for transform instancing prior to the trace also became very slow compared to primitive instancing, presumably due to the independent acceleration objects attached to every instance and the overhead of computing all the affine transformations at acceleration structure build time.

Traces were also done where the direction of every particle was in the negative $z$ direction, since this is the direction with the fewest number of surfaces between the source particles and the boundary cell.  Performance was increased marginally, but this can only be taken advantage of in geometries where there is a direction of least heterogeneity (like 2D lattices).% why would you do this exactly? I don't understand...

Figure \ref{prelim_optix_G650M} shows the trace rate versus dataset size test, but run on an NVIDIA GeForce GT 650M instead of a Tesla K20, the discrete graphics card in a MacBook Pro, Mid-2012 Retina model.  The same trends can be observed except that overall trace rate is much slower, which is not surprising,% why not?
 and the impact of transform instancing is very pronounced in runs with fewer particles.  This study was done to compare the performance of a smaller non-compute card compared to a larger compute card.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/prelim_optix_G650M.eps}
     \caption{Trace rates of an NVIDIA GeForce GT 650M performing cell queries with the PIP algorithm. \label{prelim_optix_G650M} }
\end{figure}% is there a way to keep the legend from covering the data?   This is the best I can do...

From the OptiX geometry study it can be concluded that this library can be used to handle the geometry representation in a Monte Carlo neutron transport code.  Using a primitive-based geometry instancing method, a BVH acceleration structure, and running as many parallel rays as possible provides the best performance.  OptiX also seems to be insensitive to the number of objects in a scene after about ten primitives.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{WARP in Detail}

The preliminary studies showed that thread divergence can be effectively reduced with CUDPP without incurring huge costs, that large datasets need to be run to hide memory latency, and that a BVH acceleration structure in OptiX performs best for randomly-oriented distributions of rays.  Since most data will be accessed randomly in all cases, the data layout will be regularized as much as possible and the particles will be sorted by reaction, sweeping out completed particles and grouping similarly-interacting particles together to maximize warp coherence and occupancy. 
% It seems like laying out the data is trying to fight random access, not that you're making that choice because of random access?
 Since histories complete randomly, history data access will become sparse even if sweeping out is not done. % so why do it?
 In any case, the radix sort%what's that?
 will preserve monotonically increasing thread IDs within a reaction group, so its impact on data access should be small, and the only cost be that of performing the operation.
 % Ok, I'm just confused but what you're trying to say in this paragraph. 

As stated before, WARP was designed to read ACE-formatted data, perform all reaction types as prescribed by the data, use a Serpent-like unionized energy grid to regularize data access, use an event-based transport algorithm with parallelized operations for sorts and sums, use OptiX for general 3D geometry representation (without explicit nesting), use an SOA for neutron history data, and perform all operations on the GPU unless strictly forbidden.  In terms of implementation, OptiX is used for 3D geometry representation; physics routines that use real cross section data are made in CUDA; the compaction routine in CUDPP is substituted for a radix sort; and tallies have been implemented, as were criticality and fixed source solution methods.  These expansions yield WARP, a program for continuous energy Monte Carlo neutron transport on GPUs in general 3D geometries, which will be compared for accuracy and execution speed against Serpent and MCNP.

WARP is written in C/C++ with some Python (which will be explained later in this section), and relies on the OptiX framework for geometry representation and the CUDPP libraries for dataset-wide parallel operations. Single precision floating point numbers are used throughout in order to realize the full computational capacity of the GPU and to allow simulations to be carried out on more affordable and higher clocked GeForce cards.  Using single precision numbers may be dangerous when there are very dilute isotopes or very rare reactions reactions present, as roundoff error may make their contributions zero. % I'd also worry about criticality calculations...
 Buffer overflow and roundoff error in the tallies may also be a problem with single precision, but this can be mitigated by accumulating the tallies frequently in a double precision vector.  If double precision data is found to be needed, WARP can easily be changed, and doing so may be an interesting experiment in the future.

Many details of WARP have been described thus far. The subsequent sections will discuss the cross section data access patterns and the energy related routines.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Layout}

As described in Chapter \ref{chap:background}, the nuclear data required in Monte Carlo simulations are very heterogeneous.

\subsection{Unionized Cross Sections}

It is particularly troublesome that each cross section has its own independent energy grid.  Using point-wise data for continuous energy simulations requires an interpolation be performed between points, and to do this, the code must somehow scan the energy grid array to find the points between which the neutron's energy falls.  If every isotope has its own grid, this search must be done for every isotope, and can become very expensive.  This is why a unionized energy grid structure, like that implemented in Serpent, is used in WARP \cite{jaakko_xs}.

Unionizing the cross sections simply means that the energy grids of the cross sections are all unionized into a single, larger structure that every isotope can use, i.e.\ many 1D vectors are transformed into a single 2D matrix indexed by incoming energy and MT reaction number. %I'm assuming this is material number, can't remember if you told us, sorry!
 Figure \ref{unionized_layout} shows the unionization process with two small, arbitrary energy grids and their corresponding cross section vectors.  

%It would be clearer if tied your explanation to the figure and added a little more detail. I think this is better?
Unionizing the energy grid makes holes in the data, %tell us why
which are filled by linear interpolation. The colors in the unionized energy grid represent which isotope the grid value came from, the green cross section data blocks are interpolated values, and the red blocks are placeholder zeros to preserve thresholds. It is clear that the resulting dataset is larger than the sum of the individual cross sections since it contains redundant data, but it will be much easier to search.%how is it redundant? I thought it was just filled in? maybe just unecessary?
  This tradeoff between regularizing the data in this way as opposed to accessing the energy grids separately is worthwhile because...  
  
\begin{figure}[h!] 
\centering
\includegraphics[width=0.6\textwidth]{graphics/unionized.eps}
\caption{Unionizing two cross section vectors. \label{unionized_layout} }
\end{figure}

Because the cross sections unionized in this way the cross section values for every isotope can be read along a single row once the bounding energy indices, $E_i<E<E_{i+1}$, are found for a given interaction.  This format reduces the number of energy searches needed and promotes data locality by keeping all the data points for all isotopes in a given energy range adjacent to one another in memory (if row-major matrix format is used).  On CPUs, the data for an energy value can be read as a cache line and accessed quickly by the cores.  On a GPU, since a thread needs to read the entire energy line (one energy over all isotopes) in sequence in order to compute the macroscopic cross section on-the-fly, the memory loads are not coalesced and memory bandwidth is wasted (adjacent threads do not access adjacent memory at the same time).  % Maybe we need to know how you're computing cross sections for this to be clear. Perhaps note that you'll discuss the impact of this tradeoff later and move this discussion to cross section calculation? 

To mitigate this inefficiency, the data can be recast to use the  \lstinline{float4} datatype, which is a vector type that is 16 bytes long.  When requested, the entire 16 bytes will be loaded in one transaction rather than 4 separate 4-byte transactions, maximizing bandwidth and minimizing the number of global requests (and reducing the impact of latency).  Also, since all of the total cross sections are all needed at one time, %? all for all materials? 
 they are stored together in the first columns of the unionized data array.  After this block of total cross section columns, the individual reaction cross section are stored as blocks in the same material order as the total cross section block (all the cross sections for an isotope are in a single block).% what? I thought you were storing total for all materials, then scattering for all materials, etc. that's opposite from all cross sections for one isotope are in a block. 
   This is done so the macroscopic or microscopic kernel needs only scan a single, contiguous block of data in the array.

\subsection{Distribution Data as a Linked List}

The cross section data can be regularized with the unionized grid method, but there are other data distributions that WARP needs to conduct accurate simulations.  The most prevalent of which are scattering matrices that give tabulated probability distributions for outgoing CM angle $\mu$ and incident neutron energy.  Since these matrices have their own incoming energy grid, these values can be unionized into the cross section dataset as well.  This transforms many 2D scattering matrices into a 3D matrix indexed by incoming energy, outgoing $\mu$, and reaction MT number.  

Since the energy grid of the scattering matrices is often much, much coarser than the main cross section energy grid, this operation produces a \emph{very large} amount of redundant data.  It was attempted with $^{235}$U, and resulted in a 3D matrix that was 6.4 GB (30,991 energy grid points, 598 $\mu$ CDF points, 51 reactions).  This is just for a single isotope's angular distributions of all reactions, and the dataset already cannot fit on the on-card DRAM on most GPU cards.  

After determining that unionizing with the main energy grid was an unacceptable method, another approach was tested where the energy grids of the scattering matrices were unionized between themselves, but not with the main energy grid.  This would reduce the amount of unnecessary data, but would require an additional search be made on the unionized scattering matrix energy grid.  The memory usage of this method with $^{235}$U was reduced to 12 MB (103 energy grid points, 598 $\mu$ CDF points, 51 reactions).  When tested with $^{235}$U and $^{238}$U, usage for the scattering data increased to 68 MB (171 energy grid points, 1072 $\mu$ CDF points, 97 reactions).  When tested with $^{235}$U, $^{238}$U, $^{16}$O, and $^{1}$H, usage became 1.1 GB (1325 energy grid points, 1237 $\mu$ CDF points, 170 reactions).  Therefore, this method was also deemed too costly for systems with a practical number of materials.  The distributions are therefore left in their original formats for use with GPUs, where storage space is much more limited compared to CPUs.


\begin{figure}[h!] 
\centering
\includegraphics[width=0.7\textwidth]{graphics/unionized_pointers.eps}
\caption{Making a link to distribution data in the unionized dataset. \label{unionized_pointers} }
\end{figure}

After these failed attempts for resolving the scattering data heterogeneity problem, a method was formulated to eliminate the secondary energy search without having massive data replication.  This method introduces two new matrices identical in size to the unionized cross section matrix, but instead of containing data values, they contain pointers to the location of the appropriate distribution data for the reaction at the energy index determined by the grid search.  Two matrices are needed - one for the scattering distributions and another for the energy distributions.  This way, the unionized dataset is only replicated 3 times, the distribution data can be copied to the card in its original format, and only a single gird search needs to be performed.  This is similar to a linked list, where a there is a pointer at the end of an object pointing to the next object in a list.  

In this case, once a reaction is sampled to occur, there is a pointer readily available at the same index in a second matrix to the appropriate distributions, and no searching needs to be done.  Since the distribution PDFs need to be read serially by each thread, the float4 format can also be of use here.  If there are no distributions for a particular reaction or energy, a null pointer is inserted into the matrix.  For fission reactions that do not have scatting distributions (neutron emission can be assumed to be isotropic \cite{openmc}), the $\nu$ value for the grid energy is stored in the scattering pointer matrix instead of a pointer.  This way a search for the appropriate $\nu$ value does not have to be performed either.  Figure \ref{unionized_pointers} shows a pointer matrix for the unionized cross section matrix in Figure \ref{unionized_layout}.

\subsection{Embedded Python}

Now that the data layout has been discussed, how the data is loaded or reformatted from ACE-formatted data files will be explained.  An initial effort was made to write an ACE-parsing script in C from scratch, but this was abandoned in favor of using the existing ACE module of the PyNE (Python for Nuclear Engineering) package \cite{pyne} (why reinvent the wheel?).  The PyNE package ``is a suite of tools to aid in computational nuclear science \& engineering. PyNE seeks to provide native implementations of common nuclear algorithms, as well as Python bindings and I/O support for other industry standard nuclear codes.'' \cite{pyne}.  PyNE contains a Python module that can parse ACE data files into Python objects that can be more easily handled than flat data arrays.  This module was written by Paul Romano as a preliminary project for OpenMC, whose ACE library was later written in Fortran based on the methods developed in this Python module \cite{pyne,openmc}.  

Some parts of PyNE have a C++ API as well as a Python API, but this is not the case for the ACE module.  It was originally written in Python, not C++, so it has no C++ API.  This created a problem for WARP since WARP is written in C/C++ and could not directly make use of the ACE module.  Fortunately, there is a very effective C API for Python!  This API allows one to initialize and run a Python instance from a C program.  In WARP, this API is used to start a Python instance where PyNE can be used to load cross sections from ACE data files.  NumPy [reference] is then used to unionize the energy grids of the requested isotopes and perform the linear interpolation to fill in the gaps.  Once this is done, the Python instance returns the NumPy array to WARP as a C data structure.  The data is copied to an internal array and the Python object is cleared.  

WARP then loops through the unionized array, requesting distribution data from the Python instance.  The scattering and energy distribution data are also copied for the requested energy range in a similar fashion, and a device pointer for the distribution is written into the scattering and energy pointer matrices.  When all the needed data has been copied over to WARP, the Python instance is terminated and its memory freed.  Since the unionized energy grid is an index of the matrix that needs to be searched before a row of the unionized cross section matrix can be accessed, the energy grid is stored as its own contiguous array rather than a column of the matrix.  Figures \ref{unionized_layout} and \ref{unionized_pointers} show the energy grid as a column simply for illustrative purposes.

The unionized cross section dataset in WARP is resident in the global memory of the GPU.  Storing it in constant memory would seem to make sense, but since it is limited to 64kB, the entire dataset simply cannot be stored here.  Texture memory was also considered, as this could possibly contain all the data, but it is optimized for 2D data locality, whereas the dataset format is mostly accessed in a linear fashion across rows and randomly across columns (since energy-changing reactions are sampled randomly).  

Both the constant and texture memory spaces are cached, so the random access inherent in Monte Carlo simulations may make cached access worse than non-cached because of cache miss penalties.  This effect was not studied in the initial development of WARP, however.  Nelson reports using the constant memory space in his simulations, with little-to-no performance improvement \cite{nelson}.  Using the texture memory to store the cross section matrix might benefit from the free linear interpolation that can be performed with the texture element load, however. % wrap up sentence :``so that's why I picked global..."

In its current state, WARP does not use the thermal scattering (S($\alpha,$$\beta$)) tables or unresolved resonance parameters.   These tables improve the physical fidelity of the simulation, but these features can be turned off in production codes and direct comparisons can be made without them.  Their incorporation may lead to more divergent program flow and is left as an area of future work.

\subsection{Python Wrapper}

Work is also underway for wrapping the WARP shared library with Python.  This would be done via SWIG \cite{swig}, a piece of software that automatically wraps compiled languages like C/C++ in high-level scripting languages.  This is being done for convenience and usability reasons.  With the C++ classes exposed in Python, the main() function can be replaced with a Python script, eliminating the need to recompile WARP applications when different geometries or different run parameters are desired. % you need to recompile warp for this now?

The Python wrapping approach deviates from the standard flat text input file structure that most Monte Carlo codes use.  Flat text input relies on keywords and adds a layer where input files need to be parsed and data structures are then built in the application based on the information parsed from the input.  Using Python to directly access the classes and their data removes this layer, and allows a user to build complex applications.  Since the results would also be resident in a Python session and would therefore be easily available to the user for potting scripts or analysis tools.  To process data in the same way from text-file-based output, the output needs to be parsed with a user written function or processed by hand, which is time consuming and can lead to human error.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Tasks}
\label{sec:tasks}

Now that the necessary data has been loaded and reformatted in a GPU-friendly way, the details of the actual process of transporting neutrons will be described in this section.  Neutron transport consists of an inner and outer transport loop.  The inner loop, shown in Figure \ref{warp_inner_loop}, actually transports the neutrons through the problem geometry and samples the reaction CDFs.  The blocks in the figure represent independent kernel launches, and are executed left-to-right.  The first step is using OptiX to perform the material query, since this information is needed to query the proper cross sections and to determine the distance to the nearest surface along the neutron's path of travel.  Once this is known, a kernel is launched to do a search on the unionized energy grid to determine the index $i$ that satisfies $E_i<E<E_{i+1}$.  Next, the macroscopic kernel is launched. This kernel computes the total macroscopic cross section for the material, samples the interaction distance, samples which isotope the neutron interacts with, moves the neutron to the nearer of the interaction/intersection distance, and sets the neutron's reaction number to the resample flag if the intersection is closer.  

Since the macroscopic cross section has been computed, the tally kernel is launched and scores a specified flux tally.  After the flux is scored, the microscopic kernel is launched to determine which reaction type in the determined isotope occurs.  The radix sort is done next since the reaction has been determined, and this operation sorts the neutrons by reaction type and updates the values in the remapping vector.  The next four kernels are launched concurrently, as indicated by being in the same horizontal position in Figure \ref{warp_inner_loop}.  Each of these kernels performs the specific functions necessary to model their different reactions, and since a neutron cannot undergo two reactions simultaneously, the data they access does not overlap and these kernels can be launched concurrently.  If a neutron's reaction number is the resample flag, it skips the microscopic and reaction kernels and isn't processed again until the material query, which resets the reaction number to zero.   This loop cycles until all neutrons in a batch are terminated through absorption or leakage.  

\begin{figure}[h!] 
\centering
\includegraphics[width=0.9\textwidth]{graphics/warp_inner_loop.eps}
\caption{WARP inner transport loop that is executed until all neutrons in a batch are completed. \label{warp_inner_loop} }
\end{figure}

The outer loop, shown in Figure \ref{warp_outer_loop}, uses the result from the inner loop to set up the next batch of neutrons.  The inner loop serves to determine the yields of the batch's neutrons.  The yield array is then reduced to determine the multiplication factor, $k_\mathrm{eff}$, of the batch. The yields are then divided by $k_\mathrm{eff}$ to make the yield as close to one as possible without biasing the fission point distribution.  Since dividing by $k_\mathrm{eff}$ is unlikely to yield an integer, the rebased yield is sampled between the bounding integers to preserve the mean across many neutrons.  
% I think you need some equations to clarify what's returned, how you get k, what you're dividing, and how you get k again.
 
After the yields are rebased, a prefix sum %what's that?
 is performed on the yield array.  The value for each particle's element of the prefix sum is the total number of fission neutrons before it.  Since this is known, the ``pop'' routine can insert the sampled fission neutrons into the next batch's history vector with no risk of writing into the same element.  Since the yields were rebased to make $k_\mathrm{eff}\approx1$, the pop kernel will also fill the the entire history vector with newly-sampled fission neutrons.  Once the pop samples the fission spectrum, it inserts particles into to history vector a points where fission occurred, but with a new energy and an isotropically sampled direction, and the inner loop transports the new batch. % is it a lot of work to make a map of what's happening here? If so, don't worry about it. 

\begin{figure}[h!] 
\centering
\includegraphics[width=0.9\textwidth]{graphics/warp_outer_loop.eps}
\caption{WARP outer transport loop that is executed in between neutron batches for criticality source runs. \label{warp_outer_loop} }
\end{figure} %did you tell us about CURAND?

Fixed source runs do not have an outer loop since the source is not dependent on the flux.  If there is enough memory available, the entire number of requested neutrons are initialized based on the source definition.  The neutrons are transported simultaneously, and any secondary neutrons produced from fission or (n,2/3/4n) reactions are popped into the history vector at the end of the inner loop to be transported with the original source particles in the next pass of the loop.

\subsection{Grid Search Kernel}

Much effort has been expended in attempting to find an algorithm that requires only one search on the main unionized energy grid to find the index $i$ that satisfies $E_i<E<E_{i+1}$, but so far such an algorithm has not been established.  It is convenient that the values in the energy grid are monotonically increasing, but since the array is composed of floating point values with arbitrary spacing an inverse function cannot made to calculate an index in $O(1)$ time.  Conversely, a naive approach would be to scan the array from beginning to end, performing comparisons along the way, and therefore calculating the cross section array index in $O(n)$ time, where $n$ is the number of elements in the array.  This method does not scale well and will not be considered.  

An extreme method taking advantage of the space-time tradeoff would be to use a simple lookup vector where the points are linearly spaced.  In order to include all original data, the spacing of the lookup vector, $dx$, would have to be smaller than the smallest found in the data.  The lookup vector index could simply be calculated by $i=E/dx$, and cross sections could be searched in constant time.  The spacing around resonances in the nuclear data is very small, however, and makes this method's memory usage unacceptably large.  For uranium-235, the smallest difference is $10^{-12}$ MeV over a range of $10^{-11}$ to 20 MeV, indicating that the vector would need to be around $20/10^{-12} \approx 10^{14}$ elements long.  Logarithmic spacing could also be used, but again assuring that all original data is included may make the regular grid too fine, resulting in unacceptable memory use.

% It's not clear to me that this discussion of serpent methods fits here. Frame them as other options and talk about why you don't use them? you could just tell us what serpent actually does and why that is or isn't a good idea for warp. the way it's written now is sort of wandering in the weeds a bit. 
%Many methods were considered in the development of Serpent, most notably reconstructing the unionized dataset with an evenly-spaced grid whose indices could be calculated analytically.  The cross sections are either linearly interpolated or set to the lower point value (histogram method) in the new regular grid.  The reported effects% of this strategy?
% introduce more uncertainty into the results %than what?
% since information is lost around tightly-grouped energy points at resonances. This is the cost of faster search times.  The histogram method is equivalent to the lookup vector outlined previously, but without the constraint of including all the original data--this makes the vectors smaller and...(uses less memory? is faster since there's less data?).  
 
%Another approach described in Serpent overlays a regular grid over the original unionized data that contains pointers to the cross section data within each energy interval in the overlaid grid.  With this structure, the small overlay bins are calculated analytically, and the points within are searched iteratively. This method decreases search time and does not discard data, but many redundant points are included within the intervals, leading to increased memory usage \cite {jaakko}.  This increased memory problem can be overcome by including the index boundaries within the interval instead of the entire interval itself. This way the iterative search domain can be substantially reduced with an analytic function without much memory overhead. % I don't understand what you are/were pointing to. 

Another approach could be to use a high order polynomial approximation of the unionized grid for the initial guess of an iterative method.  This would be similar to the Serpent pointer search method, but would eliminate the pointer vector and use the analytical computation to find an initial index in the main grid instead of computing the index of the pointer vector. 

The binary search is the classic search algorithm for searching irregular data.  Instead of progressing through the array from beginning to end, it is continually bisected until the correct interval is found.  In other words, the search begins at the center of the vector.  If the searched-for value is less than the middle value, the next loaded value is the middle value of the first bisection.  If the searched-for value to greater than the middle value, the next loaded value is the middle value of the second bisection.  This process repeats iteratively until the value is found.  The binary search algorithm is attractive because it runs in $O(\log_2(n))$ loads/comparisons in the worst case, and uses no storage except for the loaded and comparing values \cite{binary}.

% okay, I tried re-writing, I give up. I'm confused. I almost get it, but can't figure out how to write it. The picture makes sense, but I don't understand the way you're explaining the algorithm. 
The binary search could be inefficient from a memory bandwidth standpoint since each thread needs to perform this search.  When access becomes much more sparse after the first few iterations of the algorithm, bandwidth is lost from accessing single floating point values in a non-coalesced manner.  Since \lstinline{float4} values can be loaded in a single transaction, it may make bandwidth sense to take advantage of them.  Figure \ref{quad_tree} shows how a thread would traverse this data structure.  The lowest level of the tree holds the original grid values, and the tree is built bottom-up. The tree is searched, however, top-down. When the energy value satisfies $E_{j-1}<E<E_{j}$, where $j$ is the local node's \lstinline{float4} index, the thread accesses the next \lstinline{float4} node located at pointer $p_j$.  This traversal is done iteratively until a null pointer is encountered, which signals that the final main energy grid index $i$ that satisfies $E_{i}<E<E_{i+1}$ has been found.  %I don't entirely understand this, because you're following the red lines (which you should mention) but they don't satisfy $E_{j-1}<E<E_{j}$. 

Instead of bisecting the data on each iteration, requiring only one value to be loaded, the data can be divided in fives if four values are loaded.  Such an algorithm would converge faster than a binary search and would use as much bandwidth as possible.  However, since the algorithm now has four values, the index that corresponds to the values is not clear (in binary search it is simply whatever the current index is).  To resolve this, the search can be made into a linked list, where a second \lstinline{float4} vector is loaded containing pointers to the next \lstinline{float4} value vector.  Pointers on a GPU are typically 64-bit, however, so using a tree like this would require 3 loads of a float2, not 2, and the implications of this are unclear.
 

\begin{figure}[h!] 
\centering
\includegraphics[width=1.0\textwidth]{graphics/quad_tree.eps}
\caption{A search employing a four-leaved tree structure where $E_{13}<E<E_{14}$. \label{quad_tree} }% and I don't understand your picture. what do the red lines mean?
\end{figure}

WARP currently uses a simple binary search algorithm because of this algorithm's logarithmic scaling and ease of implementation.  It will be determined if the grid search is a significant portion of the simulation time, and if it is, WARP could benefit by implementing more advanced searching algorithms in the future.
% you might want to start with what WARP does and then go through other options in a pros/cons sort of way with the idea being potential future work. 

\subsection{Pseudo-Random Numbers}

Random numbers are the basis of the Monte Carlo method, and generating them must be done efficiently.  Since these numbers must be computed, they are not strictly random, but rather pseudo-random since they are randomly distributed but can be deterministically computed.  NVIDIA provides a random number library called CURAND that can generate large arrays of randomly-distributed numbers \cite{curand}.  Initially, WARP precomputed all the random numbers it would need for a single transport loop iteration.  This could be done since direct sampling methods were used everywhere (which was incorrect for the target velocity in scattering) and there was a maximum number of numbers required.  The threads would simply access random number values stored in this giant array (about 20 numbers were needed per thread).  This is a very bandwidth-intensive way to access random numbers since they are always written and read from global memory.  In these early implementations, the random number generation and access was taking about a quarter of the transport loop time.  In addition, the introduction of the correct rejection sampling method for the target velocity disqualifies precomputation since there was no longer a maximum number of numbers needed.

Taking these points into consideration, it was decided to use a linear congruential random number generator (LCRNG) for computing random numbers in the transport kernels.  It allows the kernels to generate random numbers with the recursion relations shown in \eqref{lcrng}, where $x_{n-1}$ is the previous random number.  The values $a$, $c$, and $m$ are taken from Pierre L'Ecuyer's manual for configurations with a high figure of merit \cite{lcrng}.   % should that be capitalized and in italics?

\begin{equation}
\begin{split}
x_{n} &= a x_{n-1} + c \mod m  \\
a &= 116646453 \\
m &=  2^{30} \\
c &= 7
\end{split}
\label{lcrng}
\end{equation}

%LCRNGs are based on integer numbers that must be converted to floats when the numbers are needed.  This is done by simply dividing by the modulus value, $m$. 
% you already have this in the formula - so this seems redundant.
OpenMC uses a similar LCRNG implementation but with different values for $a$, $m$, and $c$ since it's double rather than single precision.  The values were chosen close to the full 32-bit range for maximum floating point resolution.  Having a modulus as a power of two is very convenient as well since the modulus can be performed by bit truncation rather than a full modulus operation \cite{openmc}.  In other words, the operation can be done by a bitwise AND operation between $a x_{n-1} + c$ and $2^{30}-1$, which in binary is all ones below the 30th bit.

Using a LCRNG keeps global access down and improves performance greatly, since a single seed value is loaded at the start of the transport loop, stored in fast registers, then written back at the end of the loop.  CURAND is used outside of the LCRNG cycle to advance the seed bank with the Mersenne Twister algorithm (which has a period of $2^{19937} - 1$) to make sure no correlated numbers are used as intermediate seeds.

\subsection{Macroscopic Cross Section Kernel}

The macroscopic kernel in WARP computes all values related to macroscopic cross sections.  These include total macroscopic cross section for the material, interaction distance, which isotope the neutron interacts with, moving the neutron to the nearer of the interaction/intersection distance, and setting the neutron's reaction number to the resample flag if the intersection is closer.  

Since the isotopes' total cross sections are stored together in the unionized cross section matrix, the total macroscopic cross section can scan the contiguous row of this part of the matrix, interpolating the value between $i$ and $i+1$, which was found by the grid search kernel.  The interpolated values are multiplied by the atom density vector (in atoms/barn-cm) and the sum is accumulated as shown in \eqref{macro_vector}, where $M_k$ is the number density and $\Sigma_{t,k}(E)$ is the macroscopic cross section of isotope $k$.  When the material vector has been completely scanned and total cross section is computed, the array is scanned again, but this time a random number is used to determined which isotope the neutron interacts with via \eqref{isotope_selection}.

\begin{equation}
\Sigma_t(E) = \sum_{k=0}^{N_\mathrm{isotopes}} M_k \: \Sigma_{t,k}(E)
\label{macro_vector}
\end{equation}

The material's total macroscopic cross section is written to an array so the flux tally routine does not have to recompute it, which would take many more global memory transactions than loading it from the array. % Are the macro xsecs computed once per calculation and stored, or do you recompute them at any point? 
 Next, the interaction distance is sampled via \eqref{first_col_samp} and is compared against the nearest surface distance, which was computed with OptiX in the first step of the transport loop.  If the interaction distance is nearer, the neutron's coordinates are changed to this location.  If the surface is closer, the neutron is moved to the surface and its reaction number is changed to the resample flag value, which will cause it to skip all other kernels until OptiX determines the new material and resets its reaction flag.

\subsection{Flux Tally Kernel}

The flux tally kernel simply scores a collision in a predefined cell into a bin via \eqref{flux_tally}.  Currently, WARP only allows evenly spaced logarithmic bins since their indices can be calculated analytically (as opposed to an arbitrary input grid).  The tally index, $j$, out of $N_\mathrm{tally}$ bins for a neutron with energy $E$ is calculated via \eqref{tally_index}.  Since multiple threads could be adding to the element located at $j$, atomic operations are used to perform to sum, specifically the atomicAdd() function.  Atomic functions perform the load-compute-store operations in a single, uninterruptible transaction.  This eliminates the risk of data races occurring.

\begin{equation}
j = \mathrm{floor} \left( \frac{ \log \left( \frac{E}{E_\mathrm{min}} \right)} { \log \left( \frac{E_\mathrm{max}}{E_\mathrm{min}} \right)} (N_\mathrm{tally}-1) \right)
\label{tally_index}
\end{equation}

Since WARP uses single precision floats and integers, buffer overflow can be a problem when a tally is scored frequently (or at least more of one than if double precision was used).  Because of this, the tallies are copied from the device during each outer loop iteration.  The tally values are divided by the total number of source neutrons and are accumulated into double or long integer arrays, as appropriate, on the host.  After copying, the device arrays are zeroed.  Dividing and accumulating also serves to increase the accuracy of the arithmetic since the tally values are kept within a smaller range of values. Roundoff error is exacerbated by adding small floating point numbers to large ones, which is more likely when simply accumulating all of the tally data without periodically normalizing by particle count.  The required intermittent copying during transport increases the device-host communication, but this impact is minimal and can be overlapped with other routines since it is only done once in the outer loop.

\subsection{Microscopic Cross Section Kernel}

Unlike the macroscopic kernel, the microscopic kernel only has one job - to determine which reaction the neutron undergoes in the isotope it has been sampled to interact with.  This is done by scanning the isotope's subrow in the unionized cross section matrix as show in \eqref{micro_vector}. 
% this equation shows the pdf, it doesn't show how you actually select an interaction. Also, do you re-compute the pdf every time or do you store it?
 $N_{\mathrm{start},k}$ is the starting index of the row for isotope $k$, $N_{\mathrm{end},k}$ is the index of the isotope's last reaction cross section, and $\sigma_{t,k}(E)$ is isotope $k$'s total microscopic cross section.  The starting and ending indices for an isotope are precomputed and stored in a separate array.  For most runs, this array and the material isotope density array can be stored in fast shared memory.

\begin{equation}
\begin{gathered}
m <  N_{\mathrm{end},k}\\
 PDF_m = \frac{1}{\sigma_{t,k}(E)} \sum_{z=N_{\mathrm{start},k}}^{z=m}  \sigma_{i,z}(E)
 \end{gathered}
\label{micro_vector}
\end{equation}

Once the reaction $m$ is sampled, the sorting routine is launched to remap data references and sweep completed histories out, then reaction kernels can be launched to carry out the individual reactions.

\subsection{Interaction Kernels}

There are only four reaction kernels since all disappearance reactions are taken care of in the radix sort.  That only leaves the three different kinds of scattering and fission ((n,2/3/4n) reactions follow laws of continuum scattering but have yields that are taken care of in the pop routine).  The scattering reactions change the direction and energy of the neutrons, and their distributions are sampled as outlined in Section \ref{sec:stat}.  

The implementation of these methods is where the pointer array becomes useful.  Once the row, $i$, and column, $m$, of the unionized cross section matrix have been determined, the scattering kernels simply need to access these coordinates in the scattering and energy pointer matrices to load the appropriate distribution data.  No additional searching needs to be done.

\subsubsection{Concurrent Kernels}

As mentioned previously, the reaction kernels should never access overlapping data so that they can all be launched concurrently.  NVIDIA cards with a compute capacity of 2.0 and higher support up to 16 concurrent kernel launches.  Kernel launches are submitted to nonzero execution streams by declaring additional streams to be present and passing the stream objects to the kernel launches as an extra launch parameter.  Kernels launched on different streams can be executed concurrently and their blocks can be interleaved, which is why it is very important for them to act on independent data. %tie this more directly to reaction kernels. the first sentence does, but the rest of the paragraph is sort of detached. 

Kernels launched without an explicit stream are launched on the default stream 0, which are synchronous (program flow does not continue on the host until the launch is complete).  If kernel synchronization is important after execution completes (as it is here, reactions must be carried out before the OptiX trace), the streams must be synchronized with an explicit stream synchronization command or with a synchronous operation like a cudaMemcpy().% when does it need to be synchronous? how does this relate to interaction kernels? what does your 2nd parenthetical statement mean? 

\subsection{Parallel Operations}

The array-wide parallel operations in WARP are done with the CUDPP library.  The functions used are reductions for calculating $k_\mathrm{eff}$ via \eqref{k_eff_batch}, prefix sums for calculating the indices for the source pop kernels, and sorts for keeping warps coherent and full after the microscopic kernel.

\subsubsection{Remapping with Radix Sort}

Since the reaction numbers are integers they can be sorted by very efficient algorithms like a radix sort, which performs in $O(kn)$, where $n$ is the number of values to be sorted and $k$ is the number of significant digits of the values.  The reaction numbers in WARP are four digits or less, so $k=4$.  The reaction number encodings in WARP are shown in Appendix B in Table \ref{WARP_rxn_numbers}.  Since reactions greater than 900 represent a terminated particle, the sort will push all the completed history references to the end of the remapping vector.  

Another benefit pushing all of the completed references to the end of the vector is that, as neutrons start to complete, the length of the sort can be reduced and it will be computed faster.  Figure \ref{radix_sort} shows how the radix sort will arrange the remap vector and how threads will be mapped to the data.  CUDPP's radix sort conveniently sorts for key-value pairs, and WARP uses the reaction numbers as keys and the data index as values, effectively creating a remap vector.

\begin{figure}[h!] 
\centering
\includegraphics[width=1.0\textwidth]{graphics/radix_horiz.eps}
\caption{A radix key-value sort creating a remapping vector. \label{radix_sort} }
\end{figure}

There is one more piece of information that is needed to launch the reaction kernels concurrently--the number of particles undergoing each type of reaction.  This is determined by launching a comparison kernel on the sorted array, which detects the places where the reaction numbers change.  The algorithm to do this is a simple adjacent comparison.  If there are $N$ elements in the array, $N-1$ threads are launched that load values tid and tid+1.  If the values are different, tid is the last index of a reaction type. If a transition is determined to be valid, its thread can write to a small array and no data races should occur because there are coarse boundaries for the reaction kernels and each boundary is unique.  There are only two boundaries for each reaction group, this %what's this?
 can be done without atomic operations since only one thread should ever write to an element.  
 
The array % of what?
needs to be read by the host in order to determine the number of blocks that need to be launched for each reaction kernel and where they%who are they?
 start accessing the remapping vector.  Therefore, it makes sense to have this edge array as mapped memory, where values are implicitly copied from the device.  Declaring the array as mapped allows CUDA to handle when it makes sense for the copy to happen and can potentially hide the communication overhead.  It also makes the code simpler, since explicit copy commands are not needed.  
 
If a launched thread lies on a boundary, it writes its index into a vector containing 11 elements, the start and end indices for the four reaction kernels (three types of scattering kernels and the fission kernel), and the the resample block.  Terminated neutron data are pushed to the end of the remapping vector and will not be accessed by active blocks, eliminating the need for an explicit absorption kernel (which \emph{is} needed in the non-remapped implementation of WARP).

The standard MT numbers are not well suited for producing contiguous blocks of reaction types with a single sort.  Fission and other neutron-producing reactions that terminate a primary history are in between scatter reactions, and the resample flag (MT=800) is larger than every reaction type other than leakage (MT=999) and geometry miss (MT=997).  All disappearance reactions, like (n,$\gamma$), are greater than 102.  To resolve this problem and make clear, contiguous reaction blocks, the MT reaction numbers are slightly modified in WARP.  

Any reaction over 900 signifies a terminated history, so when the cross section data is loaded, any reaction larger than 102 has 1000 added to it to ensure it will be pushed out of the remapping vector upon sorting.  Since fission also terminates particles but needs a yield value sampled after the microscopic kernel samples it to occur, any reactions in the MT=11-45 block have 800 added to their MT numbers.  Therefore, after the sort is performed, the fission-like reactions are already near then end of the remapping vector.  After the fission kernel processes these histories, their reaction number have 100 added to them, making them part of the completed history block.  The reaction numbers for scattering (MT=2, 51-91) are not changed. MT=11-45 not including the fission numbers can usually be treated as continuum scattering (ENDF law 44), but may not be true for all isotopes. With the reactions laid out as such, a contiguous remapping vector can be formed and the total number of active neutrons can be calculated from a single, efficient radix sort operation.  

%Finer-grained reaction kernels may be a future development point of WARP.  
% okay, but that doesn't fit as the last sentence of the previous paragraph...I'd leave it out unless you want to talk more about what this might mean.

\subsubsection{Criticality Source}

% I think if you explain all of this up where I was confused about what you were doing with the yields that would help. this is a much clearer version of what came before.

As was mentioned previously, when the simulation is run in criticality mode, the source points for neutron batches depend on the fission points in the previous batch.  But if $k_\mathrm{eff}$ is not equal to one, each source neutron no longer corresponds to a secondary neutron.  This is the same problem in the time-independent neutron transport equation, where getting rid of the time-dependent term causes the equation to be inconsistent if the multiplication factor is not one.  To remedy this, the fission source term is divided by the multiplication factor to force the equations to be consistent (assuming the multiplication factor is known).  Similarly, the fission yield vector of a batch of neutrons can be renormalized to artificially make the multiplication factor equal to unity by rearranging \eqref{k_eff_batch} as shown in \eqref{k_eff_rebase}, where $n$ is the batch number, $N_d$ is the length of the dataset, $y_j$ is the secondary neutron yield of particle $j$, and $N_f$ is the total number of secondary neutrons.%define N_s

\begin{equation}
\label{k_eff_rebase}
\sum_{j=0}^{N_d}  \frac{y_j }{k_{\mathrm{eff},n}} = \frac{N_{f,n}}{k_{\mathrm{eff},n}} \Rightarrow N_{s,n+1} = N_{s,n}
\end{equation}

Since WARP handless all neutrons with equal importance, yields must be integers, but $y_j/k_\mathrm{eff}$ rarely will be.  Stochastic rounding is used to round $y_j/k_\mathrm{eff}$ to the lower integer with probability $y_j/k_\mathrm{eff} - \mathrm{floor}(y_j/k_\mathrm{eff})$.  Since yields range from 2-5 neutrons, there will be many individual instances of each yield, and yields will approach the mean $y_j/k_\mathrm{eff}$.

Once the yield vector is rebased by dividing by $k_\mathrm{eff}$ and stochastically rounding to a nearest integer, a prefix sum is done on the yield vector.  The $j^{\text{th}}$ value of an exclusive prefix sum, or exclusive scan as CUDPP calls it, is sum of the array values before index $j$ as shown in \eqref{prefix_sum}.  Such a vector can be used in the pop kernel, which writes the secondary particle data into the next batch's initial data based on the yield vector.

\begin{equation}
p_j = \sum_0^{j-1} \: x_i
\label{prefix_sum}
\end{equation}

After the prefix sum is done, the source pop kernel is launched.  A thread is launched for every element of the history dataset, and if a thread encounters a yield value of zero, it returns.  If a nonzero value is encountered, the thread samples $y_j$ secondary particles and writes them into the history data from $p_j$ to $p_j+y_j$.  Since the yields have been rebased, this should either just fall short of or slightly over the total dataset size, $N_d$.  The yield vectors are immediately set to zero for all particles in the next generation.

Needing to store generational information to determine $k_\mathrm{eff}$ and the next generation's source distribution forces criticality calculations to use a batch-like transport approach.  Transport on the GPU becomes inefficient when few active neutrons remain in the dataset.  Formulating a way where generations did not need to be run in series could benefit the GPU greatly.  A potential way would be to associate generational information with neutrons, transporting them as in a fixed-source problem, then post-processing the results to determine $k_\mathrm{eff}$. 

\subsubsection{Fixed Source \& Subcritical Multiplication}

In fixed source mode, the secondary particles can be popped back into the active particle dataset at the end of the inner loop since generational information is not needed and only the system response to the pulse is desired. %what pulse?
 In this mode, the number of source neutrons is given as an input and, memory allowing, a dataset of source neutrons this size is made.  These neutrons and transported, and any secondary-producing reactions are popped back in to the actively transporting dataset.  This is done by performing a compaction operation on the \emph{completed} dataset indices and a prefix sum is done on the yield vector.  
 
A pop routine is used in a manner similar to a criticality source run, but the threads with a nonzero yield write into the indices where a neutron has been terminated. This is specified by writing into indices $p_j$ to $p_j+y_j$ of the compaction vector instead of $p_j$ to $p_j+y_j$ of the dataset itself.  This reactivates terminated particle data for transporting secondary neutrons.  The terminated neutron data is all replaced by data appropriately sampled for the secondary neutron reaction.  As mentioned previously, it is necessary to be subcritical in fixed source mode, or subsequent generations will grow instead of shrink and the total number of neutrons needing to be transported to calculate the response will diverge.

% Some sort of wrap up of this chapter would be nice. A quick summary saying that you selected all these methods so now you can efficiently do all the transport stuff on the GPU. 
