\chapter{Introduction}

Nuclear reactors have the highest energy density of any energy-producing technology currently available \cite{energy_density}.  This is because of their ultimate source of energy - the atomic nucleus.  Compared to the the atom as a whole, the nucleus is a very small, dense region that contains most of the atom's mass.  The nucleus is made of protons and neutrons and is held together by strongest elementary force known in nature - the strong nuclear force. The strong force binds atomic nuclei together and keeps matter stable.  

There are certain reactions that can happen to a nucleus that will cause it to become unstable, however.  When a nucleus of $^{235}$U absorbs a free neutron, there is a high probability that the nucleus will rapidly become unstable and split.  This splitting is called fission, and it produces a large amount of energy.  When the sums of the masses of the incident neutron, the fragments, and neutrons that result from fission are measured, they add up to less mass than that of the parent $^{235}$U atom and the added neutron.  Mass has actually been converted into energy \cite{krane, duderstadt}.  This conversion also occurs in chemical reactions.  If the masses of two atoms are measured before they chemically bond and after, the bonded compound will have a slightly smaller mass than the initial reactants.  This ``mass defect'' is the amount of mass that has been converted to energy in creating the new state, whether nuclear or atomic \cite{mass_energy}.  The electron has about $2\times10^3$ times less mass than a proton or neutron, and the forces involved in atomic bonds are weaker than those in nuclear bonds, so the energetics of atomic reactions are about $10^6$ times less than that of nuclear reactions \cite{krane}.  Since a nucleus must be present in both atomic and nuclear reactions, the reactant mass is always dictated by the nucleus, but nuclear reactions release much more energy than atomic reactions.  This is the reason nuclear fission produces so much energy from such a small mass compared to chemical energy sources like burning natural gas or coal.   $^{235}$U, for example, releases a total of 192.9$\pm$0.5 MeV per fission \cite{duderstadt}.  Compared to chemical energy sources where the energy released per reaction is on the order of 1 eV.  Fission energy yield is 8 orders of magnitude larger.
% You need a sentence explaining how chemical energy sources work so it is clear how they are different.  -  added a few :)

Most of the energy released from fission ends up as kinetic energy of the large fission fragments, and is quickly converted to heat a short distance from the fission site.  This heat can be used to perform many tasks, but arguably the most useful task is to drive a thermodynamic cycle to convert much of the heat into electricity.  Since the specific energy of nuclear fuel is around six orders of magnitude larger than chemical sources, the mass and volume of fuel needed to run a nuclear is orders of magnitude smaller, and the power cycle produces much less waste mass, even though its waste is very radioactive \cite{waste_mass}.  

As with anything very powerful, nuclear technology must be handled with great responsibility.  The reactor's behavior must be accurately predictable by designers and well-characterized for operators to prevent accidents that could release radioactivity into the open environment as well as to make sure the power plant is a reliable source of clean, affordable electricity.  To ensure this required reliability and safety, accurate simulations are needed to predict what will happen to the reactor if conditions within it change.  Since reactors are  expensive machines \cite{nuclear_cost}, accurate simulations are needed in the design phase as well; accurate enough to provide confidence to designers, regulators, and the public at large that a reactor will be safe before constructing a demonstration plant.  

A common and accurate way to conduct any of these simulations is to solve the neutron transport equation using Monte Carlo methods.  The Monte Carlo Method requires few approximations to be made in the simulation model, and therefore can produce physically-accurate results.  However, Monte Carlo is much more computationally expensive than deterministic methods.  Typically, Monte Carlo simulations need to be run on supercomputers to produce results in a reasonable amount of time for problems relevant to engineering.

General purpose graphical processing units (GPGPUs, referred to as GPUs henceforth) are an emerging computational tool, sporting higher memory bandwidth and computational throughput as well as lower power consumption per operation compared to central processing units (CPU), the standard type of computer processor.  GPUs are touted as being ``massively parallel,'' home to thousands of computational ``cores,'' and capable of turning a desktop into a ``personal supercomputer.''  Some applications can see upwards of a hundredfold speedup by running on GPUs \cite{nvidia_speedups}. These factors make GPUs very attractive to use in extremely parallel, computationally-intensive simulations like Monte Carlo neutron transport, where trillions or more independent neutron histories are tracked.   

Some argue that the speedup gains are an illusion, and multicore CPUs are more than capable of similar performance if enough optimization is done \cite{debunk}. Some think that adding another programming paradigm is the wrong direction for computer science, and that it would be better if more resources were invested into existing technologies instead of spreading resources more thinly on new ones \cite{fewer_lang}.  

A key argument against both of these concerns is that learning a new language is not required. CUDA (Compute Unified Device Architecture, NVIDIA's parallel computing platform \cite{cuda}) adds minimal extensions to the C programming language, and is therefore easy to program in if a developer already knows C. Another argument is that substantial performance gains can be seen without much detailed optimization.  Getting speedups of 100x \emph{will} require thorough optimization, but getting 10x speedup of a serial code is commonly seen with little to no algorithmic changes of CPU code.  This is because GPUs were developed to be parallel from their inception, unlike CPUs, and GPU hardware is able to hide much of the details about how the parallel computations actually execute.  CUDA allows a developer to write scalar code (as opposed to vector code), and CUDA maps this to vectors on the hardware \cite{cuda}.  In addition, an empirical  bandwagon argument can be made for at least attempting to port codes to the GPU.  Many developers are porting and seeing reasonable speedups \cite{nvidia_speedups}, which is more than enough reason to at least attempt using CUDA for Monte Carlo neutron transport. 

\section{Why Monte Carlo?}

When applied to neutron transport, the central concept of the Monte Carlo Method is to directly simulate what microscopically happens to neutrons in nature by tracking every interaction they undergo, from birth to death.  Once a sufficiently-large number of these ``histories'' are completed, sums and/or averages are taken over certain attributes to determine aggregate, macroscopic behavior \cite{mcnp}.  Directly simulating what every individual neutron is doing is a rather brute-force way of implementing this method since the macroscopic behavior is what matters in the end.   The benefit of the brute-force strategy is that very few assumptions have to be made, giving Monte Carlo the potential to be the most accurate way to simulate nuclear reactors.  

Conversely, deterministic methods discretize the spatial and energy domains, introducing approximations and leading to inaccuracies.  The main drawback in using the Monte Carlo Method, however, is that its convergence is governed by the central limit theorem. For many problems, obtaining sufficiently-low statistical error is slow compared to deterministic approaches \cite{jaakko,openmc}.  This is why any way of accelerating Monte Carlo methods is of interest to the nuclear engineering community and why GPUs are being studied in this work.  In other words, it is a simulation method that can greatly benefit from acceleration. 

\section{Why GPUs?}

GPUs have gradually increased in computational power from the small, job-specific boards of the early 1990s to the programmable powerhouses of today.  Compared to CPUs, they have a higher aggregate memory bandwidth, much higher floating-point operations per second (FLOPS), and lower energy consumption per FLOP \cite{cuda}.  Because one of the main obstacles in exascale computing is power consumption \cite{exascale}, many new supercomputing platforms are gaining much of their computational capacity by incorporating GPUs into their compute nodes.  In the November 2013 Top 500 list, there are 41 GPU-accelerated supercomputers, some of which gain 50\% of their computational capacity from GPU coprocessor cards \cite{top500}.  Supercomputers in the number two and six spots use GPUs as well.  Since CPU-optimized parallel algorithms are usually not directly portable to GPU architectures (or at least not without losing substantial performance), transport codes may need to be rewritten to execute efficiently on GPUs.  Unless this is done, nuclear engineers cannot take full advantage of these new supercomputers for reactor simulations.

Table \ref{gpu_cpu_comp} shows a breakdown of features of both an Intel i7 (Westmere-EP) CPU and an NVIDIA Tesla C2075 (Fermi) GPU \cite{cent, cpu_latency}.  An AMD CPU was used in this work, but the i7 has similar enough characteristics to be representative, and the data of interest was more readily available.  The AMD processor has similar values apart from the frequency, therefore comparisons made here are still valid.  

At first glance, it may appear that the GPU completely outstrips the CPU.  The GPU has higher single precision FLOPs, which indicates that the GPU can do work faster than the CPU.  The GPU also has a higher memory bandwidth, which implies that data can be loaded from memory at a higher rate.  Data is necessary to do calculations, and the higher arithmetic rate of the GPU would be wasted if the processors couldn't be fed with data at a high rate.  GPUs also have a higher concurrent thread capability.  ``Thread'' is the term used for the sequence of instructions given to a processor to complete a specified task.  

Typical Monte Carlo neutron transport algorithms are ``task-based.''  The thread will transport the neutron until it is terminated through absorption or by leaking out of the system.  The sequence of events the neutron undergoes from start to finish is called a neutron ``history.''  The basic unit of work for a thread in a task-based algorithm is a history.  Neutron histories are independent of each other and the algorithm is parallelized by running many histories in independent, parallel threads.  Thus, having as many parallel threads as possible should provide the greatest performance.  This parallelization method requires that the threads can execute completely independently of each other, i.e. the actions of one thread does not affect the other parallel threads in any way.  This is a MIMD (multiple instruction multiple data) way of executing threads.  In MIMD execution model, different threads can execute different instructions on their data at a point in time.  Threads appear to completely independent, and having threads at different points int he transport algorithm is not a problem since they are allowed to execute their own instructions.

It seems that GPU cards would be perfect for running Monte Carlo neutron transport because they can run large numbers of concurrent threads.  The concurrent thread number is based on the width of the processor's SIMD (single instruction multiple data) units, however.  SIMD is an execution model some processors use in order to lower the number of instructions needed per amount of computation done, which increases both power and computational efficiency \cite{simd_power}.  SIMD requires the same instructions to be carried out over every element in a concurrently-processed data vector.  From a thread standpoint, SIMD requires threads to execute the same instruction at a point in time.  If a set of threads does not execute the same instruction at the same time (the data they act on can be different), the GPU will serialize them.  The subset of threads executing the first instruction will all execute together, then the subset executing the second instruction will execute after them.  Monte Carlo typically breaks instruction regularity because of its conditional statements based on random numbers.  Therefore, if Monte Carlo algorithms are to be used on GPUs, they must be implemented in a manner that carefully takes into account the limitations of the GPU.

\begin{table}[h]
\centering
\caption[A comparison of an NVIDIA GPU and an Intel CPU]{A comparison of an NVIDIA GPU and an Intel CPU \cite{cent, cpu_latency,opteronperformance}.}
\label{gpu_cpu_comp}
\begin{tabular}{| l | r | r |}
\hline
Processor & Intel i7 (Westmere-EP) & NVIDIA Tesla C2075 (Fermi) \\
\hline
\hline
Processing Elements & 6 cores, 2 issue, & 14 cores, 2 issue, \\
& 4-way SIMD &  16-way SIMD  \\
\hline
Frequency & 3.46GHz &  1.15 Ghz \\
\hline
Resident Strands / Threads (max) & 48 & 21,504 \\
\hline
SP GFLOP/s & 166 & 1030 \\
\hline
Mem. Bandwidth &  32 GB/s & 144 GB/s \\
\hline
Global Latency & $\sim$50 clocks & 200-800 clocks  \\
\hline
FLOPs / byte & 5.2  & 7.2 \\
\hline
Register File & 6kB & 2MB \\
\hline
Local Storage / L1 Cache & 192 kB & 896 kB \\
\hline
L2 Cache & 1536 kB & 0.75 MB \\
\hline
L3 Cache & 12 MB & - \\
\hline
\end{tabular}
\end{table}

The memory subsystems of GPUs also function in a SIMD-like way.  In order to use the full memory bandwidth of the device, more than one piece of data must be loaded and used per transaction, and the only way multiple pieces of data can be loaded simultaneously is if they are adjacent in memory.  In other words, if a program requests a single piece of data at location $i$, then requests a piece at location $i+10$, these requests will be split into two separate transactions that yield only one data element each.  Two data elements in two transactions produces an effective bandwidth of one element per transaction time.   On the other hand, if the program requests data at $i$, $i+1$, $i+2$, and $i+3$, the entirety of the requested data can be retrieved in a single transaction.  Four data elements in a single transaction yields and effective memory bandwidth of four elements per transaction time, four times higher than the previous scenario.  Having a memory subsystem that handles requests in this way is not ideal for Monte Carlo methods, however.  Data is accessed in a very random way because of the random nature of the simulation, and requested data is unlikely to be adjacent.  This means the full memory bandwidth of the GPU will not be used unless this problem is mitigated in some way.
% Great explanation!

Another undesirable feature of the GPU is that they have very high global memory latency compared to a CPU.  Memory latency is the number of clock cycles, or amount of time, it takes for a data request to be fulfilled.  As Table \ref{gpu_cpu_comp} shows, the GPU's global memory latency is about an order of magnitude higher than the CPU's \cite{cpu_latency,cuda}.  
% Wait, isn't 200-800 clocks longer than 50 clocks? I'm confused... oops.  table was all messed up, fixed!
GPUs try to eliminate the effect of large global latency by pipelining memory access.  Pipelining means threads that have their data can execute as other threads are waiting for their data to load.  If many requests are known, the data can be continually loaded as threads start to execute their jobs.  The hope is that the jobs take longer than the memory loads, eventually all data arrives, and the later threads appear to have zero latency for their memory access.  This is why it is important for GPUs to have such a large number of concurrent threads.  It allows them to pipeline data access and minimize the impact of memory latency.

Another notable feature is that the GPU has a greater FLOPs/byte of memory bandwidth ratio than the CPU.  This implies that GPUs could be used to turn a compute-bound problem into a bandwidth-bound problem.  This may seem like a deficit, but the GPU has a higher maximum memory bandwidth, so even though a problem is bandwidth-bound on a GPU, it may still require less execution time than on a CPU.

\begin{table}[h]
\centering
\caption[Breakdown of the cost benefits of GPUs assuming maximum performance and linear CPU scaling]{Breakdown of the cost benefits of GPUs assuming maximum performance and linear CPU scaling \cite{cost_sheets1,cost_sheets2,c2075,opterondate,opteronperformance}}
\label{gpu_money}
\begin{tabular}{| l | r | r |}
\hline
Processor & 4x AMD Opteron 6172  & 3x NVIDIA Tesla C2075 \\
  & @ 2.1 GHz (processors only) &  (cards only)  \\
\hline
\hline
Approximate Price (Q2 2012)& \$8,000 & \$ 7,000 \\
\hline
Max.TeraFLOP & 0.8 & 3.1 \\
\hline
Price / GigaFLOP & \$10 & \$2.26 \\
\hline
Price / History Power & \$0.16 & \$0.11 \\
(assuming 10$^3$ histories/s & & \\
per core and 25x GPU speedup) & & \\
\hline
Thermal Power & 460 W & 675 W \\
\hline
Yearly electricity cost & \$252  & \$96  \\
per TeraFLOP (\$.05 / kWh)   & & \\
\hline
\end{tabular}
\end{table}

Table \ref{gpu_money} shows a cost comparison of an AMD Opteron 6128 CPU and an NVIDIA Tesla C2075 GPU \cite{cpu_latency,cuda}.  The prices shown are rounded values from purchases made by the UC Berkeley Department Nuclear Engineering \cite{cost_sheets1,cost_sheets2}.  The CPU price is for four Opteron 6172 processors, and the GPU price is for three NVIDIA Tesla C2075 cards.  These numbers are shown because the department's computer cluster contains CPU and GPU nodes with these configurations.  The main benefit from using GPUs is the substantially lower electric cost per FLOP, though the capital cost per amount of work can also be lower that an equivalent CPU configuration.  To put this in a nuclear engineering context, it is useful to compare cost per unit work done, where work done will be called ``history power'': histories run per second.  Assuming a GPU speedup factor of 25 (approximately the average value of the accelerated applications reported by NVIDIA \cite{nvidia_speedups}) and that CPU codes scale perfectly linearly, the price per history power of the GPU configuration shown in Table \ref{gpu_money} is about 31\% lower than that of CPU configuration.  This indicates that GPUs may have lower capital costs as well as lower power costs per history power if they are able to provide a speedup greater than 16 over serial CPU codes (this is where their history powers are equal under the stated assumptions). 
% I changed the history power discussion to be consistent with your table.

\section{Goals and Impacts}

Many supercomputers are incorporating GPUs into their nodes to gain efficiency, which is the main obstacle in exascale computing.  Monte Carlo simulations are very computationally intensive, but their use is often required to adequately characterize nuclear reactors.  To perform Monte Carlo computations in a reasonable amount of time, supercomputers are needed.  Monte Carlo codes written for use on CPUs cannot be directly run on GPUs, and no high-fidelity Monte Carlo neutron transport applications exist for the GPU.   The goal of this work is to develop a program, WARP, that accelerates accurate continuous energy neutron transport simulations in general, 3D geometries by using a GPU.  If the name ``WARP'' were an acronym, it would stand for ``weaving all the random particles,'' with the word ``weaving'' referring to the lock-step way in which ``all the random particles'', i.e. the neutrons, are sorted into coherent bundles and transported.  Using the word ``warp'' is also a nod to NVIDIA's terminology for a group of 32 concurrent threads.  

Even though GPU computing is still in its very early stages, developing WARP hedges risk against the nuclear engineering community's computational tools becoming under powered or even obsolete.  GPUs are common in personal computers and workstations.  Developing WARP will also expose a substantial amount of  computing power that couldn't otherwise be used and which could drastically reduce simulation times for people without access to ``traditional" supercomputers.  If WARP is able to produce accurate results around 20 times faster than a CPU code, the GPU could indeed be called a ``personal supercomputer.''   Many supercomputers have nodes with around 20 CPU cores in them, and a 20x speedup with the WARP would make have a GPU equivalent to having a supercomputer node in a personal computer.
%GPUs have CPU cores? This is the first time you use this language...  sorry, definitely confusing, fixed.

Running Monte Carlo simulations on GPUs is not a new idea.  Ever since they became programmable, people have been trying to take advantage of the GPU's highly parallel, high computational throughput features.  There are several codes that perform photon transport very well on GPUs \cite{henderson,archer}, and there have also been studies that ran criticality simulations on a GPU using event-based algorithms \cite{tianyu,tianyu_snamc,nelson}.  In the most recent study, only three reactions were considered, each reaction used an artificial, single-group cross section, and only hardcoded, simple geometries were possible \cite{tianyu,tianyu_snamc}.  An even earlier study attempted to use continuous-energy cross sections to perform criticality calculations, but did not use standard data, did not perform inelastic scattering according to the full ENDF laws, did not incorporate a general or scalable geometry representation, and did not implement an effective task-parallel algorithm \cite{nelson}.  Due to all of their various shortcomings, none of these code were able to produces results comparable to production Monte Carlo neutron transport codes.
% Is that true? What about professor Xu from RPI? I know that he does MC on GPU for medical applications...  I misspoke, there are no codes that do CE MC neutron transport.  They used single-group cross sections, and handling XS is where most of the work is!!!  Also, nobody did sorting or a global event based algorithm properly!

WARP sets itself apart from any previous endeavors in its breath of scope and its novel adaption of the event-based transport algorithm.  Previous codes have either used synthetic, simplified, and/or incomplete nuclear data.  WARP will loaded standard data files and accurately simulate each reaction type specified.  WARP will also use a flexible, scalable, and optimized geometry representation where previous studies have used simplified and restricted geometry models.  Previous works have examined event-parallel algorithms, but have not parallelized them effectively and therefore did not see the benefits of adopting such an algorithm on a GPU.  WARP will use highly-parallelized algorithms and slightly modify the original vision of the event-based algorithm to better suit execution on the GPU.  All of the previous event-based algorithms tried to implement a ``shuffle'' operation where neutron data was actually sorted into reaction-contiguous blocks \cite{nelson,tianyu_snamc}, similarly to vectorized Monte Carlo Methods developed decades ago \cite{vector,vujic_vector}.  WARP will eliminate the overhead of actually moving data by simply moving references to the data instead.  WARP also changes the unionized energy grid data format to reduce the number of data loads needed to scan cross sections.  In addition, an important part of WARP's development is also to determine if existing task-based Monte Carlo algorithms can be preserved or if they need to be modified in order to take full advantage of GPUs.

These features aim to make WARP a competitive Monte Carlo neutron transport code that efficiently executes on a GPU.  To validate WAPR's accuracy, it will be compared against MCNP and Serpent, two production Monte Carlo neutron transport codes.  WARP uses standard nuclear data files, can run in fixed source and criticality modes, can calculate integral parameters such as the multiplication factor, reaction rates, and fission rate distribution and other distribution functions such as the neutron spectrum.  Since WARP should be performing the same calculations on the same data, the runtimes of WARP compared to the reference codes can also be made.  Any comparison of GPU-generated results to production Monte Carlo neutron transport codes has never been done before, and WARP will be the first to do so.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reference Monte Carlo Neutron Transport Codes}

The main purpose in developing WARP is to accelerate Monte Carlo nuclear reactor simulations by using the computational power of GPUs.  To determine if WARP is successful in doing this, WARP will compared against Serpent \cite{jaakko} and MCNP \cite{mcnp}, two Monte Carlo neutron transport codes that are used in the nuclear engineering community.  Each code has different features and strengths and weaknesses in different areas. Comparing against both codes will certify that WARP is doing the correct calculations and determine if WARP's GPU implementations are effective in accelerating them.  

\subsection{MCNP}

Monte Carlo simulations were one of the first applications of early computers.   This is reflected in the correspondence of John von Neumann and  Robert Richtmyer  in 1947.  In his letter, von Neumann outlined how to use statistical calculations to solve the neutron diffusion equation.  Shortly after, Enrico Fermi invented FERMIAC11 at Los Alamos National Laboratory to track neutrons as they travelled through fissile materials by the Monte Carlo Method \cite{mcnp}.  With the introduction of Fortran in 1957, it became possible to write more general neutron transport code systems than writing directly in machine code, and MCNP has been developed (in various incarnations) at Los Alamos National Laboratory since 1963 \cite{mcnp}.
 
Despite the age of the project, MCNP tries to include all relevant advancements in the Monte Carlo Method, includes detailed and accurate physical models and data, and is the ``gold standard'' of Monte Carlo neutron transport codes.  MCNP has become a ``repository for physics knowledge'' and ``represents over 500 person-years of sustained effort'' \cite{mcnp}.   WARP is similar to MCNP in that MCNP reads ACE-formatted nuclear data and uses ray tracing to handle boundary crossings.  Serpent is capable of reproducing the results from MCNP very well \cite{jaakko}, but MCNP will be included in the comparisons to further validate the results.

\subsection{Serpent}

``Serpent is a three-dimensional continuous-energy Monte Carlo reactor physics burnup calculation code, developed at VTT Technical Research Centre of Finland since 2004. The publicly available Serpent 1 has been distributed by the OECD/NEA Data Bank and RSICC since 2009, and next version of the code, Serpent 2, is currently in a beta-testing phase and available to registered users by request'' \cite{serpent}.  It is written in ANSI C and makes use of MPI and OpenMP for parallelization \cite{jaakko}.

 According to the Serpent website, Serpent is suggested for use in ``spatial homogenization and group constant generation for deterministic reactor simulator calculations; fuel cycle studies involving detailed assembly-level burnup calculations; validation of deterministic lattice transport codes; full-core modeling of research reactors, SMR's, and other closely coupled systems; coupled multi-physics applications (Serpent 2);  and educational purposes and demonstration of reactor physics phenomena'' \cite{serpent}.  Serpent 1 has been extensively validated against standard nuclear reactor criticality benchmarks and typically compares very well with results given by MCNP and Keno-VI \cite{serpent}.

Like many Monte Carlo neutron transport codes (MCNP and Keno-VI included), Serpent uses a universe-based combinatorial solid geometry (CSG) model that determines which volume a neutron is in by using binary logic on second-order surfaces \cite{mcnp, serpent}.  Serpent uses a combination of ray tracing and the Woodcock delta-tracking method to track neutron movements, which is different from what many Monte Carlo codes do \cite{serpent}.  
% I think this is true and what you meant by however.
The Woodcock method ``has proven efficient and well suited for geometries where the neutron mean-free-path is long compared to the dimensions, which is typically the case in fuel assemblies, and especially in HTGR particle fuels'' \cite{serpent}, and limits excess geometry sampling that can occur when strong absorbers are present.

Serpent also brought the use of a unionized energy grid format for the cross section and reaction data into the mainstream.  The native way nuclear data is formatted gives each isotope cross sections a unique energy grid.  It is done this way so the minimum number of data points is included in the cross section to achieve an accuracy level.  Since each isotope's grid is different, a search must be done on it to determine what data points a neutron's energy lies between.  When there are many isotopes present, doing many energy searches to look up cross sections can become expensive.  Serpent's method is to unionize all the energy grid for all the isotopes so they all have the same energy structure and only a single grid search is needed \cite{jaakko}. 
% You might want to explain what a unionized energy grid format is.  done.
  This reformatting of the nuclear data regularizes the access of the data and increases performance, but comes at the cost of a larger memory footprint.  Serpent was not the first code to use this type of data structure \cite{vector}, but is the first widely-used code to do so.  Serpent can also perform cross section preprocessing at arbitrary temperatures for more accurate calculations with minimal computational cost. Serpent also does on-the-fly Doppler broadening from 0K data, so only one set of nuclear data needs to be stored in memory (though this strategy has an additional computational cost) \cite{serpent}.

%Serpent has also been extensively validated against benchmark problems as well as against other Monte Carlo codes like MCNP and Keno-VI.  It shows good agreement  with the reference cases, but can sometimes have statistically significant differences from the other codes, depending on the problem.  Despite this, Serpent is very accurate for most problems \cite{serpent}.
% You've already said this.

\section{Outline}

Chapter 2 will cover background material that was needed in the development of WARP.  It covers nuclear physics, nuclear reactor analysis, and the kinematics involved in neutron transport.  Then, the mathematics behind the governing equations is discussed, the solution method explained, and how the required probability distributions are sampled is detailed.  After the physics and mathematics sections, the computer hardware is discussed as is the OptiX ray tracing framework, which is the major library used in WARP.  The chapter concludes with an overview of the previous research of Monte Carlo neutron transport on GPUs and how WARP fits into the landscape.
%above you said no one has done this, but now you're talking about past work?  resolved.

Chapter 3 discusses the actual routine implementations used on the GPU in WARP.  It first goes over the exploratory studies done in preparation for developing WARP.  These studies show the algorithmic benefits of a very important feature of WARP--remapping the data references.  It also goes over how OptiX execution is optimized for best performance in reactor-like geometries.  After the preliminary studies, the data layout for cross sections is explained and its similarities and differences from Serpent pointed out.  The last topic discussed is the CUDA kernels written by hand for WARP.  These routines process the neutrons as they travel through the problem geometry and provide the ``glue'' to connect all the important tasks that WARP requires to processing the neutron data.
%You might want to take a moment here to talk about which of these things are novel. I know you'll go into it in the section, but clearly laying out in the intro what is "my contribution" to the field is helpful to saying "yeah, this guy should get a PhD".  Good point.  I put this in the goals and impacts section instead of here.

Chapter 4 discusses the initial results produced by WARP.  Four ``benchmark'' criticality cases are shown, each highlighting different requirements of the transport routines.  For criticality source runs, the flux spectra, multiplication factors, and runtimes are compared to those from Serpent and MCNP.  Fission source distributions from WARP are compared with Serpent only.  A single fixed source calculation is shown to illustrate WARP's capabilities, and is compared with Serpent and MCNP.  In the conclusion of the Chapter, the runtimes of remapping and non-remapping versions of WARP are compared to determine the necessity of remapping the neutron data references.

The final chapter draws conclusions from the previous chapters.  In addition to drawing conclusions about the best-performing algorithms and configurations for conducting continuous energy Monte Carlo neutron transport on GPUs, the success of WARP in completing this work's initial goals is discussed.  After these concluding remarks, a future roadmap for WARP is proposed, and the amount of work needed to be done to make WARP fully featured and reliable enough to gain community acceptance is enumerated.





