\chapter{Introduction}

Nuclear reactors have the highest energy density of any energy-producing technology currently available [cite].  This is due to their ultimate source of energy - the strong nuclear force, the strongest elementary force known in nature. The strong force binds atomic nuclei together and keeps matter stable.  When a heavy nucleus splits, as it does when a free neutron is absorbed by a fissile nucleus, it's daughter nuclei are more tightly bound than the parent, and the excess binding energy is released in various forms, most of which is deposited as heat near the split nucleus.  This heat can used to perform many tasks, but arguably the most useful task is to drive a conversion cycle used to convert a substantial amount of the heat into electricity.  Since the specific energy of nuclear fuel is around six orders of magnitude larger than chemical sources, the same fuel can be used in reactors for years, and the power cycle produces much less waste mass, even though its waste is very radioactive.  

As with anything very powerful, nuclear power must be handled with great responsibility.  The reactor's behavior must be very predictable and very well characterized in order for operators to prevent anything from going wrong and potentially releasing radioactivity into the open environment as well as to make sure the power plant is a reliable source of clean, affordable electric energy.  To ensure this required reliability and safety, very accurate simulations are needed in order to predict what will happen to the reactor if conditions within it change.  Since reactors are very expensive machines, accurate simulations are needed in the design phase as well; accurate enough to provide confidence to designers, regulators, and the public at large that a reactor will be safe before constructing a demonstration plant.  A popular way to conduct these simulations is to apply the Monte Carlo method to the neutron transport equations.  The Monte Carlo method requires few approximations to be made in the simulation model, and therefore can produce very physically-accurate results.  However, it is much more computationally expensive than other methods, and most often Monte Carlo simulations need to be run on supercomputers to produce results in a reasonable time for problems relevant to engineering.

General purpose graphical processing units (GPUs) are an emerging computational tool, sporting very high memory bandwidth and computational throughput as well as lower power consumption per operation compared to standard CPUs.  They are touted as being ``massively parallel,'' home to thousands of computational ``cores,'' and capable of turning a desktop into a ``personal supercomputer.''  This makes them very attractive to use in extremely parallel, computationally-intensive simulations like Monte Carlo neutron transport where trillions, or more, of independent neutron lives are tracked.  

In this study, a simple question is proposed and answered: can the existing Monte Carlo algorithms be preserved or will they need to be rewritten in order to take full advantage of these new, powerful processor architectures?

\section{Why Monte Carlo?}

When applied to neutron transport, the central concept of the Monte Carlo method is to directly simulate what  microscopically happens to the neutrons in nature; every interaction they undergo, from birth to death.  Once a sufficiently large number of these ``histories'' are completed, sums and/or averages are taken over certain attributes to determine aggregate, macroscopic behavior.  Directly simulating what every individual neutron is doing is a rather brute-force way of simulating things since the macroscopic behavior is what matters in the end, but very few assumptions have to be made to conduct Monte Carlo simulations, making them one of the most accurate ways to simulate nuclear reactors.  There is one huge drawback in using the Monte Carlo Method, however.  It is a statistical way of simulation and is subject to the laws of probability, mainly the central limit theorem stipulating slow convergence compared to deterministic approaches.  This is why any way of accelerating them is of interest and why GPUs are being studied in this work.

\section{Why GPUs?}

GPUs have gradually increased in computational power from the small, job-specific boards of the early 90s to the programmable powerhouses of today.  Compared to CPUs, they have a higher aggregate memory bandwidth, much higher floating-point operations per second (FLOPS), and lower energy consumption per FLOP.  Because one of the main obstacles in exascale computing is power consumption, many new supercomputing platforms are gaining much of their computational capacity by incorporating GPUs into their compute nodes.  Since CPU optimized parallel algorithms are usually not directly portable to GPU architectures (or at least without losing substantial performance), transport codes may need to be rewritten in order to execute efficiently on GPUs.  Unless this is done, nuclear engineers cannot take full advantage of these new supercomputers for reactor simulations.

Conventional CPU-based parallel Monte Carlo algorithms typically use the approach of one particle history per CPU thread.  Data is accessed when needed and threads execute completely independently of each other.  This is a task-parallel approach, the typical algorithm for which is shown in Figure 1, and works well on CPUs due to their large cache and control sizes (relative to cores) and the lower latency of accessing global memory (smaller penalty for random access).  This approach has been historically called history-based parallelism as well.  To fully utilize the computational power of GPUs, this approach must be modified.  Threads need to execute identical instructions and therefore must be in the same stage of the transport algorithm.  Studies have been done trying to directly port task-parallel Monte Carlo algorithms to GPUs with some success, most notably that of Liu, et. al. [10], and they suggest that a data-parallel approach similar to those used in vector computers should be employed [1, 10].  This approach is also called event-based parallelism since events, such as scattering or surface crossing, are done in parallel for many particles at once.

Figure 2 shows the data parallel algorithm used to maintain thread coherence (i.e. executing the same instructions) between GPU threads.  It is similar to the vectorized approach used with the early vector computers, which also were able to use a single-instruction, multiple-data (SIMD) execution model [1, 2].  A single-instruction multiple-thread (SIMT) and SIMD are similar in the sense that identical instructions are carried out across different pieces of data, but SIMT allows threads to act independently, albeit with a performance penalty.   The two main features of Figure 2 are that independent GPU kernel launches are used for each step of the transport chain and that the kernels operate on a large set of particle data (as opposed to transporting one particle at a time).  By doing this, the transport cycle is broken into steps in which a single task, such as determining reaction type or finding boundary intersection, is performed across large dataset of particles and threads can be kept more coherent.

\section{Added Value}

The aim of this work is first to produce a program that runs accurate continuous energy neutron transport simulations on the GPU in general 3D geometries using standard nuclear data files.  The second goal is to make these simulations as fast as possible.  Producing this program, which has been named WARP, will be the first step in hopefully creating a full featured reactor simulation program that runs on GPU accelerator cards.  The added value in doing this comes from the fact that many supercomputers are gaining power from GPUs and in order for nuclear engineers to take advantage of this new computing power, which they always need more of, a new code must be written to run on them.  Even though GPU computing is still in its very early stages, developing WARP hedges risk for the simulation community against their computational tools becoming under powered or even obsolete.  Conversely, it also exposes a substantial amount of  computing power that couldn't otherwise be used on personal computers, workstations, and laptops.  Targeting GPUs could enable smaller computers to take on substantial reactor simulation tasks, and potentially reduce design iterations for people without access to supercomputers.  Other than being a nod to NVIDIA's terminology, if WARP were an acronym, it would stand for ``weaving all the random particles,'' with the words ``weaving all'' referring to the lockstep way in which ``all the random particles'', i.e. the neutrons, are sorted into coherent bundles and transported.

\section{Objective}


\section{Outline}

1 paragraph summaries of each chapter.  will do once they are written!
