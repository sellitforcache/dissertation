\chapter{Introduction}

Nuclear reactors have the highest energy density of any energy-producing technology currently available [cite].  This is due to their ultimate source of energy - the strong nuclear force, the strongest elementary force known in nature. The strong force binds atomic nuclei together and keeps matter stable.  When a heavy nucleus splits, as it does when a free neutron is absorbed by a fissile nucleus, it's daughter nuclei are more tightly bound than the parent, and the excess binding energy is released in various forms, most of which is deposited as heat near the split nucleus.  This heat can used to perform many tasks, but arguably the most useful task is to drive a conversion cycle used to convert a substantial amount of the heat into electricity.  Since the specific energy of nuclear fuel is around six orders of magnitude larger than chemical sources, the same fuel can be used in reactors for years, and the power cycle produces much less waste mass, even though its waste is very radioactive.  

As with anything very powerful, nuclear power must be handled with great responsibility.  The reactor's behavior must be very predictable and very well characterized in order for operators to prevent anything from going wrong and potentially releasing radioactivity into the open environment as well as to make sure the power plant is a reliable source of clean, affordable electric energy.  To ensure this required reliability and safety, very accurate simulations are needed in order to predict what will happen to the reactor if conditions within it change.  Since reactors are very expensive machines, accurate simulations are needed in the design phase as well; accurate enough to provide confidence to designers, regulators, and the public at large that a reactor will be safe before constructing a demonstration plant.  A popular way to conduct these simulations is to apply the Monte Carlo method to the neutron transport equations.  The Monte Carlo method requires few approximations to be made in the simulation model, and therefore can produce very physically-accurate results.  However, it is much more computationally expensive than other methods, and most often Monte Carlo simulations need to be run on supercomputers to produce results in a reasonable time for problems relevant to engineering.

General purpose graphical processing units (GPGPUs, referred to as GPUs henceforth) are an emerging computational tool, sporting very high memory bandwidth and computational throughput as well as lower power consumption per operation compared to standard CPUs.  Some applications can see upwards of a hundredfold speedup by running on GPUs.  They are touted as being ``massively parallel,'' home to thousands of computational ``cores,'' and capable of turning a desktop into a ``personal supercomputer.''  This makes them very attractive to use in extremely parallel, computationally-intensive simulations like Monte Carlo neutron transport where trillions, or more, of independent neutron lives are tracked.   

Some argue that the speedups gain are an illusion, and multicore CPUs are more than capable of similar performance if enough optimization is done. Some think that adding another programming paradigm is the wrong direction for computer science, and that it would be better if more resources were invested into existing technologies instead of spreading resources more thinly on new ones.  The an argument against both of these concerns is that CUDA (Compute Unified Device Architecture, NVIDIA's parallel computing platform) is fairly easy to program in and see substantial performance gains.  Maybe not 100x, but getting 10x is fairly easy.  This is due to the fact that GPUs were developed to be parallel from the beginning, unlike CPUs.  There is much less clutter and layering visible to the programmer, and CUDA is basically C with a few additional decorators, hardly a new language.   In addition, an empirical  bandwagon argument can be made for at least attempting to port codes to the GPU.  Many developers are porting and seeing reasonable speedups without any advanced training, more tat enough reason to at least attempt using CUDA. 

In this study, a simple question is proposed and answered: can the existing Monte Carlo algorithms be preserved or will they need to be rewritten in order to take full advantage of these new, powerful processor architectures?

\section{Why Monte Carlo?}

When applied to neutron transport, the central concept of the Monte Carlo method is to directly simulate what  microscopically happens to the neutrons in nature; every interaction they undergo, from birth to death.  Once a sufficiently large number of these ``histories'' are completed, sums and/or averages are taken over certain attributes to determine aggregate, macroscopic behavior.  Directly simulating what every individual neutron is doing is a rather brute-force way of simulating things since the macroscopic behavior is what matters in the end, but very few assumptions have to be made to conduct Monte Carlo simulations, making them one of the most accurate ways to simulate nuclear reactors.  Deterministic methods must discretize the spatial and energy domains, inevitably leading to unavoidable inaccuracies.  There is one huge drawback in using the Monte Carlo Method, however.  It is a statistical way of simulation and is subject to the laws of probability, especially the central limit theorem which stipulates slow convergence compared to deterministic approaches.  This is why any way of accelerating them is of interest and why GPUs are being studied in this work.  In other words, it is a simulation method that has a great need for acceleration. 

\section{Why GPUs?}

GPUs have gradually increased in computational power from the small, job-specific boards of the early 90s to the programmable powerhouses of today.  Compared to CPUs, they have a higher aggregate memory bandwidth, much higher floating-point operations per second (FLOPS), and lower energy consumption per FLOP.  Because one of the main obstacles in exascale computing is power consumption, many new supercomputing platforms are gaining much of their computational capacity by incorporating GPUs into their compute nodes.  In the Novermber 2013 Top500 list, there are 41 GPU-accelerated supercomputers, some of which gain 50\% of their computational capacity from GPU coprocessor cards \cite{top500}.  Supercomputers in the number two and six spots use them as well.  Since CPU optimized parallel algorithms are usually not directly portable to GPU architectures (or at least without losing substantial performance), transport codes may need to be rewritten in order to execute efficiently on GPUs.  Unless this is done, nuclear engineers cannot take full advantage of these new supercomputers for reactor simulations.

Table \ref{gpu_cpu_comp} shows a breakdown of features of both an AMD Opteron Magny-Cours CPU and an NVIDIA Tesla C2075 GPU.  At first glance, it may appear that the GPU completely outstrips the CPU.  It has higher single precision FLOPs (Floating Point OPeration), higher memory bandwidth, and higher concurrent thread capability.  These are all great features, and it seems that these cards would be perfect for running Monte Carlo neutron transport, especially due to the number of threads they can concurrently run.  The concurrent thread number is based on the width of the processor's SIMD lanes, however.  SIMD, or Single Instruction Multiple Data, is a way some processors run in order to lower then number of instructions needed per amount of computation done, which increases both power and computational efficiency.  SIMD requires the same instructions to be carried out over every element  in a concurrently-processed data vector, and Monte Carlo typically breaks this instruction regularity due to its conditional statements based on random numbers.  Therefore, if Monte Carlo algorithms are to be use on GPUs, they must be execute in a careful manner taking into account the limitations of the GPU.

\begin{table}[h]
\centering
\caption{A comparison of a NVIDIA GPU and an Opteron CPU \cite{cent}.}
\label{gpu_cpu_comp}
\begin{tabular}{| l | r | r |}
\hline
Processor & Intel Westmere-EP (i7) & NVIDIA Tesla C2070 (Fermi) \\
\hline
\hline
Processing Elements & 6 cores, 2 issue, & 16 cores, 2 issue, \\
& 4-way SIMD &  16-way SIMD  \\
\hline
Frequency & 3.46GHz &  1.54 Ghz \\
\hline
Resident Strands / Threads (max) & 48 & 24576 \\
\hline
SP GFLOP/s & 166 & 1577 \\
\hline
Mem. Bandwidth &  32 GB/s & 192 GB/s \\
\hline
Global Latency & 200-800 clocks & ~50 clocks \\
\hline
FLOPs / byte & 12.5  & 16.1 \\
\hline
Register Fils & 6kB (?) & 2MB \\
\hline
Local Storage / L1 Cache & 192 kB & 1024 kB \\
\hline
L2 Cache & 1536 kB & 0.75 MB \\
\hline
L3 Cache & 12 MB & - \\
\hline
\end{tabular}
\end{table}

Another considerably undesirable feature of the GPU is that they have very high global memory latency compared to a CPU.  As Table \ref{gpu_cpu_comp} shows, the Fermi card's global memory latency is about an order of magnitude higher than the Operton's \cite{cpu_latency,cuda}.  Since data is accessed in a very random way in Monte Carlo simulations, access regularity can again be broken, and the global access latency could slow a simulation down significantly.  GPUs try to eliminate the effect of their large global latency by pipelining memory access as well as relaying on access regularity.  Pipelining means threads that have their data can execute as other threads are waiting for their data to load.  If many requests are known, the data can be continually loaded as threads start to execute their jobs.  The hope is that the jobs take longer than the memory loads, and eventually all data arrives, and the later threads appear to have zero latency for their memory access.  This is why it is important for GPUs to have such a large number of concurrent threads.  It allows them to pipeline data access and minimize the impact of memory latency.

Another notable feature is that the GPU has greater FLOPs /byte of memory bandwidth ratio.  This implies that GPUs could be used to turn a compute-bound problem into a bandwidth-bound problem.  This may seem like a deficit, but the GPU does, after all, have a higher maximum memory bandwidth, so even though a problem is memory-bound on a GPU, it may still perform better than on a CPU.

Table \ref{gpu_money} shows a comparison of an AMD Opteron Magny-Cours CPU and an NVIDIA Tesla C2075 GPU \cite{cpu_latency,cuda}.  This prices shown are rounded values from purchases made by the UC Berkeley Department Nuclear Engineering.  The CPU price is for a complete server, basically CPUs and RAM.  The disk and  other components are a secondary cost.  The price shown for the GPUs are for the cards only since they can live in a very cheap host server with very little RAM and budget CPUs.  The main benefit from using GPUs is not the capital price per FLOP, but rather the substantially lower electric cost per FLOP.  Also, assuming that a GPU application performs 25 times faster than a single CPU code and the CPU code scales linearly, the capital price per Monte Carlo ``history power,'' or histories run per second, is about 3 times that of CPUs.  Again, this assumes 25 times speedup over a CPU code.  

\begin{table}[h]
\centering
\caption{Breakdown of the cost benefits of GPUs}
\label{gpu_money}
\begin{tabular}{| l | r | r |}
\hline
Processor & 4x Opteron 12-core  & 3x NVIDIA TESLA C2075 \\
  & @ 2.1 GHz &  (cards only)  \\
\hline
\hline
Approximate Price (Q2 2012)& \$10,000 & \$6,000 \\
\hline
Max.TeraFLOP & 0.4 & 3.1 \\
\hline
Price / GigaFLOP & \$25 & \$1.94 \\
\hline
Price / History Power (10$^3$ h / s) & \$1.00 & \$0.38 \\
(assuming 25x GPU speedup) & & \\
\hline
Thermal Power & 225W & 130W \\
\hline
Yearly electricity cost & \$503.70  & \$95.37 \\
per TeraFLOP (\$.05 / kWh)   & & \\
\hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Standard Monte Carlo Neutron Transport Codes}

The main purpose in developing WARP is to accelerate Monte Carlo nuclear reactor simulations by using the computational power of GPUs.  To know if WARP is successful in doing this, it will compared against Serpent and MCNP, two Monte Carlo neutron transport codes that are used in the nuclear engineering community.  Each code has different features and strengths and weaknesses in different areas.  Comparing against these two codes will certify the accuracy of WARP's results and determine if WARP's GPU implementations are effective in accelerating the calculations.

\subsection{MCNP}

Monte Carlo simulations were one of the first applications of early computers.   This is reflected in the correspondance of John von Neumann and  Robert Richtmyer  in 1947.  In his letter, von Neumann outlined how to use statistical calculations to solve the neutron diffusion equation.  Shortly after, Enrico Fermi invented FERMIAC11 at Los Alamos National Laboratory to track neutrons as they travelled through fissile materials by the Monte Carlo Method \cite{mcnp}.  With the introduction of Fortran in 1957, it became possible to write more general neutron transport code systems than writing directly in machine code, and MCNP has been developed in (various incarnations) at Los Alamos National Laboratory since 1963 \cite{mcnp}.
 
Despite the age of the project, MCNP tries to include all relevant advancements in the Monte Carlo Method, includes detailed and accurate physical models and data, and is the ``gold standard'' of Monte Carlo neutron transport codes.  MCNP has become a ``repository for physics knowledge'' and ``represents over 500 person-years of sustained effort \cite{mcnp}.''   WARP is similar to MCNP in that MCNP reads ACE-formatted nuclear data and uses ray tracing to handle boundary crossings.  Serpent is capable of reproducing the results from MCNP very well, but MCNP will be included in the comparisons to further validate the results.

\subsection{Serpent}

``Serpent is a three-dimensional continuous-energy Monte Carlo reactor physics burnup calculation code, developed at VTT Technical Research Centre of Finland since 2004. The publicly available Serpent 1 has been distributed by the OECD/NEA Data Bank and RSICC since 2009, and next version of the code, Serpent 2, is currently in a beta-testing phase and available to registered users by request \cite{serpent}.''

 According to the Serpent website, Serpent is suggested for use in ``spatial homogenization and group constant generation for deterministic reactor simulator calculations, fuel cycle studies involving detailed assembly-level burnup calculations, validation of deterministic lattice transport codes, full-core modeling of research reactors, SMR's, and other closely coupled systems, coupled multi-physics applications (Serpent 2),  and educational purposes and demonstration of reactor physics phenomena \cite{serpent}.''  Serpent 1 has been extensively validated against standard nuclear reactor criticality benchmarks as well as normally comparing very well with results given by MCNP and Keno-IV \cite{serpent}.

Like many Monte Carlo neutron transport codes (MCNP and Keno-IV included), Serpent uses a universe-based combinatorial solid geometry (CSG) model which determines which volume a neutron is in by using binary logic on second-order surfaces \ref{mcnp, serpent}.  Serpent uses a combination of ray tracing and the Woodcock delta-tracking method to track neutron movements, however \cite{serpent}.  The Woodcock method and  ``has proven efficient and well suited for geometries where the neutron mean-free-path is long compared to the dimensions, which is typically the case in fuel assemblies, and especially in HTGR particle fuels \cite{serpent}.'' 

The combination of two tracking methods overcomes the efficiency problems normally encountered with delta-tracking in the presence of localized heavy absorbers (Leppänen, 2010b).

The main drawback of delta-tracking is that the track-length estimate of neutron flux is not available and reaction rates have to be calculated using the potentially less-efficient collision estimator. This is usually not a problem in reactor calculations when reaction rates are scored in regions of high collision density. However, the efficiency of the collision estimator becomes poor in small or optically thin volumes located far or isolated from the active source. Methods for obtaining better statistics for detector calculations are being developed for Serpent 2, but in general the code is not the best choice for shielding applications.

Unionized energy grid format

Continuous-energy cross sections in the library files are reconstructed on a unionized energy grid, used for all reaction modes (Leppänen, 2009b). The use of a single energy grid results in a major speed-up in calculation, as the number of CPU time consuming grid search iterations is reduced to minimum. Macroscopic cross sections for each material are pre-generated before the transport simulation. Instead of calculating the cross sections by summing over the constituent nuclides during tracking, the values are read from pre-generated tables, which is another effective way of improving the performance.

The drawback of the unionized energy grid approach is that computer memory is wasted for storing redundant data points. The grid size may become prohibitively large in burnup calculations, often involving over 250 actinide and fission product nuclides. To overcome this issue, Serpent 2 provides different optimization modes for small and large burnup calculation problems, in which the the unionized energy grid approach is used selectively (Leppänen, 2012a).

Doppler-broadening of cross sections

A built-in Doppler-broadening preprocessor routine allows the conversion of ACE format cross sections into a higher temperature. This capability results in a more accurate description of the interaction physics in temperature-sensitive applications, as the data in the cross section libraries is available only in 300K intervals. The method has been validated with good results and the routine works efficiently without a major increase in the overall calculation time (Viitanen, 2009). In addition to the pre-processor routine, Serpent 2 has the option to adjust nuclide temperatures on-the-fly (see description of on-going work in Serpent 2) . Both methodologies are currently limited to continuous-energy cross sections, and do not cover unresolved resonance probability tables or S(?,?) scattering data.

Burnup calculation

The burnup capability in Serpent is entirely based on built-in calculation routines, without any external coupling. The number of depletion zones is not restricted, although memory usage becomes a limiting factor for Serpent 1 when the number of burnable materials is large.

Fission products and actinide daughter nuclides are selected for the calculation without additional user effort and burnable materials can be sub-divided into depletion zones automatically. The irradiation history is defined in units of time or burnup. Reaction rates are normalized to total power, specific power density, flux or fission rate and the normalization can be changed by dividing the irradiation cycle into a number of separate depletion intervals. Volumes and masses needed for the normalization are calculated automatically for simple geometries, such as 2D fuel pin lattices. The values can also be obtained from a Monte Carlo based volume calculation routine or entered manually. Predictor-corrector calculation is optional and used by default for each burnup step. In addition to the conventional predictor-corrector method based on linear interpolation, Serpent 2 offers various higher-order methods and sub-step solutions for burnup calculation (Isotalo, 2011b; 2011c; 2013b).

Radioactive decay and fission yield data used in the calculation is read from standard ENDF format data libraries. The decay libraries may contain data for almost 4000 nuclides and meta-stable states, all of which is available for the calculation. The total number of different nuclides produced from fission, transmutation and decay reactions is generally lower, in the order of 1500. The concentrations of all included nuclides with decay data are tracked in the burnup calculation, and the number of nuclides with cross sections typically ranges from 200 to 300. Energy-dependent fission yields are available for all main actinides (31 nuclides in ENDF/B-VII data).

Integral one-group transmutation cross sections are calculated either within the transport cycle, or by collapsing the continuous-energy reaction cross sections after the cycle using a flux spectrum collected on the unionized energy grid. The spectrum collapse method speeds up the calculation by a factor of 3-4, and due to the high energy resolution of the flux spectrum, the errors in the results are practically negligible. Similar methodology has been successfully used with coupled Monte Carlo burnup calculation codes (Haeck, 2007; Fridman, 2008a; 2008b) before it was implemented in Serpent.

Serpent has two fundamentally different options for solving the Bateman depletion equations. The first method is the Transmutation Trajectory Analysis (TTA) method (Cetnar, 2006), based on the analytical solution of linearized depletion chains. The second option is the Chebyshev Rational Approximation Method (CRAM), an advanced matrix exponential solution developed for Serpent at VTT (Pusa, 2010; 2011; 2012;. 2013a;. 2013b;. 2013c). The two methods have shown to yield consistent results, both when used with Serpent (Leppänen, 2009a) and in separate methodological studies (Isotalo, 2011a).

Fission product poison Xe-135 can be handled separately from the other nuclides, and iterated to its equilibrium concentration during the transport simulation. The equilibrium calculation is independent of the depletion routine, and the iteration can also be performed in transport mode without burnup calculation.

Parallelization

Serpent 1 uses the Message Passing Interface (MPI) for parallel calculation. Parallelization of the transport routine is implemented by dividing the neutron histories between the parallel tasks and combining the results after the simulation has been completed. This approach is simple and efficient, but it lacks error tolerance and dynamic load sharing. The overall calculation time is dependent on the slowest task, which is why the method is best applied in a symmetric parallel environment.

Parallelization in Serpent 2 is based on a hybrid OpenMP / MPI approach. The main advantage of thread-based OpenMP is that all CPU cores within the computational node are accessing the same memory space, which makes it possible to use all available cores in the calculation without running into problems with excessive memory usage.

In addition to the neutronics simulation, parallelization in the burnup calculation mode divides also the preprocessing and depletion routines between several CPU's.

Results and output

Spatial homogenization was the main intended application for Serpent when the project was started in 2004. The work is still in progress, and the group constant generation capability in Serpent 2 currently covers:

?	 Homogenized few-group reaction cross sections
?	Scattering and scattering production cross sections and matrices up to Legendre order 7
?	Diffusion coefficients
?	Assembly discontinuity factors (ADF's), surface and corner fluxes and currents for square and hexagonal fuel lattices
?	Group-wise peaking factors for pin-power reconstruction
?	Poison cross sections for Xe-135 and Sm-149 and their precursors
Homogenization can be performed in critical spectrum, or using a leakage correction based on the deterministic solution of B1 equations (Fridman, 2011; 2012). Work is currently under way for the calculation of adjoint-weighted time constants for transient simulator codes, and ADF's in reflector regions and colorset configurations, which requires the explicit solution of local homogeneous flux, as the net current over the homogenized region is not reduced to zero.

User-defined tallies can be set up for calculating various integral reaction rates. The spatial integration domain can be defined by a combination of cells, universes, lattices and materials, or using a three-dimensional super-imposed mesh. The number and structure of detector energy bins is unrestricted. Various response functions are available for the calculation, including material-wise macroscopic and isotopic microscopic cross sections and ACE format dosimetry data. Serpent 2 calculates adjoint-weighted point kinetics parameters and effective delayed neutron fractions using the iterated fission probability (IFP) method (Leppänen, 2014b), relying on an implementation similar to that in MCNP5 (Kiedrowski, 2011).

Output for burnup calculation consists of isotopic compositions, activities, spontaneous fission rates and decay heat data. The results are given both as material-wise and total values. Group constants and all the other output parameters are calculated and printed for each burnup step.

All numerical output is written in Matlab m-format files to simplify the post-processing of the results. The code also has a geometry plotter feature and a reaction rate plotter, which is convenient for visualizing the neutronics and tally results (see the gallery for examples).

Validation

Each Serpent update is validated by comparison to MCNP by running a standard set of assembly calculation problems. Effective multiplication factors and homogenized few-group reaction cross sections are within the statistical accuracy from the reference results, when the same ACE libraries are used in the calculations. Validation against MCNP has also been carried out with equally good results for calculations involving individual nuclides, by comparing the flux spectra produced by the two codes. Differences to other Monte Carlo codes (Keno-VI) are small, but statistically significant discrepancies can be observed in some cases. Differences to deterministic lattice codes are generally larger, mainly due to the fundamental differences between the calculation methods.

Validation of burnup calculation routines is considerably more difficult, due to the lack of a perfect reference code. In addition to discrepancies in the transport simulation, there are additional factors related to decay and fission yield data, methods used for solving the Bateman equations, number of nuclide concentrations tracked for each burnable material, depletion algorithms, and so on. The results are generally good compared to other burnup calculation codes, but there are some significant discrepancies as well. Comparisons to CASMO-4E (Leppänen, 2009a), for example, show a consistent few-percent over-prediction in the build-up of Pu-239 and noticeable differences in the concentrations of the main fission product poisons, Xe-135 and Sm-149. The differences most like originate from several factors related to the data and methods used by the two codes, but the root cause is yet to be discovered.

Systematic validation for criticality safety analyses using experimental configurations and data from the International Handbook of Evaluated Criticality Safety Benchmark Experiments is currently under way. Another validation project, in which Serpent-generated group constants are used for fuel cycle and transient simulator calculations by nodal diffusion ARES, HEXBU-3D, TRAB-3D and HEXTRAN, developed at VTT and the Finnish Radiation and Nuclear Safety Authority (STUK), was recently started. The first study involved the Serpent-ARES code sequence for the calculation of a hot zero-power initial core configuration of a commercial PWR (MIT BEAVRS Benchmark). The results showed good agreement compared to experimental measurements and reference 3D Monte Carlo calculations (Leppänen, 2014b).

Performance

Serpent 1 is optimized for performance in infinite lattice calculations. The typical single-CPU running time on a 2.6 GHz AMD Opteron PC varies from 5 to 20 minutes when 3 million neutron histories are simulated, although it should be noted that the performance depends on several factors. Owing to the unionized energy grid approach, the running time is not strongly dependent on the complexity of the material compositions. Compared to fresh fuel calculations, the transport cycle usually slows down by less than a factor of 1.5 when modeling irradiated fuels consisting of 100-250 nuclides.

The main factor limiting the performance of Serpent 1 is that the parallelization is based on distributed-memory MPI, which means that the memory demand is multiplied by the number of parallel tasks. Serpent 2 currently runs slower with a single CPU, but the capability to run parallel calculation with OpenMP without limitations in memory demand means that the code clearly outperforms Serpent 1 by making more efficient use of computational resources. Studies on the parallel scalability of Serpent 2 are currently under way.

\cite{jaakko}


\section{Added Value}

Producing this program, which has been named WARP, will be the first step in hopefully creating a full featured reactor simulation program that runs on GPU accelerator cards.  The added value in doing this comes from the fact that many supercomputers are gaining power from GPUs and in order for nuclear engineers to take advantage of this new computing power, which they always need more of, a new code must be written to run on them.  Even though GPU computing is still in its very early stages, developing WARP hedges risk for the simulation community against their computational tools becoming under powered or even obsolete.  Conversely, it also exposes a substantial amount of  computing power that couldn't otherwise be used on personal computers, workstations, and laptops.  Targeting GPUs could enable smaller computers to take on substantial reactor simulation tasks, and potentially reduce design iterations for people without access to supercomputers.  Other than being a nod to NVIDIA's terminology, if WARP were an acronym, it would stand for ``weaving all the random particles,'' with the words ``weaving all'' referring to the lockstep way in which ``all the random particles'', i.e. the neutrons, are sorted into coherent bundles and transported.

\section{Objective}

The first goal of this work is first to produce a program (WARP) that runs accurate continuous energy neutron transport simulations on the GPU in general 3D geometries using standard nuclear data files.  This program will be able to run in both fixed source and criticality source modes.  It will also be able to produce neutron spectra.  The second goal is to make these simulations as fast as possible.

\section{Outline}

1 paragraph summaries of each chapter.  Will do once they are written!
