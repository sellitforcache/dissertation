% (This is included by thesis.tex; you do not latex it by itself.)

\begin{abstract}

% Pro-tip: to facilitate version control diffing, it can be easier to make each sentence its own line so then you can just tell which sentences change.
% Or you can try to limit line length (but that seems like too much of a pain).

GPUs have gradually increased in computational power from the small, job-specific boards of the early 1990s to the programmable powerhouses of today. Compared to CPUs, GPUs have a higher aggregate memory bandwidth, much higher floating-point operations per second (FLOPS), and lower energy consumption per FLOP. Because one of the main obstacles in exascale computing is power consumption, many new supercomputing platforms are gaining much of their computational capacity by incorporating GPUs into their compute nodes. Since CPU-optimized parallel algorithms are not directly portable to GPU architectures (or at least not without losing substantial performance gain), transport codes need to be rewritten to execute efficiently on GPUs. Unless this is done, we cannot take full advantage of these new supercomputers for reactor simulations. 

WARP is a three-dimensional (3D) continuous energy Monte Carlo neutron transport code that has been developed at UC Berkeley as a first attempt to efficiently map the Monte Carlo transport algorithm onto the GPU while preserving its benefits, namely, very few physical and geometrical simplifications.  Using NVIDIA's OptiX ray tracing framework [add reference] for geometry representation, adopting a unionized energy grid structure for cross sections, regularizing memory access through linked arrays, introducing parallel-efficient search and sorting algorithms, and efficiently eliminating completed neutron data from being accessed are the main strategies used to implement Monte Carlo on GPUs.

In the initial benchmarking of the best-performing mode, in which $10^6$ source neutrons per criticality batch are used, WARP is capable of delivering results that are nearly identical to MCNP 6.1 and Serpent 2.1.18, but with run times that are 11-82 times lower.  GPUs achieve their high throughput by being deeply pipelined, and it is very important to use large datasets (i.e. large numbers of threads) whenever possible to offset the cost of high memory latency.  When the active neutron population inevitably becomes small, the overheads and latencies cannot be effectively hidden by pipelining and it becomes important to minimize launch overhead by launching the minimum number of thread blocks necessary to process the remaining neutrons.

%You need some sort of wrap-up sentence or paragraph. This is sort of stilted and doesn't tell me much about what your conclusions are. Should I keep investigating this? Should I adopt it now? What does your research mean and what are your recommendations? 


\end{abstract}
