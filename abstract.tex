% (This is included by thesis.tex; you do not latex it by itself.)

\begin{abstract}

GPUs have gradually increased in computational power from the small, job-specific boards of the early 90s to the programmable powerhouses of today. Compared to CPUs, they have a higher aggregate memory bandwidth, much higher floating-point operations per second (FLOPS), and lower energy consumption per FLOP. Because one of the main obstacles in exascale computing is power consumption, many new supercomputing platforms are gaining much of their computational capacity by incorporating GPUs into their compute nodes. Since CPU optimized parallel algorithms are not directly portable to GPU architectures (or at least without losing substantial performance gain), transport codes need to be rewritten in order to execute efficiently on GPUs. Unless this is done, we cannot take full advantage of these new supercomputers for reactor simulations. 

WARP is a 3D continuous energy Monte Carlo neutron transport code that has been developed at UC Berkeley as a first attempt to efficiently map the Monte Carlo transport algorithm on the GPU while preserving it's benefits, namely, very few physical and geometrical simplifications.  Using NVIDIA's OptiX ray tracing framework for geometry representation, adopting a unionized energy grid structure for cross sections, regularizing memory access through linked arrays, introducing parallel-efficient search and sorting algorithms, and efficiently eliminating completed neutron data from being accessed are the main factors in completing the task.

In the initial benchmarking of the best performing mode where $10^6$ source neutrons are use per criticality batch, WARP is capable of delivering near identical results to MCNP 6.1 and Serpent 2.1.18 but with runtimes 11-82 times lower, depending on the problem geometry.  GPUs are deeply pipelined to achieve their high throughput specifications, and it is very important to use large datasets (i.e. large numbers of threads) whenever possible to offset the cost of high memory latency.  When the active neutron population inevitably becomes small, the overheads and latencies cannot be effectively hidden by pipelining and it becomes important to minimize launch overhead by launching the minimum number of thread blocks necessary to process the remaining neutrons.

\end{abstract}
