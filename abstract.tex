% (This is included by thesis.tex; you do not latex it by itself.)

\begin{abstract}

% Pro-tip: to facilitate version control diffing, it can be easier to make each sentence its own line so then you can just tell which sentences change.
% Or you can try to limit line length (but that seems like too much of a pain).
% EG - Abstract should focus on describing the work you did, to make clear what are the unique contributions, and what are the major outcome and findings. 

Graphics processing units, or GPUs, have gradually increased in computational power from the small, job-specific boards of the early 1990s to the programmable powerhouses of today. Compared to more common central processing units, or CPUs, GPUs have a higher aggregate memory bandwidth, much higher floating-point operations per second (FLOPS), and lower energy consumption per FLOP. Because one of the main obstacles in exascale computing is power consumption, many new supercomputing platforms are gaining much of their computational capacity by incorporating GPUs into their compute nodes. Since CPU-optimized parallel algorithms are not directly portable to GPU architectures (or at least not without losing substantial performance), transport codes need to be rewritten to execute efficiently on GPUs. Unless this is done, reactor simulations cannot take full advantage of these new supercomputers. 

WARP, which can stand for ``Weaving All the Random Particles,'' is a three-dimensional (3D) continuous energy Monte Carlo neutron transport code developed in this work as to efficiently implement a continuous energy Monte Carlo neutron transport algorithm on a GPU.  WARP accelerates Monte Carlo simulations while preserving the benefits of using the Monte Carlo Method, namely, very few physical and geometrical simplifications.  WARP is able to calculate multiplication factors, flux tallies, and fission source distributions for time-independent problems, and can run in both criticality or fixed source modes.  WARP can transport neutrons in unrestricted arrangements of parallelepipeds, hexagonal prisms, cylinders, and spheres.

%GPUs gain computational throughput by having large single instruction, multiple data (SIMD) units, which require the same instruction to be carried out over data elements.  This is very similar to vector computers of the past, and ``event-based'' Monte Carlo algorithms have been developed to execute on these types of computers.  Event-based algorithms are those where neutron histories undergoing the same event are inserted into vectors, and when the vector is filled, it is processed by a vector unit in parallel.  After a step of transport, the vectors no longer have data that need the same operation, so a ``shuffle'' operation is done, which moves data elements between vectors, making them uniform again for the next step of parallel processing.

WARP uses an event-based algorithm, but with some important differences.  Moving data is expensive, so WARP uses a remapping vector of pointer/index pairs to direct GPU threads to the data they need to access.  The remapping vector is sorted  by reaction type after every transport iteration using a high-efficiency parallel radix sort, which serves to keep the reaction types as contiguous as possible and removes completed histories from the transport cycle.  The sort reduces the amount of divergence in GPU ``thread blocks,'' keeps the SIMD units as full as possible, and eliminates using memory bandwidth to check if a neutron in the batch has been terminated or not.  Using a remapping vector means the data access pattern is irregular, but this is mitigated by using large batch sizes where the GPU can effectively eliminate the high cost of irregular global memory access.

%WARP uses a  unionized energy grid format to regularize the access pattern of nuclear data.  Unionizing the energy grids creates a single, universal energy grid for all cross section data, meaning a single search is needed per transport step instead of one for every isotope included in the simulation.  
WARP modifies the standard unionized energy grid implementation to reduce memory traffic.  Instead of storing a matrix of pointers indexed by reaction type and energy, WARP stores three matrices.  The first contains cross section values, the second contains pointers to angular distributions, and a third contains pointers to energy distributions.  This linked list type of layout increases memory usage, but lowers the number of data loads that are needed to determine a reaction by eliminating a pointer load to find a cross section value.

Optimized, high-performance GPU code libraries are also used by WARP wherever possible.  The CUDA performance primitives (CUDPP) library is used to perform the parallel reductions, sorts and sums, the CURAND library is used to seed the linear congruential random number generators, and the OptiX ray tracing framework is used for geometry representation.  OptiX is a highly-optimized library developed by NVIDIA that automatically builds hierarchical acceleration structures around user-input geometry so only surfaces along a ray line need to be queried in ray tracing.  WARP also performs material and cell number queries with OptiX by using a point-in-polygon like algorithm.

In the initial testing where $10^6$ source neutrons per criticality batch are used, WARP is capable of delivering results that are anywhere from 6 to 600 pcm away from MCNP 6.1 and Serpent 2.1.18, but with run times that are 11-82 times lower, depending on problem geometry and materials.  On average, WARP's performance on a NIVIDIA K20 is equivalent to approximately 45 AMD Opteron 6172 CPU cores.  Larger batches are typically perform better on the GPU, but memory limitations of the K20 card restricted batch size to $10^6$ source neutrons.

WARP has shown that GPUs are an effective platform for performing Monte Carlo neutron transport with continuous energy cross sections.  Currently, WARP is the most detailed and feature-rich program in existence for performing continuous energy Monte Carlo neutron transport in general 3D geometries on GPUs, but compared to production codes like Serpent and MCNP, WARP has limited capabilities.  Despite WARP's lack of features, its novel algorithm implementations show that high performance can be achieved on a GPU despite the inherently divergent program flow and sparse data access patterns.  WARP is not ready for everyday nuclear reactor calculations, but is a good platform for further development of GPU-accelerated Monte Carlo neutron transport.  In it's current state, it may be a useful tool for multiplication factor searches, i.e. determining reactivity coefficients by perturbing material densities or temperatures, since these types of calculations typically do not require many flux tallies. 

\end{abstract}
