%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
\label{chap:background}

In this chapter, details regarding the simulation theory are gone over.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reactor Analysis}

reactors in general, LWR, fast breeders, GenIV, waste streams, power conversion, coupling between TH, need for fast simulations in oder to do convergence iterations

The fission chain reaction is sustained from the fact that when a uranium (or other fissile heavy metal) nucleus fissions, 2 or more neutrons are released.  If, on average, one of these neutrons goes on to induce another fission, the system in which the fissions are occurring is \emph{critical}, or able to maintain a constant neutron population and fission reaction rate (i.e. power) without any external neutron sources.

\subsection{Point Kinetics, Multiplication Factor}

The most important parameter of a nuclear reactor is its effective multiplication factor, $k_\mathrm{eff}$, which is the ratio of subsequent neutron generations.  This number can partly describes the rate of change of the neutron population if the average neutron lifetime is know.  If a reactor is treated as a point, i.e. with no dimensional dependence, the rate of change of its neutron population can be described with the differential equation shown in \eqref{point_kin2} where $N(t)$ is the neutron population at time $t$ and $l_p$ is the average neutron lifetime \cite{duderstadt}. 

\begin{equation}
\label{point_kin1}
N(t+l_p) \simeq N(t)+l_p \frac{dN(t)}{dt} = k_\mathrm{eff} N(t) 
\end{equation}

\begin{equation}
\label{point_kin2}
\frac{dN(t)}{dt} \simeq \frac{k_\mathrm{eff} -1}{l_p}N(t)
\end{equation}

The solution to this equation is show in in \eqref{reactor_period}.  The neutron lifetime is usually treated as constant since it mostly depends on the material present in the reactor (which change very slowly compared to the neutron population), so the only parameter that influences the reaction period $T$ is $k_\mathrm{eff}$.  When the multiplication factor is unity, the reactor period is infinity, implying that the neutron population is in a steady state.  When it is less than one, the period is negative, implying that the neutron population will decrease.  The converse it true when it is positive.  Then the period is positive and the neutron population increases with time.

\begin{equation}
\label{reactor_period}
\begin{split}
N(t) &= N_0 \exp \left( \frac{t}{T} \right) \\
T &= \frac{l_p}{k_\mathrm{eff} -1}
\end{split}
\end{equation}

\subsection{Six Factor Formula}

The multiplication factor is the most important parameter in a nuclear reactor because it dictates the time rate of change of its power.  It is a global number, and a lot of physics gets integrated into it.  From a high level, there are six factors that can influence the multiplication factor: 
the thermal fission factor, $\eta$, or the average number of neutrons produced per thermal fission;
the thermal utilization factor, $f$, or the probability that an absorbed thermal neutron was absorbed by a fissile nuclide;
the resonance escape probability, $p$, or the probability that neutrons are not captures in large resonances as they scatter from high energy; 
the fast fission factor, $\epsilon$, the ratio of all fission neutrons to those born from thermal fissions;
the fast non-leakage probability, $P_\mathrm{FNL}$, the probability that high-energy neutrons will not leak out of the system;
the thermal non-leakage probability, $P_\mathrm{TNL}$, the probability that low-energy neutrons will not leak out of the system.

Multiplying all these factors together yields the number of neutrons born from fission given a single entering neutron, i.e. the multiplication factor.  This formula, known as the ``six factor formula,'' is shown in \eqref{six_ff_eq}.  Many physics factors go into each of the formula's factors, like geometrical configuration, cross section energy dependence, material composition, and scattering kinematics, to name a few major components.  A graphical interpretation of the six factor formula is shown in Figure \ref{six_ff_chain}.

\begin{equation}
\label{six_ff_eq}
k_\mathrm{eff}  = \eta f p  \epsilon P_\mathrm{FNL} P_\mathrm{TNL}
\end{equation}


\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/six_ff.eps}
     \caption{The reaction chain represented by probabilities in the six factor formula. \label{six_ff_chain}}
\end{figure}

The six factor formula relates to the Monte Carlo method since it is basically dimensionally-average Monte Carlo.  In other words, it describes what happens to the average neutron, and an individual neutron's lifetime, or ``history,'' could be simulated by sampling the constant probabilities that the factors represent as shown in Figure \ref{six_ff_chain}.  After a large group, or ensemble, of these histories is assembled, they can be averaged to determine the multiplication factor.  This is the basic idea of Monte Carlo neutron transport.  The results are trivial in this case, since there is no dimensionally in the problem and the probabilities are constant and the multiplication can be found analytically, but the six factor formula is way to think about the process without complicating it with dimensionality.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Nuclear Interactions}
\label{subsec:interactions}

The core of a Monte Carlo simulation is explicitly modeling the individual interactions the neutron undergoes as it moves through matter.  At the highest level, these reactions can be broken down into two categories, elastic reactions and inelastic reactions.  Elastic reactions are those where both momentum and kinetic energy are conserved, i.e. ones where any energy the neutron loses is given to the target nucleus.  Inelastic reactions are those that conserve momentum but do not conserve kinetic energy, i.e. ones where the kinetic energy of the particles can be converted to a vibrational mode of the target nucleus, etc.  These reactions are further divided by the number and type of secondary particles they produce.  Reactions that produce no secondary neutrons are called ``disappearance'' reactions since the neutron effectively disappears, reactions that produce a single neutron are typically called scattering reactions since they effectively change the incoming neutron's energy and direction, and reaction the produce more than one particle are simply called ``secondary-producing'' reactions. Table \ref{nuc_reaction_summary} show a summary of the reaction classifications, and it can be seen that elastic scattering is the only elastic reaction.   

\begin{table}[h]
\centering
\caption{Summary matrix of how neutron reactions are classified.}
\label{nuc_reaction_summary}
\begin{tabular}{| l | c | c | c |}
\multicolumn{4}{c}{Number of Secondaries} \\
\hline
  & 0 & 1 & $>$1 \\
 \hline
 Elastic & & Elastic Scatter & \\
 \hline
 Inelastic & Disappearance & Inelastic Scatter & Secondary-Producing \\
\hline
\end{tabular}
\end{table}

The probabilities for individual reactions occurring are expressed in terms of cross sections.  In simple terms, nuclear cross sections are like a geometric cross sections -they represent the ``size'' of the target nucleus for a particular reaction.  The classical analogy is that if neutrons and nuclei are hard spheres, and neutrons are randomly shot through a material, more neutrons will hit the larger targets than the smaller ones.  Cross sections are also expressed in units of area, the ``barn,'' which is $10^{-24}$ cm$^2$.  This unit was coined by Baker and Holloway while performing scattering experiments with uranium since ``a cross section of $10^{-24}$ cm$^2$ for nuclear purposes was really as big as a barn''\cite{LAMS523}.  Of course, nuclear cross sections have no literal meaning in terms of the actual sizes of the nuclei, they only represent the likelihood of a particular reaction occurring.  Working from the macroscopic scale, which is where measurements are taken, a \emph{macroscopic cross section}, represented by greek capital sigma ($\Sigma$), is the probability of a reaction happening within an infinitesimal distance d$x$.  With this parameter, we can write down an equation that describes the survival probability of a group of particles.  Describing a group is necessary since the dimension x is \emph{continuous}.  Given a particle packet containing N particles and $\Sigma$, their interaction probability over $d$x, the change the population over $d$x is the product of the population N and the interaction probability, as show in \eqref{pop_diff}.

\begin{equation}
\frac{d\mathrm{N}}{d\mathrm{x}} = - \Sigma \mathrm{N}
\label{pop_diff}
\end{equation}

Integrating \ref{pop_diff} over an interval yields an expression for the number of \emph{non-interacting} particles left in a packet after crossing that interval.  Dividing the surviving number by the initial gives a dimensionless expression for the non-interaction probability, P$_\mathrm{NI}$, over the interval x$_1$ as shown in \eqref{pop_NI}.

\begin{equation}
\mathrm{P}_\mathrm{NI} = \frac{\mathrm{N}}{\mathrm{N}_0} = \mathrm{e}^{- \Sigma \mathrm{x}_1}
\label{pop_NI}
\end{equation}

\begin{equation}
\Sigma = \frac{ - \mathrm{ln}(    \mathrm{I} / \mathrm{I}_0  )  }  {\mathrm{x}_1}
\label{pop_beam}
\end{equation}

Since these expressions also apply to beam intensity, they can be used to measure the cross sections for materials by rearranging the equations into the form shown in \eqref{pop_beam}.  If the source intensity, the exiting intensity, and the material thickness are all known, the macroscopic cross section for that material can be determined.  On a microscopic level, however, the neutron interaction probabilities only depend on the type of nucleus, not the aggregate material.  To correct for this, the measured macroscopic cross section can be divided by the number density of nuclei in the material to give the microscopic cross section, $\sigma$, which is material-independent and only depends on the isotope.  In \eqref{micro_exp}, N represents the number density in units of cm$^{-3}$, $\rho$ represents the material density in terms of g*cm$^{-3}$, and M represents the nuclear mass in g.  The microscopic cross section has units of barns, and is what is given in nuclear data files.  They are more general since they are not material-dependent and can be summed into a total material macroscopic cross section by weighting individual microscopic cross sections with the number density of an isotope in a material, as show in \eqref{material_sum_xs}.  In this expression, $f_\mathrm{i}$ is the atomic fraction of isotope i in the material.


\begin{equation}
\Sigma = N \sigma = \frac{\rho}{M} \sigma
\label{micro_exp}
\end{equation}

\begin{equation}
\sum_{\mathrm{i}=1}^{\mathrm{N}_\mathrm{isotopes}} f_\mathrm{i} =1
\label{fraction_norm}
\end{equation}

\begin{equation}
\Sigma_{\mathrm{material}} = \mathrm{N}_1 \sigma_1 +  \mathrm{N}_2 \sigma_2 + \mathrm{N}_3 \sigma_3+ ... = \sum_{\mathrm{i}=1}^{\mathrm{N}_\mathrm{isotopes}} \mathrm{N}_\mathrm{i} \sigma_\mathrm{i} = \rho\frac{\sum_{\mathrm{i}=1}^{\mathrm{N}_\mathrm{isotopes}} f_\mathrm{i}\sigma_\mathrm{i} } { \sum_{\mathrm{i}=1}^{\mathrm{N}_\mathrm{isotopes}} f_\mathrm{i} \mathrm{M}_\mathrm{i}}
\label{material_sum_xs}
\end{equation}

The above expressions do not take energy into account, so they are assumed to be at a single energy.  Using such simple expressions to determine the cross section of an isotope would only be valid with the use of a very, very precisely mono-energetic beam.  The quantum mechanical details that go into cross sections can cause them to depend strongly on the energy (or velocity) of the neutron and the target nucleus.  Many nuclides have resonances where the interactions probability spikes to very large values.  This typically happens when the incoming energy is exactly that of an energy level of the resultant compound nucleus \cite{duderstadt}.   Figure \ref{xs_e_dependence_li} shows the energy dependence of various reactions type in lithium-6 and Figure \ref{xs_e_dependence_u} shows the dependence of others in uranium-235.  

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/xs_li6.eps}
     \caption{The energy dependence of some reactions in lithium-6.\label{xs_e_dependence_li}}
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/xs_u235.eps}
    \caption{The energy dependence of some reactions in uranium-235.   \label{xs_e_dependence_u}}
\end{figure}

It is important to note the complexity shown in uranium-235 in the 0.1eV to 40keV range.  This is referred to as the ``resonance region'' and adds much of the complexity in accurately modeling neutron transport.  There are no simple functional representations available for these types of cross sections, so they must be represented in a point-wise tabular format.  It is also important to note that at low energies, cross sections typically lose structure and exhibit the same ``1/$v$'' scaling.  This is due to the particle and the target having more time to interact with each other.  The energy levels where this phenomenon exists is similar the thermal energy of the target and the target no longer appears stationary from the neutrons perspective.  If the neutron is slow enough, the targets appear to be moving instead of the neutron, and more of them come into the neutrons sphere of interaction.  This is the qualitative reason for 1/$v$ scaling at low energies.  Due to to their identical scaling, the ratios between the reaction types stays constant as velocity is decreased but the overall interaction probability with respect to distance increases.

In the next subsections, the characteristics and kinematics of the different reaction types are explained.

\subsection{Elastic Scattering}

Elastic scattering conserves both the momentum and the kinetic energy of the reacting particles and occurs when the neutron does not enter the nucleus, only bounces off of it's potential field.  Since there is only a single exiting particle, elastic scattering is a two-body interaction and the exiting energies can be determined if the angle between the exiting particle is known.  This problem is greatly simplified if the interacting particles' velocities are transformed into the center-of-mass (CM) frame, where net momentum is zero. The velocity of the CM is defined in \eqref{vCM} where $m$ and $M$ are the neutron's and the target's respective masses, $v$ and $V$ are their respective velocity vectors, and $A$ is the ratio of the target's mass to the neutron's mass (also know as the ``atomic weight ratio'' or AWR).  The derivation from here follows closely with that in ref \cite{jaakko}.

\begin{equation}
A = \frac{M}{m}
\label{AWR}
\end{equation}

\begin{equation}
\boldsymbol{v_{\mathrm{CM}}} = \frac{ m \boldsymbol{v} + M \boldsymbol{V} }    {m+M} = \frac{ \boldsymbol{v} + A \boldsymbol{V} }    {1+A}
\label{vCM}
\end{equation}

The CM velocities of the target and the neutron are then calculated by subtracting the CM velocity out of them, ash shown in \eqref{CM}.  The $_\mathrm{c}$ subscript will denote the CM-frame values from now on, whereas $v_{\mathrm{CM}}$ will denote the velocity of the CM frame relative to the lab frame.

\begin{equation}
\begin{split}
 \boldsymbol{v_{_\mathrm{c}}} &= \boldsymbol{v} - \boldsymbol{v_{\mathrm{CM}}} \\  
 \boldsymbol{V_{\mathrm{c}}} &= \boldsymbol{V} - \boldsymbol{v_{\mathrm{CM}}}
 \end{split}
\label{CM}
\end{equation}

Once in the CM frame, the equation for conservation of momentum can be written as \eqref{consMomCM}.  The primed values are those after the collision.  Since the net momentum is zero, the directions of the neutron and the target must be in opposite directions as shown in \eqref{rotationCM}.

\begin{equation}
\begin{split}
m \boldsymbol{v_{_\mathrm{c}}} + M \boldsymbol{V_{_\mathrm{c}}} &= m \boldsymbol{v_{_\mathrm{c}}^\prime} + M \boldsymbol{V_{_\mathrm{c}}^\prime} = 0\\
    \boldsymbol{v_{_\mathrm{c}}} + A  \boldsymbol{V_{_\mathrm{c}}} &=     \boldsymbol{v_{_\mathrm{c}}^\prime} + A  \boldsymbol{V_{_\mathrm{c}}^\prime} = 0
\end{split}
\label{consMomCM}
\end{equation}

\begin{equation}
\begin{split}
\boldsymbol{v_{_\mathrm{c}}^\prime} &= - A  \boldsymbol{V_{_\mathrm{c}}^\prime} \\
\boldsymbol{v_{_\mathrm{c}}} &= -A  \boldsymbol{V_{_\mathrm{c}}}
\end{split}
\label{rotationCM}
\end{equation}

The equation for conservation of energy is shown in \ref{consECM}.  Energy is a scalar quantity, and therefore these equations are as well.  There are no vectors, which are indicated with boldface type, since $v^2 =( \boldsymbol{v} \cdot \boldsymbol{v})$.  $Q$ is the amount of energy released by the reaction, and is zero here but is convenient to include in this derivation for use in inelastic collision kinematics where it is nonzero.

\begin{equation}
\begin{split}
m v_{_\mathrm{c}}^2 + M V_{_\mathrm{c}}^2 &= m v_{_\mathrm{c}}^{\prime2} + M V_{_\mathrm{c}}^{\prime2} + 2Q \\
    v_{_\mathrm{c}}^2 + A  V_{_\mathrm{c}}^2 &=     v_{_\mathrm{c}}^{\prime2} + A  V_{_\mathrm{c}}^{\prime2} + \frac{2Q}{m}
\end{split}
\label{consECM}
\end{equation}

There are now two unknowns (the primed values) and two equations, and the final velocities of the neutron and the target can be determined.  Substituting \eqref{rotationCM} into \eqref{consECM} and solving yields either equation in \eqref{finalvCM}, depending on how the substitution is done.  It can be seen that if $Q$ is zero, as it is in elastic scattering, the initial and final velocities are the same for the particles and the interaction only causes a rotation in the CM frame.

\begin{equation}
\begin{split}
v_{_\mathrm{c}}^{\prime} &=  \sqrt{ v_{_\mathrm{c}}^{2} + \frac{2AQ}{m(A+1)}  }  \\
V_{_\mathrm{c}}^{\prime} &= \sqrt{ V_{_\mathrm{c}}^{2} + \frac{2Q}{mA(A+1)}  } 
\end{split}
\label{finalvCM}
\end{equation}

At first glance, it seems like the interaction has been fully characterized, but \eqref{rotationCM} only relates the initial states of the neutron and the target to one another and the final states of neutron and the target to one another.  The initial state and final state of the neutron still need to be related.  It has been mentioned already that the interaction is only a rotation in the CM frame, so the initial and final state of the neutron's direction can be related by a three-dimensional rotation formula.  An efficient algorithm is given by the Rodrigues' rotation formula, shown in \eqref{RodriguesRot}.  In the formula, $\boldsymbol{k}$ is an auxilliary unit vector around which $\boldsymbol{v}$ is being rotated.  From this unit vector, $theta$ is the angle $\boldsymbol{v}$ is rotated away from $\boldsymbol{k}$ such that $(\boldsymbol{v} \cdot \boldsymbol{v_{\mathrm{rot}}}) = |v||v_{\mathrm{rot}}|\cos\theta$.  It is ``efficient'' in the sense that to rotate a vector, a full 3x3 matrix does not need to be constructed and multiplied by the vector. In other words, matrix-vector operations are not needed and it can be carried out with only vector operations \cite{}.

\begin{equation}
 \boldsymbol{ v_{ \mathrm{rot}}} = \boldsymbol{v} \cos \theta + (\boldsymbol{k} \times \boldsymbol{v}) \sin \theta + \boldsymbol{k} (\boldsymbol{k} \cdot \boldsymbol{v})(1-\cos \theta)
\label{RodriguesRot}
\end{equation}

If the polar and azimuthal rotation angles, $\theta$ and $\phi$, respectively, are determined, the initial neutron velocity vector can be rotated through these angles to its final value.  After the rotation is done, the final velocities are known in the CM frame and they can be transformed back to the lab frame to give the final velocities there, as shown in \eqref{xformbackCM}.

\begin{equation}
\begin{split}
 \boldsymbol{v^{\prime}}  &= \boldsymbol{v_{\mathrm{c}}^{\prime}} + \boldsymbol{v_{\mathrm{CM}}} \\  
 \boldsymbol{V^{\prime}} &= \boldsymbol{V_{\mathrm{c}}^{\prime}} + \boldsymbol{v_{\mathrm{CM}}}
 \end{split}
\label{xformbackCM}
\end{equation}


\subsection{Inelastic Level Scattering}

Inelastic scattering is the other type of scattering a neutron can undergo where kinetic energy is no longer conserved.  An amount of energy is either released from the reaction to the particles, or more commonly, taken from the particles and lost to the reaction.  In neutron-nucleus collisions, the target nucleus can be excited to a higher energy state than its ground state if the colliding neutron has a high enough energy to do so.  If it does and the collision excites the nucleus, a discrete amount of energy equal to the energy of the excited state is lost to the reaction.  These excited states typically do not have long half lives, and a gamma ray is emitted when the nucleus relaxes to its ground state.  This type of reaction is called inelastic level scattering due to a an excited energy \emph{level} becoming occupied by the target nucleus.  

Since it is still a two-body interaction, the kinematics of the reaction are identical to elastic scattering except the $Q$ value is nonzero and negative.  These reactions have a threshold energy below which their cross sections are zero since the neutron would not have enough energy to excite the state.   Figure \ref{Elevels} shows the energy levels in lead-XXX, which is often used as a fast neutron moderators due to its many, low-lying energy levels and its large inelastic scattering cross sections.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/noise.eps}
     \caption{The energy levels of lead-XXX\cite{}. \label{Elevels}}
\end{figure}

\subsection{Inelastic Continuum Scattering}

At energies above the distinct levels there lies a continuum in the nuclear energy states.  This isn't truly a ``continuum'' in the strictest sense, but the energy levels become so close together they they effectively form a domain where energy can take continuous values.  Unlike the discrete $Q$ values corresponding to a single excited state used in inelastic level scattering, the $Q$ value of the reaction now follows a distribution.  

In bound nuclei, S(a,b),  compound nucleus?

\subsection{Disappearance Reactions}

Unlike scattering reactions, where the energy and direction of the neutron is changed but continues to transport, \emph{disappearance} reactions remove a free neutron.  Typically this \emph{capture} of a neutron leaves the daughter nucleus in an excited state, which them relaxes to ground state by emitting a gamma ray.


\subsection{Fission}

Fission means ``the splitting splitting of something into two parts'' in the literal sense of the word.  This is exactly what nuclear fission is as well.  It is when a nucleus splits into two smaller nuclei, called \emph{fission fragments}.  When heavy nuclei undergo fission, they release energy, and typically emit a few other particles, including neutrons.  That this reaction releases energy is the reason heavy elements, like uranium, can be used as an energy source.  This is because the fission fragments have a higher binding energy per nucleon compared to the parent nucleus.  Figure \ref{binding_e_per_nuc} shows the average binding energy per nucleon for a wide range of isotopes.  Note that the peak of the curve is at Iron-56, the most tightly bound nucleus, and that heavier nuclides are lower than it.  Fission splits the parent into two lighter nuclides, and since the fragments are more tightly bound, the excess binding energy from the parent is released.  The released energy isn't simply deposited as heat directly, however, it is released in a range of forms.  Fission doesn't just release some energy, it releases a substantial amount of it.  Uranium-235, for example, releases a total of 192.9$\pm$0.5 \emph{MeV} per fission \cite{duderstadt}.  Compared to chemical energy sources where the energy released per reaction is on the order of 1\emph{eV}.  Fission energy yield is 8 orders of magnitude larger!  Not all of this energy is converted to heat, bit a substantial amount is.  Table \ref{fission_dist} shows the fraction of this total energy that is given to each entity.  Note that a significant fraction is given to neutrinos, which is essentially lost due to their very small interaction cross section.  The kinetic energy of the fission fragments has the majority of the energy, and since they are heavy and charged, their energy is deposited as heat very near to the fission site.  Other particles, light photons and neutrons, carry some of the fission energy further away from the fission site, but their energy mostly still ends up as heat.
  
\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/binding_energy.pdf}
     \caption{Average binding energy per nucleon \cite{wikimedia_binding}.\label{binding_e_per_nuc}}
\end{figure}

\begin{table}[h]
\centering
\caption{Average distribution of fission energy \cite{duderstadt}.\label{fission_dist}}
\begin{tabular}{| l | c | r | r |}
\hline
Particle & Energy (\%) & Range & Time \\
\hline
Fission fragment kinetic energy & 80 & $<$0.1cm & prompt \\
\hline
Prompt neutrons & 3 & 10-100cm & prompt \\
\hline
Photons & 4 & 100cm & prompt \\
\hline
Fission product $\beta$ decay & 4 & short & delayed \\
\hline
Neutrinos &  5 & extremely long & delayed \\
\hline
Nonfission reactions from n capture & 4 & 100cm & delayed \\
\hline
\end{tabular}
\end{table}

Fission is technically considered a disappearance reaction as well since the fission-inducing neutron is absorbed by the nuclide.  Even though is is absorbed, more than 2 new neutrons are released by fission, enabling the fission chain reaction to occur.  An important parameter of the fission chain reaction is $nu$, the fission neutron yield.    The total yield is different than the prompt yield in that it also includes neutrons produced from photon-induced emission and from fission product decay.  These neutrons are not ``prompt'' in that they are not emitted immediately from the fission itself.  The processes that create these ``delayed'' neutrons (decay and nuclear relaxation) take time to occur.  Prompt neutrons appear immediately after fission, whereas delayed neutrons can appear anywhere between 0.6 to 80 seconds after.  The kinetics of a nuclear chain reaction depend heavily on the fraction the delayed neutrons which sustain the chain reaction.  Since they have a relatively large time delay, they cause the time between subsequent neutron generations to become longer.  This time between generations is called the \emph{effective neutron lifetime}, and is approximately $10^{-4}$ seconds for prompt neutrons and $0.1$ seconds for delayed neutrons in light water (thermal spectrum) reactors \cite{duderstadt}.  

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/nu_compare.eps}
     \caption{Total fission yield for uranium-235, uranium-238, and plutonium-239. \label{nu_compare}}
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/xs_fissile.eps}
     \caption{Fission cross sections for uranium-235, uranium-238, and plutonium-239. \label{xs_fission_only}}
\end{figure}

Figure \ref{nu_compare} shows the energy dependence of the total fission neutron yield, $nu_\mathrm{T}$. It is shown at one temperature since it have very weak temperature dependence.   It can be seen that it is constant until energies in the MeV range, where it increases sharply due to the energy the incident neutron provides being sufficient to eject a bound neutron.  Figure \ref{xs_fission_only} shows the total fission cross sections for two \emph{fissile} isotopes, uranium-235 and plutonium-239, and one \emph{fertile} isotope, uranium-238.  Fissile isotopes are those which neutrons at any energy can induce fission.  The figure shows that uranium-238 has a fission cross section, but it isn't significant until above 1 MeV.  For uranium-238, fission is a threshold reaction.  Simply absorbing a neutron does not provide enough energy to split the nucleus.  Additional energy is required, which can be provided in the form of an incident neutron's kinetic energy.  Uranium-238 isn't fissile, but it can be a significant contributor of fission reactions in reactors where the neutron population is \emph{fast}, i.e. mainly high-energy.  As mentioned before, uranium-238 is considered fertile, which means that it produces a fissile isotope after it absorbs a neutron, and in this case decays to plutonium-239.  

Fission spectrum?

\subsection{Other Inelastic Secondary-Producing Reactions}

This category encompasses all the other reactions neutrons undergo.  There are two types, ones that produce secondary neutrons in some amount and those that do not.  Those that do not may still produce other particles, like alpha particles, tritons, protons, et cetera.  Since they do not produce secondary neutrons, however, they are basically equivalent to a disappearance reaction in the scope of neutron transport.  Even though they aren't capture reactions, they can contribute significantly to an isotope's absorption of neutrons.  Figure \ref{xs_e_dependence_li} shows that the (n,$^3$H) in lithium-6 is by far the main component of the total cross section and makes it a very strong absorber of low energy neutrons.  Boron-10 is another great example of this.  Its (n,$\alpha$) reaction has a very high cross section for low energy neutrons, and it is widely used in thermal spectrum reactors in safety and control systems.

Of the reactions that produce secondary neutrons, the (n,2n) reaction is most significant, due to it having the lowest threshold energy.  At higher incident neutron energies, (n,3n) and even (n,4n) can become possible.  Other particle combinations are possible as well, such as (n,n$\alpha$), and these reactions act like an inelastic scatter interaction where the relationship between scattering angle and energy no longer applies due to there being three bodies to distribute energy to instead of only two.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Temperature Effects}
\label{sec:temp}

Thus, far there has only been mention of the target nuclide's by how it manifests itself in the 1/$v$ behavior of cross sections at low energy.  Most of the time, it is a good assumption that the thermal motion of the target nuclide is negligible.  Thermal energy is on the order of .01 eV, and this is a good assumption when neutrons are at MeV range energies, but when they scatter and lose energy, they can come near the thermal energy of the material.  When this happens, the target nuclide no longer appears stationary, and assuming that it has zero velocity in scattering calculations is inaccurate.  MCNP sets the threshold at 400kT, which corresponds to about 10 eV for materials at room temperature, above which the target nuclide can be assumed stationary.  Below this threshold, it is important to model thermal effects.  If this wasn't done, a neutron could keep scattering off of zero-energy targets and it's energy could approach zero, which is not physical.  Neutrons can only scatter down to a state where they are in thermal equilibrium with the material they are traveling through.  This creates a ``thermal peak'' at low energies where neutrons collect, especially in materials where the scattering cross section is large and neutrons scatter many times before they are absorbed.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/MB_dist.eps}
     \caption{Maxwell-Boltzmann distribution for the speed of heavy and light nuclides at different temperatures.   \label{MB_dist}}
\end{figure}


\subsection{Doppler Effect}


The other effect that thermal motion has is the \emph{Doppler effect}.  This is where resonances are broadened as temperature increases due to the thermal distribution becoming wider.  The effect is also known as \emph{Doppler broadening} for this reason.  Figure \ref{MB_dist} shows the Maxwell-Boltzmann distributions at various temperatures for a heavy particle and for a light particle.  This is the distribution of speeds particles in a ``gas'' have if they only interact by scattering off of one another.  Most solids can be modeled as a dense gas when there are no strong anisotropies in their structure, which is why modeling the target velocities in this way is called the ``free gas model.''  Note that the broadening effect is much more pronounced for light nuclei.  

The widening of resonances effects reactors in significant ways, most notably in that more neutrons are absorbed in the resonance region as the scatter down to thermal energies.  The number of neutrons lost to capture increases and reduces the overall multiplication factor.  It is important for reactor safety that this occurs since it produces a negative reactivity feedback for temperature and helps prevent power excursions and meltdowns.  If the multiplication factor is above unity, the power starts to rise, and therefore so does the temperature (if the coolant flow rate remains the same).  When the temperature goes up, the increased captures causes it to go back down, stopping the power from increasing.  Of course, there are many different types of reactivity feedback phenomena, but the temperature feedback is generally negative due to Doppler broadening.  Capture is increases most in fission products since they often have strong absorption resonances and are lighter than fuel nuclides.  Very light nuclides typically do not have absorption resonances, so Doppler broadening has littler effect on their absorption rates.  This is why temperature feedback is least effective in fresh fuel where there are few fission products.  Figure \ref{xs_eu_broaden} shows the effect in europium-155, a fission product with a high capture cross section.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/xs_eu_broaden.eps}
     \caption{Doppler broadening of the 1 eV fission resonance in Eu-155.  \label{xs_eu_broaden}}
\end{figure}

At 0K, resonances are very sharp and can be treated as delta functions.  The reaction rates actually depend on the relative velocity of the neutron and the target nuclide, and since the target nuclides have a spectrum of energies, the relative velocity does as well.  Assuming the target is at rest requires summing all the contributions to the reaction rate from the different relative velocities given a single neutron energy.  From the reaction rate, a \emph{thermally averaged} cross section can be calculated be used in simulations assuming a target at rest.  This is especially significant at resonances since slight movements in velocity can produce very large differences in cross section.  The overall effect is that sharp resonances are \emph{effectively} broadened since they start to contribute to reaction rates once the thermal distribution of relative velocities starts overlap them.   The expression for thermally averaged cross section is shown in \eqref{broaden}, where $v_n$ and $\boldsymbol{v_n}$ are the velocity and speed of the neutron, respectively, $\boldsymbol{v_t}$ is the velocity of the target, $v_\mathrm{rel} = || \boldsymbol{v_n} -\boldsymbol{v_t}||$ is the relative velocity of the neutron to the target, and $M(v_t)$ is the thermal distribution of target speeds.

\begin{equation}
\bar{\sigma} = \frac{1}{v_n} \int v_\mathrm{rel} \sigma(v_\mathrm{rel}) M(v_t) dv_t
\label{broaden}
\end{equation}

The exact method for doing this is shown in \eqref{broaden}\cite{Cullen_Weisbin_1976}.

\begin{equation}
\begin{split}
Find a copy\\
of the paper!
 \end{split}
\label{broaden_kernel}
\end{equation}

Then say some things about it once you get details, particularly that it is an expensive operation

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Nuclear Data}

Cross section data is distributed by the Department of Energy in \emph{ENDF} files.  ENDF stands for ``evaluated nuclear data file,'' and can contain data for nuclear decay, photons, atomic relaxation, fission yields, thermal neutron scattering, and charged particle reactions as well as neutron reactions.  They are called ``evaluated'' because an organization decides, or evaluates, which experimental data is the highest quality and will be included in the final dataset.  They also decide how to represent regimes that haven't been measured yet by comparing simulation results to experiments.   The first data released was ENDF/B-I in 1968 and the latest set is ENDF/B-VII, which was released in 2006.  They have a standard format that dates back to when the data was stored on magnetic tapes, and are sometimes referred to as ``tapes'' to this day.  The format is rather archaic and contains a lot of redundant information about record locations which was useful when the tape head had to physically move between points in the tape.  

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/data_levels.eps}
     \caption{Hierarchy in ACE formatted neutron data libraries.  \label{data_levels}}
\end{figure}

Many Monte Carlo codes read ACE formatted data rather than the original ENDF file.  ACE stands for ``a compact ENDF'' and strips out a lot of the extra information unnecessary for neutron transport.  ENDF tapes also contain information that is valuable for charged particles and photons as well as neutrons, and this information is discarded for neutron transport.  As Figure \ref{data_levels} shows, ACE files not only contain cross sections, but also angle and energy distributions used in scattering and fission.  ENDF assigns a number to each type of reaction called the \emph{MT} number.  Table \ref{MT_numbers}, taken from a LANL website, shows the major numbers \cite{MTnums}.  It can clearly be seen that there are many reactions a neutron can undergo, most of which have very energy strong dependance.  Most of the complexity in modeling nuclear reactors comes from the fact that the data needed to model neutrons is very complicated.  It may be the most important part of the simulation, it is what ties the calculations to reality.  If inaccurate data is used, the results will not have any physical meaning, and the simulation becomes a mathematical exercise. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Neutron Transport}

Now that the events which can happen to neutrons have been outlined, we will move to describing the neutron population itself.  Since the neutron population in reactors is large, on the order of $10^{XXX}/\mathrm{cm}^3$, and the neutrons themselves have very small radii, about $1.75\times10^{-17}$ cm, the distribution of neutrons can be treated as an continuum.  We will eventually derive the \emph{neutron transport equation}, which is a linearized version of the Boltzmann transport equation.  It is linear since it is assumed that neutrons do not interact with each other.  This is usually a good assumption in normal matter since the neutron density present in reactors is many orders of magnitude smaller than the material density and neutrons are much more likely to interact with it than each other.  

Other than eliminating neutron self interaction, there are a handful of other assumptions that go into the equation that will hold true for the rest of the derivations.  One that follows veery closely is that neutrons are assumed to be points in space, so even at very high densities they sill will not interact with each other, and later when we start to discretize spaces, neutrons cannot be in more than one unique volume by overlapping.  The next assumption is that any relativistic effects are negligible.  The energies of importance in reactor physics are below 10MeV, which is about where they start to become non-negligible for neutrons.  Since neutrons are neutral and have long interaction distances compared to their radii, they are also assumed to move in straight lines between collisions.  Materials are also assumed to be in thermal equilibrium and have isotropic properties.  Materials like graphic and water do not have isotropic properties when it comes to scattering, however.  As will be explained later, this can be corrected by adjusting the scattering kernel.

\subsection{Neutron Balance Equation}

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth,trim= 0cm 2.5cm 0cm 0cm]{graphics/diff_balance.eps} 
     \caption{The differential volume of the neutron balance equation. \label{diff_volume}}
\end{figure}

Before any explicit terms need to defined to capture the necessary physics of the transport problem, a balance equation can be written for a differential volume.  This equation simply describes the amount of neutrons entering and exiting a infinitesimally small volume and their difference being equal to the rate of change of neutrons in the volume.  We know about the reactions that neutrons can undergo, and we know they move through material, so we should have  enough information the write this abstract equation. Figure \ref{diff_volume} shows an illustration of the differential volume.

\begin{equation}
\footnotesize
\label{NBE}
\begin{array}{c}
\mathrm{rate\:of\:change} = (\mathrm{neutrons\:in}) - (\mathrm{neutrons\:out}) \\
\\
\frac{\partial n}{\partial t} = ( \mathrm{movement\:in} + \mathrm{source} + \mathrm{scatter\:in}) - ( \mathrm{movement\:out} + \mathrm{disappearance} + \mathrm{scatter\:out})
\end{array}
\end{equation}

Now that this equation has been written, we can start to explicitly define the terms.  One quantity has already been touched upon - the neutron density.  The neutron density is the fundamental quantity in reactor physics.  This is shown in \eqref{NBE} as $n$, but in the subsequent sections its dependencies will be elaborated on, as will all other dependencies.


\subsection{Neutron Distribution Function}

The \emph{neutron distribution function} simply describes the number of neutrons present in each differential quantity it represents.  The most detailed version is differential in space, angle, energy (or velocity), and time, and has inverse units of these dimensions.  In other words, it describes a density in each of it components.  In the subsequent derivations, the spatial, or position vector will be represented by $\boldsymbol{\vec{r}}$, the angular vector by $\boldsymbol{\hat{\Omega}}$, energy by $E$, and time by $t$.  The angle vector is a unit vector that specifies direction only, not magnitude.  It is easiest to express the directional vector as a unit vector in spherical coordinates $(\theta, \phi)$, the polar angle and the azimuthal angle, respectively.  Their cartesian projections are given by \eqref{ang_cart} and shown in Figure \ref{ang_relations}.

\begin{equation}
\label{ang_cart}
\begin{split}
\Omega_x &= \sin \theta \sin \phi  \\
\Omega_y &= \sin \theta \cos \phi \\
\Omega_z &= \cos \theta = \mu
\end{split}
\end{equation}

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth , trim= 0cm 2.5cm 0cm 0cm]{graphics/ang_relation.eps} % trim = l b r t
     \caption{The cartesian projections of the directional vector. \label{ang_relations}}
\end{figure}

Like any continuous distribution, it must be integrated over to calculate any quantities.  If we define the $i$th moment of the distribution according to \eqref{NDF_moments}, then the $0$th moment would be the population and the $1$st moment would be the mean.

\begin{equation}
\label{NDF_moments}
M_i = \int_{-\infty}^{\infty} n(x)  x^{i} dx
\end{equation}

For example, if we are given a neutron distribution which has no time dependence, $n(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E)$, and wanted to calculate the total number of neutrons present in a volume V, we would calculate this number by \eqref{NDF_sample}, whereas if we wanted to know the average energy, we would do this by \eqref{NDF_avg}.

\begin{equation}
\label{NDF_sample}
N_V = \int_0^\infty \int_{\boldsymbol{\hat{\Omega}}} \int_{V} n(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E) dV d\boldsymbol{\hat{\Omega}} dE
\end{equation}

\begin{equation}
\label{NDF_avg}
\bar{E} = \int_0^\infty \int_{\boldsymbol{\hat{\Omega}}} \int_{V} E n(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E) dV d\boldsymbol{\hat{\Omega}} dE
\end{equation}

\subsection{Reaction Rates}

A \emph{reaction rate} is the rate at which a certain reaction is happening in a volume.  If we go to a pulse-type scenario once more, we have $N_\mathrm{p}$ particles which travel at speed $v$ in one direction.  If $\Sigma$ is the interaction probability per unit length, multiplying it by the speed gives the probability of interaction per second, or the \emph{collision rate}.  Since there are $N$ particles, multiplying the rate by it gives the overall reaction rate, $N_\mathrm{p} v \Sigma$, of the pulse in an infinite medium.  If $N$ is now substituted for the neutron distribution function instead of a pulse, the expression now becomes the reaction rate per distribution differential, or the \emph{reaction rate density}.  This expression is shown in \eqref{RR} and is the first building block of the \emph{neutron balance equation}.

\begin{equation}
\label{RR}
R(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t)  = v(E) n(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) \Sigma(\boldsymbol{\vec{r}},E)
\end{equation}

If we assume there is only one energy, $E_0$ and one direction, $\hat{x}$, and integrate over the constant dimensions of this equation, as shown in \eqref{1d_diff}, we get an expression for the reaction rate in an infinitesimal slice dx.  $A$ is the area perpendicular to the direction of motion where the neutron population is nonzero.


\begin{equation}
\label{1d_diff}
\int_A \int_{\boldsymbol{\hat{\Omega}}} \int_0^{\infty}  dE d\boldsymbol{\hat{\Omega}} dxdydz  v(E) n(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) \Sigma(\boldsymbol{\vec{r}},E) \delta(E-E_0) \delta(\boldsymbol{\hat{\Omega}}-\hat{x})  = v \Sigma A n(x)dx 
\end{equation}

This is effectively the loss term for a beam in the $\hat{x}$ direction, and if we set it as such, we recover the beam-type expression \eqref{recover_beam}, which is equivalent to \eqref{pop_diff}.  

\begin{equation}
\label{recover_beam}
dn(x) = - v \Sigma A n(x)dx  \qquad \Rightarrow \qquad  \frac{dI(x)}{dx}= - \Sigma I
\end{equation}


\subsection{Angular and Scalar Flux}

Since the reactions rate density is dependent on the cross section and the velocity multiplied by the neutron distribution, this quantity is redefined as the \emph{angular flux} as shown in \eqref{ang_flux}.  Flux meaning that it represents a rate at which particles are passing through a surface, and angular meaning that it is differential in angle.  Again, since the reactions rates depend on this quantity, the neutron transport problem is usually written in terms of the angular flux, which is then solved for instead of the neutron distribution.

\begin{equation}
\label{ang_flux}
\psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) = v(E) n(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t)
\end{equation}

Scattering and neutron movement have angular dependence, but reaction cross sections do not, so the reactions can be written in terms of angular flux which has been integrated over all angles, or the \emph{scalar flux} or simply the \emph{flux}.  The relation between the two fluxes is show in \eqref{scalar_flux}.  This is usually the most interesting quantity in reactor physics since the reactor power profile is directly proportional to it.  The reaction rate for a reaction $i$ is shown in \eqref{scalar_flux_RR}.

\begin{equation}
\label{scalar_flux}
\phi(\boldsymbol{\vec{r}},E,t) = \int_{4\pi} \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t)
\end{equation}

\begin{equation}
\label{scalar_flux_RR}
 R_i(\boldsymbol{\vec{r}},E,t) = \int_{4\pi} \Sigma_i(\boldsymbol{\vec{r}},E) \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) = \Sigma(\boldsymbol{\vec{r}},E) \int_{\boldsymbol{\Omega}} \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) = \Sigma(\boldsymbol{\vec{r}},E) \phi(\boldsymbol{\vec{r}},E,t)
 \end{equation}
 
 From \eqref{NBE} we can see that the time derivative is in terms of the neutron density, not the angular flux.  Since reaction rates are proportional to angular flux, the balance equation will be written in terms of it.  This mean that the time dependent term must be transformed to angular flux by multiplying and dividing it by the velocity as shown in \eqref{time_depedent_flux}.

\begin{equation}
\label{time_depedent_flux}
\frac{\partial }{\partial t}n(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) = \frac{\partial }{\partial t}  \frac{v(E)}{v(E)} n(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) =  \frac{\partial }{\partial t}  \frac{\psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t)}{v(E)} = \frac{1}{v(E)} \frac{\partial }{\partial t}\psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t)
\end{equation}

\subsection{Scattering}

Scattering is highly dependent on angle and energy and requires as more detailed cross section than disappearance reactions.  Elastic scattering has a fixed relation between outgoing energy and angle, but inelastic scattering does not.  Scattering moves a neutrons energy and angle, so to be completely general in describing it, the cross section is assumed to not only ne dependent on incoming energy like all other cross sections, but also on outgoing energy, incoming angle, and outgoing angle.  Since two quantities are being related before and after a scattering event, the scattering cross section is considered \emph{doubly differential} and is sometimes called the \emph{scattering kernel} whereas the integrated value which only depends on incoming energy is called the scattering cross section.   An expression for the scattering kernel is shown in \eqref{scattering_DD_xs} with the incoming values primed.

\begin{equation}
\label{scattering_DD_xs}
\Sigma_s = \Sigma_s(E^\prime \rightarrow E,\boldsymbol{\hat{\Omega}}^\prime \rightarrow \boldsymbol{\hat{\Omega}})
 \end{equation}

There is a practical reason for separating scattering into a total cross section that describes its likelihood, or probability of a neutron entering the reaction, and a kernel that describes how it exits.  In a Monte Carlo simulation, the cross section is used to determine \emph{whether} scattering happens rather than \emph{how} it happens.  The scattering kernel is only used if a neutron has already been determined to scatter.  This is also shown in the expressions for the scattering in a differential volume.  The cross section is used as the removal of a neutron from a particular energy, whereas the kernel is used to determine from which other energies and angles a neutron is scatter \emph{into}.  This can be seen in \eqref{scattering_DD_xs}.  If the outgoing energy and angle are held constant, the quantity is the probability which other angles and energies scatter into it.

Using these two quantities, source and loss terms can be written for scattering.  The expression for loss uses the cross section, shown in \eqref{scattering_loss}, and the kernel is used in what is often called the \emph{scattering source} term, shown in \eqref{scattering_source}.  The source term need to be integrated over all other energies $E^\prime$ and angles, $\boldsymbol{\hat{\Omega}}^\prime$, which neutrons can scatter into energy $E$ and angle $\boldsymbol{\hat{\Omega}}$.

\begin{equation}
\label{scattering_loss}
R_{s, \mathrm{loss}}( \boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t ) = \Sigma_s (\boldsymbol{\vec{r}},E) \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t)
 \end{equation}
 
 \begin{equation}
\label{scattering_source}
R_{s, \mathrm{source}}(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) = \int_0^\infty  \int_{4\pi} \Sigma_s(E^\prime \rightarrow E,\boldsymbol{\hat{\Omega}}^\prime \rightarrow \boldsymbol{\hat{\Omega}}) \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}}^\prime,E^\prime,t) d\boldsymbol{\hat{\Omega}}^\prime dE^\prime
 \end{equation}
 
 \subsection{Fission Source}

The nuclear chain reaction is sustained by fission reactions producing neutrons, so it is essential to model it in any material that has fissionable isotopes.  There are three determining factors for the fission source.  The first is the fission reaction rate, which is represented by the flux multiplied by the fission cross section.  The second is the fission spectrum, which is represented by $\chi$.  Neutrons born from fission are not emitted at a single energy, and the fission spectrum describes the probability for a neutron to be emitted at a certain energy.  The energy spectrum is weakly dependent of the incoming neutron energy, which higher incoming energies producing more higher energy neutrons.  This dependence only starts making a significant difference in spectrum shape at energies above 10MeV, higher than energies usually seen in a reactor.  This is why the fission spectrum is usually treated as being independent of incoming neutron energy.  The spectra of uranium-235 and plutonium-239 for fission induced by a 0.5 MeV neutron are shown in Figure \ref{fiss_spec}, and it can be seen that plutonium-239 produces more high energy neutrons.  The last parameter is the average number of neutrons emitted in a fission event, or the \emph{fission yield}, represented by $\nu$, and is relatively flat until about 1MeV, as was shown in Figure \ref{nu_compare}.  Since 1MeV is within the typical energy range present in nuclear reactions, it is treated as a function of energy.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth ]{graphics/fiss_spec.eps} 
     \caption{The prompt fission spectrum for uranium-235 and plutonium-239. \label{fiss_spec}}
\end{figure}

To write an expression for the fission source, or the number of fission neutrons born in energy $E$, we must know the number of neutrons born from fission at all energies.  Once this is known, it is multiplied by the fission spectrum to select the fraction that will be in energy $E$.  The expression for the fission source is shown in \eqref{fiss_source}.

\begin{equation}
\label{fiss_source}
\frac{\chi(E)}{4\pi} \int_0^\infty  \int_{4\pi}   \nu(E^\prime) \Sigma_f(\boldsymbol{\vec{r}},E^\prime) \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}}^\prime,E^\prime,t) d\boldsymbol{\Omega}^\prime  dE^\prime = \frac{\chi(E)}{4\pi} \int_0^\infty   \nu(E^\prime) \Sigma_f(\boldsymbol{\vec{r}},E^\prime) \phi(\boldsymbol{\vec{r}},E^\prime,t)  dE^\prime
 \end{equation}


\subsection{Streaming}

So far all that has been  touched on is how neutrons within a volume move in angle and energy or are removed outright.  Now we will go over how they move in space, and to do this we consider a surface $S$ around our differential volume.  We want to find an expression for the net number of neutrons passing through this surface and this will be our net leakage term (incoming minus outgoing).  The angular neutron flux describes the number of neutrons crossing a differential surface at angle $\boldsymbol{\hat{\Omega}}$, but in order to perform any vector operations on it, we must multiply it by the unit vector.  We can then write and expression for the leakage by performing a surface integral over the current and surface normal's dot product, shown in \eqref{leakage_surface}.

\begin{equation}
\label{leakage_surface}
\mathrm{Leakage} = \int_S ds \cdot \boldsymbol{\hat{\Omega}} \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t)
\end{equation}
 
 We can turn this into a volume integral by applying the divergence theorem, shown in \eqref{div_theorem}.  If the volume of interest is shrunk to an infinitesimal volume and we switch the order of the dot product, we come to an expression that can be used in the balance equation.  This expression, shown in \eqref{streaming}, is often called the \emph{streaming} term, since it describes the net movement of neutrons into the differential volume due to their physical movement.
 
\begin{equation}
\label{div_theorem}
\int_S ds \cdot \boldsymbol{\hat{\Omega}} \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) = \int_V dV \nabla \cdot \boldsymbol{\hat{\Omega}}  \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t)
\end{equation}

\begin{equation}
\label{streaming}
 \lim_{V\to dV} \int_V dV \nabla \cdot \boldsymbol{\hat{\Omega}}  \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) =  \boldsymbol{\hat{\Omega}}  \cdot \nabla\psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) 
 \end{equation}
 

\subsection{Neutron Transport Equation}

Now all the terms of \eqref{NBE} have been explicitly defined.  Substituting them all in yields \eqref{NTE}, the \emph{neutron transport equation}, written here with the neutron sinks on the left side and the neutron sources on the right side.  An additional external source term, $S_{\mathrm{external}}$, has been added to account for any sources not induced by the neutron flux itself, i.e. external sources.  

\begin{equation}
\label{NTE}
\begin{split}
\frac{1}{v(E)} \frac{\partial }{\partial t}\psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) &+  \\
\boldsymbol{\hat{\Omega}}  \cdot \nabla \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) &+ \\
\Sigma_t(\boldsymbol{\vec{r}},E) \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) & \\
& =  \\
& \quad \int_0^\infty  \int_{4\pi} \Sigma_s(E^\prime \rightarrow E,\boldsymbol{\hat{\Omega}}^\prime \rightarrow \boldsymbol{\hat{\Omega}}) \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}}^\prime,E^\prime,t) d\boldsymbol{\Omega}^\prime dE^\prime  \\
&+ \frac{\chi(E)}{4\pi} \int_0^\infty  \int_{4\pi}   \nu(E^\prime) \Sigma_f(\boldsymbol{\vec{r}},E^\prime) \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}}^\prime,E^\prime,t) d\boldsymbol{\Omega}^\prime  dE^\prime\\
& + S_{\mathrm{external}}
\end{split}
 \end{equation}
 
 The neutron transport equation in this form is an integro-differential equation since it has both derivates and integrals in it.  It's spatial parts are differential, whereas its angular and energy parts are integral.  It is linear and relatively easy to solve for simple geometries and reaction parameters.  The complexity in solving it comes from the complex energy dependence of the reaction parameters and the large and complex domains over which it must be solved in order to capture all the relevant physics.  The reaction parameters can span more then 12 orders of magnitude, from $1\times 10 ^{-11}$ to $1\times 10 ^{1}$ MeV and above, and the geometries involved can include millions of individual material regions with many different mixtures of material within them.
 
 \subsection{Time Independent Neutron Transport Equation}

In most situations, the equilibrium state of the neutron population is of interest.  By setting all time derivates to zero, \eqref{NTE} becomes the time-independent neutron transport equation shown in \eqref{time_ind_NTE}.  Another parameter has been introduced, the effective multiplication factor, $k_\mathrm{eff}$, which was described earlier using the seven factor formula.  It is introduced here since by forcing the time derivates to be zero, it is implied that the multiplication factor must be equal to one.  The material and geometric properties of a problem may say otherwise, however, and failing to divide the fission source by the effective multiplication factor could make time independent equation inconsistent.  Of course, there will be no inequality if the source and sink terms are actually balanced and $k_\mathrm{eff}$ calculated from \eqref{NTE} is one.

\begin{equation}
\label{time_ind_NTE}
\begin{split}
\boldsymbol{\hat{\Omega}}  \cdot \nabla \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) &+ \\
\Sigma_t(\boldsymbol{\vec{r}},E) \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}},E,t) & \\
& =  \\
& \quad \int_0^\infty  \int_{4\pi} \Sigma_s(E^\prime \rightarrow E,\boldsymbol{\hat{\Omega}}^\prime \rightarrow \boldsymbol{\hat{\Omega}}) \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}}^\prime,E^\prime,t) d\boldsymbol{\Omega}^\prime dE^\prime  \\
&+ \frac{1}{k_{\mathrm{eff}}}\frac{\chi(E)}{4\pi} \int_0^\infty  \int_{4\pi}   \nu(E^\prime) \Sigma_f(\boldsymbol{\vec{r}},E^\prime) \psi(\boldsymbol{\vec{r}},\boldsymbol{\hat{\Omega}}^\prime,E^\prime,t) d\boldsymbol{\Omega}^\prime  dE^\prime\\
& + S_{\mathrm{external}}
\end{split}
 \end{equation}
 

\section{Discrete Methods}

The usual method in solving continuous differential and integral equations on a computer is to discretize the domain over which it is being solved.  This can be applied to the neutron transport equation as well.  Since energy is involved in integrals in the neutron transport equation, it can be discretized by integrating over set boundaries and treating the reaction parameters as constant within them.  The same method can be applied to them angular dependence.   The simplest way of discretizing the geometry is to voxelize the domain, laying down a uniform grid in every dimension and treating the material within each voxel as being uniform.  Discrete geometry representation is and area where an lot of error can occur since it is three dimensional and scales can range from millimeters to hundred of meters. 

derivations?  moments, diffusion, SN?

Although discrete methods are readily mapped to computer hardware and execute quite fast, there are drawbacks and limitations in using them.  The biggest drawback is that they're accuracy greatly depends and the discretization methods used.  ...Do I really even need to include this section?  Please advise.

\section{Monte Carlo}

The Monte Carlo method takes a different approach in mapping the transport problem to a computer.  Instead of the neutron population being continuous and the spatial, angular, and energy dimensions being discretized, it leaves the dimensions continuous and discretizes the neutron population!   Quantities of interested are then determined by integrating over the neutron population!   This way of integrating the neutron transport equation can be thought of as integrating it ``sideways.''   Deterministic approaches integrate it by discretizing the dimensions and ensuring consistency of the transport equation between the discrete points.  This is the ``standard'' way of integrating the transport equation.  In the Monte Carlo method, the discretized neutron ``shoots through'' the transport equation many times, sweeping out the entire phase space, effectively integrating it when a sum is done over the individual neutron realizations, or \emph{histories}.  It is important to note that the Monte Carlo method does not actually solve the transport equation, per se, but rather is able to \emph{estimate} the solution very well.

The Monte Carlo approach makes a physically analogous ``experiment'' on a computer.  A computer thread ``sits on top'' of a neutron as it makes a \emph{random walk} through the geometry.  The computer thread uses pseudo-random numbers to sample probability distributions that describe the interactions the neutron makes as it travels.  All the assumptions that went into deriving the neutron transport equation still hold, most importantly that neutrons travel in straight lines between interactions, the neutrons are points, and they do not interact with each other.  The other assumption is that interactions happen instantaneously and at a point, i.e. they do occur over a distance.  It is very important how surface crossing detected in the simulation since this is what changes the material data where the probability distributions come from.  WARP, in its current state, uses the traditional form of surface detection and material updates, ray tracing, the details of which will be discussed in Section \ref{sec:prelim}.  When a neutron is sampled to cross a surface, it is placed on the boundary, the material data is updated, and the interaction distance is sampled again with the neutron traveling in the same direction.  Figure \ref{random_walk} shows a cartoon of a random walk of a neutron born in the center of a cube.  The line colors represent a neutron sampling a specific material, the red ``X'' represents an absorption (walk termination), and the green ``X'' represents a surface crossing.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth,trim= 0cm 2.5cm 0cm 0cm]{graphics/random_walk_accepted.eps}
     \caption{The Monte Carlo random walk process. \label{random_walk}}
\end{figure}

There are advantages in using the Monte Carlo method for neutron transport, some of which have been mentioned.  The first and foremost is that it that very few assumptions must be done to the physics and the geometry of the problem. Paraphrasing Forrest Brown when he spoke at the Supercomputing in Nuclear Applications and Monte Carlo conference on October 30th, 2013 in Paris,  the only reason people use Monte Carlo methods is that they are able to produce physically accurate results and are trusted to so so.  Anyone developing Monte Carlo code should not abuse this trust and must put accuracy first and foremost in their implementations.

There are considerable drawbacks to using Monte Carlo as well; primarily convergence rates and silent inaccuracies.  Since it is a statistical calculation, it is bound by statistical laws which dictate that the error in Monte Carlo calculations reduces as $1/\sqrt{N}$, where N is the number of histories performed.  Also, since geometries can be large, very small but very influencing volumes can be missed by the neutron random walk if the number of histories is not large enough.  The estimated statistical error may be low at $N$ histories, but there is a chance the entire phase space has not yet been sufficiently sampled to produce accurate results.  Deterministic methods would not have this problem since there will be a discretized point in this small volume, and it will contribute to the overall calculation.  The slow convergence rate can be combated by using parallel computing, however.  Requiring that neutrons do not interact with each other was an assumption that went into deriving the neutron transport equation, and can be exploited in Monte Carlo simulations.  Since each neutron history is completely independent, histories can be run completely in parallel without communication until the results are combined at the end of the simulation.  This leads to very good scaling of parallel calculations, which can be used to reduced run times of large problems to acceptable values.

\subsection{Statistics}
\label{sec:stat}

To understand how the Monte Carlo method performs this integration, simple equations and derivations and such as calculating $\pi$ can be done.   First, given a continuous random variable $x$ that follows the probability distribution $P$, the mean value can be calculated by taking the first moment as was shown in \eqref{NDF_moments} and is reproduced in \eqref{int_mean}.  But we are interested in determining the mean from a set measurements rather than the underlying distribution.  If we have $N$ independent measurements of quantity $x$, this is simply taking the arithmetic mean, as shown in \eqref{arith_mean}.

\begin{equation}
\label{int_mean}
\mu = \int_{-\infty}^{\infty} x P(x) dx
\end{equation}

\begin{equation}
\label{arith_mean}
\bar{X}_N = \frac{1}{N} \sum_i^N x_i
\end{equation}

The shape of the distribution about the mean is important as well, since it quantifies the uncertainty.  The variance can be computed by performing the central second moment on $P$ and estimated by Bessel's Correction on the standard variance estimator \eqref{varience_est}\cite{}.  It is useful that the sample mean can be removed from the sum of the sample squares since this means that only the sample sum and the sum of sample squares need be stored.  The entire sample set does not need to be stored and used to compute the variance at the end when the sample mean is known.

\begin{equation}
\label{varience}
\sigma^2 = \int_{-\infty}^{\infty} x^2 P(x) dx- \mu^2
\end{equation}

\begin{equation}
\label{varience_est}
\sigma_N^2 =  \frac{1}{N-1} \sum_i^N (x_i-\mu)^2 =  \frac{1}{N-1} \left( \sum_i^N x_i^2-N\bar{X}_N^2 \right)
\end{equation}

The law of large number states that \eqref{arith_mean}, the sample mean, will converge to the true mean \eqref{int_mean} in the limit where $N\rightarrow\infty$.  

\begin{equation}
\label{LLN}
Pr\left(\lim_{N\rightarrow\infty} \bar{X}_N = \mu \right) =0
\end{equation}

The rate at which the estimate \eqref{arith_mean} converges to the true mean \eqref{int_mean} comes from the central limit theorem.  Shown in \eqref{CLT}, it states that the distribution of the means of taken from a large set of independent random variables will be normally distributed.  

\begin{equation}
\label{CLT}
\sqrt{N}\left(\left(\frac{1}{N} \sum_i^N x_i \right)-\mu\right) \xrightarrow[]{d} \mathcal{N}(0,\sigma^2)
\end{equation}

In this case, if a large number of measurements are taken, we would like to know how close the mean of a completely independent set measurements will be to the true mean.  In other words, the variance of the mean is desired.  Luckily, this can be estimated with only one measurement instead of directly computing the variance with many measurements.  The Bienaym\'e formula, shown in \eqref{bien}, states that the variance of a sum of uncorrelated random variables is equal to the sum of the variances of the variables.

\begin{equation}
\label{bien}
Var\left(\sum_{i=0}^N x_i \right) = \sum_{i=0}^N Var(x_i)
\end{equation}      


Applying the Bienaym\'e formula and the variance relation show in \eqref{basic_var_scaling} to the sample mean results in an expression for the variance of the mean, $Var(\bar{X}_N)$, in terms of the variance of the sample, $\sigma_N^2$.  This expression is shown in \eqref{sum_var}.  

\begin{equation}
\label{basic_var_scaling}
Var\left(a X \right) = a^2 Var\left( X \right)
\end{equation}

\begin{equation}
\label{sum_var_1}
Var(\bar{X}_i) = Var\left(\frac{1}{N}\sum_{i=0}^N x_i \right) = \frac{1}{N^2} \sum_{i=0}^N Var(x_i) = \frac{N\sigma_N^2}{N^2} =  \frac{\sigma_N^2}{N} 
\end{equation}

\begin{equation}
\label{sum_var}
 \frac{\sigma_N^2}{N} = \frac{1}{N(N-1)} \left( \sum_i^N x_i^2- N\bar{X}_N^2 \right) = \frac{1}{(N-1)} \left( \frac{1}{N} \sum_i^N x_i^2 - \left(   \frac{1}{N} \sum_i^N x_i \right)   \right)
\end{equation}

It should be noted that \eqref{sum_var_1} implies that the standard deviation of the mean always scales as $1/\sqrt{N}$.  This is a blessing and a burden in that it means any calculation's variance of the mean will go to zero, i.e. the sample mean will converge to the true mean, as long it is run long enough and enough samples are collected, but that it will converge as $1/\sqrt{N}$, which is slow.

Since the central limit theorem states that it is normally distributed, we can make use of the well-know properties of the normal distribution, namely the confidence interval.  The normal distribution has well-defined confidence intervals: 68\% of the population will lie within a single standard deviation, $\sigma$, and 95.5\% will lie within $2\sigma$.  From this knowledge, an expression for the realtive error can be written.  The expression for the relative error shown in \eqref{rel_err} is for the 68\% confidence level, and simply needs to be doubled for the 95\% level \cite{mcnp}\cite{jaakko}.

\begin{equation}
\label{rel_err}
\mathrm{Rel. Err.} = \frac{\sigma}{\bar{X}_N} = \frac{1}{\bar{X}_N}\sqrt{\frac{1}{(N-1)} \left( \frac{1}{N}\sum_i^N x_i^2-\bar{X}_N^2 \right)}
\end{equation}

\subsection{Sampling Schemes}

In order to actually transport a neutron across the geometry in a Monte Carlo simulation, a the probability distributions of the reaction types must be sampled in order to produce accurate distributions when the histories are aggregated.  There are two methods that are commonly used to do so; the direct inversion method and the rejection method.  

The \emph{direct inversion} method relies and being able to analytically integrate the probability distribution function (PDF) to create a cumulative distribution function (CDF) and being able to invert it.  The first step is to integrate the PDF to find the CDF, as shown in \eqref{CDF}.

\begin{equation}
\label{CDF}
CDF(x) = \int_0^x PDF(x^\prime) dx^\prime
\end{equation}

Since a PDF must be positive and normalized by definition, the CDF must range from 0 to 1 and increase monotonically.  Setting the CDF equal to a uniformly distributed random number $\xi$ and solving for the value to be sampled yields the sampling scheme.  Figure \ref{direct_samp} shows this graphically.  The CDF describes the probability that a value of a random number $x$ obeying the PDF will be below the value $x$.  Taking an interval gives the probability $x$ will be in the interval, as shown in \eqref{CDF_property1}.  Since the width of inverse CDF is directly proportional to the PDF at the same value of $x$, as shown in \eqref{CDF_property2}, and if $\xi$ is uniformly distributed on [0,1), the interval $\Delta \xi$ will contain a fraction of samples on [0,1) equal to $P( x_1 < x_i < x_2)$.  As the interval is reduced to 0, $P(x_i)= PDF(x)dx = d\xi$ (again, since $\xi$ is uniformly distributed on [0,1), leading to the sampling scheme shown in \eqref{CDF_inversion}.  This method is only useful if the CDF is simple enough to be inverted analytically.

\begin{equation}
\label{CDF_property1}
CDF(x_2) - CDF(x_1) = P( x_1 < x_i < x_2) = \int_{x_1}^{x_2} PDF(x) dx
\end{equation}

\begin{equation}
\label{CDF_property2}
CDF(x_2) - CDF(x_1) = \Delta \xi =  \int_{x_1}^{x_2} PDF(x) dx
\end{equation}

\begin{equation}
\label{CDF_inversion}
 x_i = CDF^{-1}(\xi_i)
\end{equation}

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/direct_samp.pdf}
     \caption{Sampling $PDF(x)=\frac{1}{2\pi^2}(x \cos x - x)$ by the direct inversion method on the domain $(0,2\pi)$. \label{direct_samp}}
\end{figure}

When the CDF is not invertible, the \emph{rejection sampling} method can be used.  It uses a secondary PDF, $f{x}$, that is greater at all points than the PDF to be sampled from and whose CDF is easily inverted.  Since the PDF is normalized, $f(x)$ now corresponds to probabilities greater than one.  This is simply a scaling problem, and the random numbers used to directly sample from it must be uniform on $[0,\int_\mathrm{domain}f(x))$ instead of $[0,1)$.   Then two random numbers are generated.  The first, $\xi_1$ is sampled from a $f(x)$ using a scaled random number as mentioned, and the second, $\xi_2$ is uniformly distributed on $[0,f(\xi_1))$.  If $\xi_2 < PDF(\xi_1)$, the sampled value $(\xi_1)$, is added to the population, else it is \emph{rejected}, or discarded from the population.  The set of accepted values of ${\xi_2}$ will then follow $PDF(x)$.  Figure \ref{rejection_samp} shows an illustration of rejection sampling.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/rejection_samp.pdf}
     \caption{Sampling $PDF(x)=\frac{1}{2\pi^2}(x \cos x - x)$ by the rejection method on the domain $(0,2\pi)$.  The auxiliary function $f(x)=x/\pi$ and the random number used to sample from it must be on $[0,2\pi)$ instead of $[0,1)$. \label{rejection_samp}}
\end{figure}

Using these two methods of sampling from probability distributions, expressions can be found for every reaction type and transport phenomenon that a neutron undergoes in it's random walk through matter.  The specific schemes are described in the following subsections.

\subsubsection{Cross Section Interpolation}

When a Monte Carlo simulation is referred to as ``Continuous Energy,'' it means that the cross sections are evaluated on the fly at the exact energy the neutron is currently at instead of using a fixed value for a range of energies.  The cross section data is finite, of course, and only includes data at discrete points in energy.  Therefore, interpolation must be performed between the data points.  This is done by linear interpolation.  It assumes that the cross section is a straight line between the surrounding energy points, $E_i$ and $E_{i+1}$, and the interpolated value is calculated using the slope of the line via \eqref{xs_interp}.

 \begin{equation}
\label{xs_interp}
\begin{gathered}
E_i < E < E_{i+1} \\
\sigma(E) = \frac{E-E_i}{E_{i+1}-E_i}(\sigma(E_{i+1})-\sigma(E_i)) + \sigma(E_i)
\end{gathered}
\end{equation}

\subsubsection{Tabular Distribution Interpolation}

For reactions that have any exiting neutrons, like scattering and fission, there are probability tables that specify at what probability certain angles and energies will be emitted.  These tables are typically have a CDF specified in a tabular format.  To sample from these tabular CDFs, a random number $\xi$ is generated.  Again, since the data is discrete, some type of interpolation must be performed to generated a value between the data points.  There are two methods specified in the ENDF format.  The first is histogram interpolation, which is similar to the linear cross section in the previous section.  It assumes the CDF is a line in between the data points, and the interpolated value is calculated to lie on this line.  A tabular PDF is also usually specified, but is not used in the actual computations in this study since the PDF value can be computed from the CDF and doing so reduces the global memory access of a GPU kernel.  Histogram interpolation is shown in \eqref{histo_interp} where $C_{i}$ is the value of the CDF at point $i$.  The outgoing cosine, $\mu^\prime$, is used, but this method can be used to sampled a CDF of any quantity.  $\mu^\prime_i$ represents the corresponding value of $\mu^\prime$ in the table for $P_i$ and $C_i$.

 \begin{equation}
\label{histo_interp}
\begin{gathered}
C_i < \xi < C_{i+1} \\
 \mu^\prime(\xi) = \frac{\xi-C_i}{C_{i+1}-C_i}(\mu^\prime_{i+1}-\mu^\prime_i) + \mu^\prime_i
\end{gathered}
\end{equation}

The second method is called linear-linear interpolation.  Instead of assuming the CDF is linear, it assumes the first derivative (the PDF) is linear over the interval $C_i < \xi < C_{i+1}$.  The linear expression for the PDF as in \eqref{lin_pdf} is substituted into \eqref{CDF_property1} and integrated to $\mu^\prime$.  Again this derivation is shown for $\mu^\prime$, but is valid for any CDF.

\begin{equation}
\label{lin_pdf}
\begin{gathered}
C_i < \xi < C_{i+1} \\
P(\mu^\prime) = \frac{P_{i+1}-P_i}{\mu^\prime_{i+1}-\mu^\prime_i}(\mu^\prime - \mu^\prime_i) + P_i = a \mu^\prime + b
\end{gathered}
\end{equation}

Integration of \eqref{lin_pdf} to $\mu^\prime$ yields \eqref{lin_pdf_int}.  This expression for $CDF(\mu^\prime)$ is set equal to the random number $\xi$ and solved for $\mu^\prime$ to give the interpolation function.

\begin{equation}
\label{lin_pdf_int}
C(\mu^\prime) = C_i  + \int_i^{\mu^\prime} P(\mu^{\prime\prime}) d\mu^{\prime\prime}  = C_i  + \frac{a}{2}(\mu^{\prime 2}-\mu^{\prime 2}_i) + b(\mu^\prime-\mu^\prime_i) = \xi 
\end{equation}

Taking advantage of the relationship between $a$ and $b$, the interpolation function can be simplified to \eqref{lin_lin} \cite{openmc}.

\begin{equation}
\label{lin_lin}
\begin{gathered}
a = \frac{P_{i+1}-P_i}{\mu^\prime_{i+1}-\mu^\prime_i} \qquad , \qquad b = P_i - a \mu^\prime_i \\
\mu^\prime = \mu_1 + \frac{1}{a}\left( \sqrt{P^2_i + 2a(\xi -C_i)} -P_i \right)
\end{gathered}
\end{equation}

Which interpolation method is used is specified in the data on a case-by-case basis.

\subsubsection{Distance To Interaction}

The first step in stochastically transporting a neutron through matter is finding the distance to the next interaction.  The expression for the probability of non-interaction was shown in \eqref{pop_NI}.  The probability of interaction in $dx$ around $x$ is $\Sigma_t dx$.  Therefore the probability of not interacting from 0 to x and then interacting at $dx$ around $x$ is simply the product.  This expression, shown in \eqref{first_col}, is commonly referred to as the \emph{first collision probability}.

\begin{equation}
\label{first_col}
P(x) = \Sigma_t e^{- \Sigma_t  x}
\end{equation}

This expression is simple and can be inverted for direct sampling.  The sampling scheme is shown in \eqref{first_col_samp}.  The expression $1-\xi$ can be replaced with $\xi$ since it is identically distributed.

\begin{equation}
\label{first_col}
CDF(x) = \int_0^x \Sigma_t e^{- \Sigma_t  x^\prime} dx^\prime = 1- e^{- \Sigma_t  x} 
\end{equation}

\begin{equation}
\label{first_col_samp}
x_i=CDF^{-1}(\xi) \qquad \Rightarrow \qquad x_i=\frac{-\ln(1-\xi) }{\Sigma_t}=\frac{-\ln(\xi) }{\Sigma_t}
\end{equation}

The total cross section $\Sigma_t$ is the total macroscopic cross section of the material the neutron is traveling through, i.e. it is the sum of the total macroscopic cross sections of its constituent isotopes as was shown in \eqref{material_sum_xs}.

\subsubsection{Isotope and Reaction Selection}

Now that the neutron has been determined to interact, it must be decided with which isotope it interacts with.  The discrete PDF of this process is shown in \eqref{isotope_selection}, where there are $i$ isotopes in the material each having a total macroscopic cross section $\Sigma_{t,i}$.  The distribution is sampled by generating a uniform random number, $\xi$, on $[0,1)$ and performing a running sum over the individual isotope's total macroscopic cross sections.  When $CDF_i > \xi$, the neutron collision is sampled to happened in isotope $i$.

\begin{equation}
\label{isotope_selection}
PDF_i = \frac{\Sigma_{t,i}}{\Sigma_t} \qquad \Rightarrow \qquad CDF_i = \frac{1}{\Sigma_t } \sum_{n=1}^i \Sigma_{t,n}
\end{equation}

Determining the reaction type is done in a similar fashion, except the number density of the material is no longer a concern since the isotope has already been selected.  Therefore, the running sum of the CDF is done over the isotope $i$'s microscopic cross sections.  Another random number is generated, and when $CDF_k > \xi$, the neutron is sampled to undergo reaction $k$ in isotope $i$.

\begin{equation}
\label{reaction_selection}
PDF_k = \frac{\sigma_k}{\sigma_t} \qquad \Rightarrow \qquad CDF_k = \frac{1}{\sigma_{t} } \sum_{n=1}^k \sigma_{n}
\end{equation}

\subsubsection{Free Gas Treatment}

As stated previously, the data in the cross section libraries are broadened for a certain material temperature.  This is done to account for the effect the nuclides' thermal motion has on the apparent width of resonances even when they are modeled as stationary.  Modeling the targets as stationary is a good approximation when the neutron has much larger larger velocities, but such a model would allow a neutron to scatter down to absolute zero.  In reality, they scatter until they come into thermal equilibrium with the target nuclides.  Once they are in equilibrium, on average they lose the same amount of energy as they gain from scattering events and their population becomes stationary in energy.  

To preserve this collection phenomena, or \emph{thermal peak}, in Monte Carlo simulations, the velocity of the target nuclide must be explicitly modeled.  Elastic scattering is the only interaction that has a neutron in the exit channel at thermal energies, and therefore is the only reaction that needs to consider target velocity.  Fission produces secondary neutrons in distributions uncorrelated with incoming neutron energy or target velocity at thermal energies, so the target motion can be disregarded.  Since the doppler-broadened cross sections produce accurate reaction rates when the targets are modeled to be stationary, this quantity must be preserved when giving the targets a nonzero velocity.  The targets follow a Maxwell-Boltzmann speed distribution, but this cannot be sampled directly for simulation purposes due to the doppler broadening preprocessing done to the cross section data.  Near resonances, portions of the thermal distribution within the resonance contribute much more to the overall reaction rate than the portions outside of it.  Directly sampling a target velocity from the thermal distribution would produce incorrect exiting neutron energies.  Rather the distribution of target velocities which contributed to the reaction rate (and therefore the thermally averaged cross section) must be calculated and sampled from.

The only use for the target velocity is in scattering kinematics to reproduce an accurate thermal peak while maintaining reaction rates.  For the sake of completeness, the following derivation is included and follows the OpenMC documentation closely which in turns follows Gelbard closely \cite{openmc}\cite{gelbard}.  The reaction rate in terms of the target and neutron velocities is shown in \eqref{RR_vel} and is equivalent to \eqref{broaden} in Section \ref{sec:temp}.

\begin{equation}
\label{RR_vel}
R = ||\boldsymbol{v_n}-\boldsymbol{v_t}|| \sigma(||\boldsymbol{v_n}-\boldsymbol{v_t}||) M(v_t)
\end{equation}

Since elastic scattering is the only reaction of interest, $\sigma(v_\mathrm{rel})$ can be assumed to be a constant.  This assumption is good for most nuclides since the elastic cross sections are in fact relatively constant at low energies.  If there is a low energy scattering resonance, this approximation may produce results that skew the distribution towards lower energies \cite{openmc}, but it is good for most nuclides.  Substituting the Maxwell-Boltzmann speed distribution for $M(v_t)$, the simplified PDF for target velocities can be written as \eqref{RR_vel_simp} where $C$ is the normalization constant calculated from \eqref{RR_vel}.

\begin{equation}
\label{RR_vel_simp}
\begin{gathered}
\beta = \sqrt{\frac{m}{2kT}} \\
PDF(\boldsymbol{v_t}) = C v_\mathrm{rel} \sigma \left[   \frac{4}{\sqrt{\pi}} \beta^3 v_T^2  \exp (- \beta^2  v_T^2 )      \right]
\end{gathered}
\end{equation}

Applying the law of cosines to $v_\mathrm{rel} = ||\boldsymbol{v_n}-\boldsymbol{v_t}||$ yields \eqref{RR_vel_expanded}, which has changed from a vector equation to a scalar one.   The new PDF has two unknowns, the target velocity $v_T$ and the angle between it and the neutron velocity, $\mu$.

\begin{equation}
\label{RR_vel_expanded}
\begin{gathered}
\beta = \sqrt{\frac{m}{2kT}} \\
PDF(v_t) = C \sigma  \sqrt{v_n^2+v_t^2-2 v_n v_t \mu} \left[   \frac{4}{\sqrt{\pi}} \beta^3 v_T^2  \exp ( -\beta^2  v_T^2 )      \right]
\end{gathered}
\end{equation}

There is a restriction on $\mu$, however.  There can be no cases where sampled values of $\mu$ and $v_T$ produce a negative $v_\mathrm{rel}$, which corresponds to the target neutron ``running away'' from the incident neutron.  Such a case is non-physical since it would imply that the reaction never happened.  This PDF cannot be directly inverted, but can be split into two PDFs, $f_1(v_T)$ and $f_2(v_T)$, and sampled by rejection.  The sampling scheme is to normalize $f_2(v_T)$ and sample from it.  The sampled is accepted based on the probability in \eqref{RR_vel_split_samp}\cite{openmc}.

\begin{equation}
\label{RR_vel_split}
\begin{gathered}
f_1(v_T,\mu) = C \sigma  \frac{\sqrt{v_n^2+v_t^2-2 v_n v_t \mu}}{v_n^2+v_t^2}\\
f_2(v_T) =  \frac{4}{\sqrt{\pi}} (v_n^2+v_t^2) \beta^3 v_T^2  \exp ( -\beta^2  v_T^2 )
\end{gathered}
\end{equation}

\begin{equation}
\label{RR_vel_split_samp}
\begin{split}
``CDF(v_T)" &= \frac{f_2(v_T)}{\int f_2(v_T)} \qquad \Rightarrow \qquad v_T^\prime  \\
p_\mathrm{accept} &= \frac{f_1(v_T^\prime,\mu)}{ f_1(v_T^\prime,\mu=0)}
\end{split}
\end{equation}

The multiplication and division by $v_n^2+v_t^2$ seen in \eqref{RR_vel_split} guarantees that $f_1$ is bounded between 0 and 1.  This is necessary since $v_\mathrm{rel}$ is not bounded in any way.  The normalized version of $f_2$ is shown in \eqref{RR_vel_split_norm} as well as a simplified version where $y=\beta v_n$ and $x=\beta v_t$.

\begin{equation}
\label{RR_vel_split_norm}
\begin{split}
``CDF(v_T)" &=  \frac{f_2(v_T)}{\int f_2(v_T)} = \frac{  (v_n^2+v_t^2) \beta^3 v_T^2  \exp ( -\beta^2  v_T^2 ) }{ \frac{1}{4\beta} \sqrt{\pi} \beta v_n +2 } \\
&= \left( \frac{4}{\sqrt{\pi}}  \frac{\sqrt{\pi}y}{\sqrt{\pi}y+2} \right) x^2 \exp(-x^2)  + \left( 2  \frac{2}{\sqrt{\pi}y+2} \right) x^3 \exp(-x^2) \\
\end{split}
\end{equation}

Since this scheme has two separate weighted distributions, the combined PDF can be sampled by sampling the second term with a probability of $2/(\sqrt{\pi}y+2)$ and the first term otherwise.  Luckily, both distributions can be directly sampled via the schemes C49 and C61 listed in the Third Monte Carlo Sampler manual \cite{3rdsampler}.  Sampling scheme C49 is for expressions of the form $\nu^{2n-1}e^{-\nu^2}$ where $n$ is an integer $\ge 1$, i.e. for gaussians multiplied by variables of odd powers.  For $n=2$, this is scheme is for the second term in \eqref{RR_vel_split_norm}, and is shown in \eqref{C49}.

\begin{equation}
\label{C49}
\begin{gathered}
\nu = \sqrt{- \ln \left( \prod_{i=1}^n \xi_i \right)} \\ 
n = 2 \qquad \Rightarrow \qquad x^\prime = \sqrt{- \ln \left( \xi_1 \xi_2 \right)}
\end{gathered}
\end{equation}

Sampling scheme C61 is for expressions of the for $\nu^{2n-1}e^{-\nu^2}$ where $n$ is a half integer $\ge 1/2$, i.e. for gaussians multiplied by variables of even powers.  When $n=3/2$, this is a sampling scheme for the first term in \eqref{RR_vel_split_norm}, and is shown in \eqref{C61}.

\begin{equation}
\label{C61}
\begin{gathered}
h = n-1/2 \\
\nu = \sqrt{- \ln \left( \prod_{i=1}^h \xi_i  \right)+ \tau^2} \\ 
\tau^2 = -\ln(\xi_1) \cos^2(\frac{\pi}{2}\xi_2)\\
n = 3/2 \quad h=1 \qquad \Rightarrow \qquad x^\prime =   \sqrt{- \ln( \xi_1) - \ln(\xi_2) \cos^2(\frac{\pi}{2}\xi_3)}
\end{gathered}
\end{equation}

After this is done and $x^\prime$ is sampled, the rejection criterion in \eqref{RR_vel_split_samp} comes into play.  The sampled value of $x^\prime$ is transformed to $v_T^\prime=x^\prime/\beta$, and a value for $\mu$ is sampled isotropically as in \eqref{mu_samp}.  Once $\mu$ is sampled, $f_1(v_T,\mu)$ can be calculated, and the sampled values of $v_T^\prime$ and $\mu$ are accepted with the probability $p_\mathrm{accept}$ as shown in \eqref{prob_accept}, i.e. if another random number $\xi<p_\mathrm{accept}$.  If the sample is rejected, the whole process is repeated until a pair of $v_T^\prime$ and $\mu$ are accepted.  Even though this is a rejections method, it has fairly high efficiency, with samples being accepted at minimum 68\% of the time and 100\% of the time as the neutron velocity goes to zero or values much greater than the target temperature \cite{mcnp}.

\begin{equation}
\label{mu_samp}
\mu = 2\xi - 1 
\end{equation}

\begin{equation}
\label{prob_accept}
p_\mathrm{accept} = \frac{\sqrt{v_n^2+v_t^2-2 v_n v_t \mu}}{v_n^2+v_t^2}
\end{equation}

\subsubsection{Stochastic Mixing}

The ENDF data tables have probability distributions for reactions at discrete energy points, but this is not physically accurate.  The reactions do not abruptly transition from one PDF to another at a single energy, they smoothly transition.  This is why \emph{stochastic mixing} is prescribed in the ENDF tables.  If the neutron energy, $E$, falls between the ENDF data energy grid values $i$ and $i+1$, the neutron will use data from table $i+1$ with probability $f$, defined in \eqref{mixing_prob} \cite{openmc}.

\begin{equation}
\label{mixing_prob}
\begin{gathered}
E_i < E < E_{i+1} \\
f = \frac{E-E_i}{E_{i+1}-E_i}
\end{gathered}
\end{equation}

As $E$ approaches $E_{i+1}$, the probability it will sample from the $i+1$ distribution go to 1 linearly.  This ensures smooth transitions between reaction data tables.


\subsubsection{Elastic Scattering}

The kinematics of elastic scattering were outlined in Section \ref{subsec:interactions}, and even though it seems only the velocities and masses of the target and neutron are needed, data tables still need to be queried to carry out the interaction.  At energies below the MeV range, elastic scattering is isotropic in the CM frame, but at these energies and above, it becomes significantly anisotropic.  Figure \ref{scattering_anisotropy} shows this in oxygen-16.  At high energies, the scattering is both forward- and backward-peaked.  

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/scattering_anisotropy.eps}
     \caption{Elastic scattering anisotropy at high energies in oxygen-16.  \label{scattering_anisotropy}}
\end{figure}

It is essential to model this phenomena accurately since the scattering angle factors heavily into the energy exchange between the particles.  Figure \ref{scattering_error} shows what can happen when elastic scattering is treated as isotropic for all energies.  The figure shows the normalized flux per lethargy in a one meter cube of water with a 2 MeV point source at its center.  The neutron flux spectra immediately under scattering resonances are too large due to neutrons not losing enough energy in scattering events.  The large backward scattering probability causes more energy to be lost on average than purely isotropic scattering.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/scattering_error.eps}
     \caption{Error in the energy spectrum caused by treating elastic scattering as isotropic for all energies.\label{scattering_error}}
\end{figure}

After the target and neutron are transformed to the CM frame, the scattering CDF histogram interpolated and sampled as described in previous subsections.  Once the scattering angle is known, the rotated angle is calculated via Rodrigues' rotation formula shown in \eqref{RodriguesRot}.  The CM-rotated neutron velocity vector is then transformed back to the lab frame and the reaction is complete.

\subsubsection{Inelastic Reactions}

Inelastic level scattering is treated identically to elastic scattering except that the $Q$ value in \eqref{finalvCM} is now nonzero.  This value is reaction-specific and must be given by the data tables.  

Inelastic continuum scattering is treated differently, however.  Instead of having a well-defined kinematic formula relating the scattering angle to the final neutron energy, continuum reactions are defined by correlated scattering and energy tables.  They follow ENDF ``law'' number 44, the Kalbach-Mann correlated energy-angle scattering law \cite{mcnp} \cite{openmc}.  In this sampling scheme, a CDF is histogram or linearly-linearly interpolated and sampled from just like other reactions, except the sampled value is now a set of ``precompound factors,'' $R$, and ``angular slopes,'' $A$, instead of $\mu$ directly.  These values correspond to a secondary PDF, shown in \eqref{law44}, which is again sampled from to find $\mu$.

\begin{equation}
\label{law44}
PDF = \frac{A}{2 \sinh (A)} ( \cosh(A\mu)+R \sinh (A\mu))
\end{equation}

The sampling scheme for this probability distribution shown in \eqref{law44samp}, where $\xi_1$ and $\xi_2$ are two additional random numbers .  Since this is a correlated angle-energy distribution, there there is an energy value along with the factors $A$ and $R$ that corresponds to a CDF bin.  This sampled energy value is scaled via \eqref{energy_scaling} just like those from a tabular energy distribution, which is the topic of the next subsection \cite{3rdsampler}\cite{openmc}.

\begin{equation}
\label{law44samp}
\begin{split}
\mathrm{if} \: \xi_1 > R:  \quad T&=(2\xi_2-1)\sinh(A)\\
\mu &= \frac{\ln(T+\sqrt{T^2+1})}{A}\\
\mathrm{else: }  \qquad & \\
\mu &= \frac{ \ln(\xi_2 \exp(A) + (1-\xi_2)\exp(-A))}{A}
\end{split}
\end{equation}

\subsubsection{Tabular Energy Distributions}

Continuous, independent energy distributions, like those specified for fission spectra, have multiple CDFs specified at various incoming neutron energies.  Stochastic mixing probability $f$, shown in \eqref{mixing_prob}, is used to select if the lower-bounding or upper-bounding distribution will be sampled from for $E_i < E < E_{i+1}$.  After the distribution is selected, the CDF is sampled with histogram or linear-linear interpolation, which ever is specified in the library.  Once the sampled energy, $E_\mathrm{sampled}$, is calculated, it must be scaled to the bounding incoming energy bins to preserve any thresholds.  This is done via \eqref{energy_scaling} where $E_{i,\mathrm{first}}$ and $E_{i,\mathrm{last}}$ are the first and last energy values of the CDF in $E_i$; and $E_{i+1,\mathrm{first}}$ and $E_{i+1,\mathrm{last}}$ are the first and last energy values of the CDF in $E_{i+1}$ \cite{mcnp}.

\begin{equation}
\label{energy_scaling}
\begin{split}
E_a &= E_{i,\mathrm{first}} +  f( E_{i+1,\mathrm{first}} - E_{i,\mathrm{first}} ) \\
E_b &= E_{i,\mathrm{last}} +  f( E_{i+1,\mathrm{last}} - E_{i,\mathrm{last}} ) \\
\mathrm{if\:i+1\:sampled:} \quad \mathrm{diff} &= E_{i+1,\mathrm{last}}  - E_{i+1,\mathrm{first}}  \qquad E_\mathrm{start}= E_{i+1,\mathrm{first}}  \\
\mathrm{else:}          \qquad \quad       \mathrm{diff} &= E_{i,\mathrm{last}}  - E_{i,\mathrm{first}}  \qquad \qquad E_\mathrm{start}= E_{i,\mathrm{first}}  \\
E^\prime &=  E1  +  ( E_\mathrm{sampled} - E_\mathrm{start})  \frac{ E_b - E_a}{ \mathrm{diff} }  
\end{split}
\end{equation}


\subsection{Flux Estimation and Tallies}

Reactions that happen within a volume can be scored, or tallied.  This is just like what a detector would do - integrate the physical response across energy and space.  MCNP uses the word ``tally'' and Serpent uses the word ``detector'' for the same measurement.  Evidently, ``tally'' does not translate into Finnish, which is why ``detector'' is used.  ``Tally'' will be used here for consistency with the majority of neutron transport codes.  Other than determining the multiplication factor, one of the main purposes of simulation reactors is determining the neutron flux spectrum.  The reaction rates and therefore the power of the reactor are proportional to the flux, making it a very important quantity in determining how a rectors will behave.  Since reactions are explicitly simulated with the  Monte Carlo method, an estimator can be used to estimate the neutron flux.  It does not directly calculate the flux, but relies on the fact that the total reaction rate is proportional to the flux.  If every interaction point is counted within a volume, this number can be used to estimate the volume-averaged flux in that volume.  Since the reaction rate is directly proportional to the flux, as per \eqref{scalar_flux_RR}, the flux can be written in terms of the reaction rate and the material's total macroscopic cross section as shown in \eqref{flux_from_RR}.

\begin{equation}
\label{flux_from_RR}
\phi(\boldsymbol{\vec{r}},E) = \frac{ R_t(\boldsymbol{\vec{r}},E) }{  \Sigma_t(\boldsymbol{\vec{r}},E) }
\end{equation}

For a representation on a computer, a discrete set of energy bins $E_1 \rightarrow E_g$ must be specified in order to calculate the flux.  The flux in group $g$, such that $E_g < E < E_{g+1}$, and in volume $j$ is shown in \eqref{flux_discrete}.

\begin{equation}
\label{flux_discrete}
\bar{\phi}_{g,j} = \frac{1}{V_j} \int_{v_j} dV \int_{E_g}^{E_{g+1}} dE \: \phi(\boldsymbol{\vec{r}},E) = \frac{1}{V_j} \int_{v_j} dV \int_{E_g}^{E_{g+1}} dE \:\frac{ R_t(\boldsymbol{\vec{r}},E) }{  \Sigma_t(\boldsymbol{\vec{r}},E) } \\
\end{equation}

Since the reaction rate $R_{g,j}$ is simply the number of collisions $N_{g,j}$ within the energy interval and within the volume $V_j$, as shown in \eqref{RR_coll}, the expression for flux can be written as the sum of the inverse total macroscopic cross section of the material in volume $V_j$.

\begin{equation}
\label{RR_coll}
\begin{gathered}
R_{g,j} = \int_{v_j} dV \int_{E_g}^{E_{g+1}} dE \: R_t(\boldsymbol{\vec{r}},E) = \sum_{i=1}^{N_{g,j}} 1
\end{gathered}
\end{equation}

The expression shown in \eqref{flux_tally} is \emph{analog estimator} of the flux.  It is called analog in the sense that the reactions are counted directly and it is analogous to physical phenomnena.  Since every collision is related to the total flux, counting every collision gives an estimate of the flux.  If a reaction has a finite cross section in a material and the reaction does not occur during the transport simulation, a reaction rate estimate is impossible to make using an analog estimator. 

\begin{equation}
\label{flux_tally}
\begin{gathered}
E_g < E_i < E_{g+1} \\
\bar{\phi}_{g,j} =  \frac{1}{V_j} \sum_{i=1}^{N_j} \frac{1}{\Sigma_t(E_i)} =  \frac{1}{V_j} \int_{V_j} dV \int_{E_g}^{E_{g+1}} dE \: \phi(\boldsymbol{\vec{r}},E) 
\end{gathered}
\end{equation}

If a \emph{collision estimator} is used instead, this is not longer the case.  A collision estimator differs by using every interaction point, no matter what actual reaction occurs.  This total score is simply weighted by the interaction probability for a particular reaction as shown in \eqref{flux_average_xs_tally} \cite{jaakko}.  The fractional interaction probability, $P_{k,m}(E_i)$, is for reaction $k$ in isotope $m$ in the material in volume $j$ at the energy $E_i$.

\begin{equation}
\label{flux_average_xs}
\bar{\Sigma}_{k,m} =  \frac{1}{V_j \bar{\phi}_{g,j} } \int_{v_j} dV \int_{E_g}^{E_{g+1}} dE \: \phi(\boldsymbol{\vec{r}},E) \Sigma_{k,m}(\boldsymbol{\vec{r}},E)
\end{equation}

This is equivalent to summing the macroscopic cross section for a particular isotope's reaction instead of the material's total cross section.  This way, even rare events can be scored.  The result it no longer the flux, however, but rather the \emph{flux averaged cross section}.  

\begin{equation}
\label{flux_average_xs_tally}
\begin{gathered}
E_g < E_i < E_{g+1} \\
P_{k,m}(E_i)= \frac{\Sigma_{k,m}(E_i)}{\Sigma_t(E_i)} \\
\bar{\Sigma}_{g,j,k,m} =  \frac{1}{V_j} \sum_{i=1}^{N_j} \frac{P_{i,j}(E_i)}{\Sigma_t(E_i)} = \frac{1}{V_j} \sum_{i=1}^{N_j} \frac{1}{\Sigma_{k,m}(E_i)}
\end{gathered}
\end{equation}

The flux averaged cross section is useful in determining the physics of a system since it weights reactions by the flux present at a certain spatial region and/or energy bin.  This way, the reaction channels that neutrons undergo can be compared on a one-to-one basis in terms of their fraction of the total number of reactions instead of having to worry about the energy spectrum and the isotope's number density as well as the normal cross section.  Of course, all tallies are subject to the statistical laws mentioned in the previous statistics subsection.


\subsection{Neutron Sources}

So far, all that has been mentioned is how to handle the different reactions in a Monte Carlo neutron transport simulation.  This is important since reactions describe what happens to neutrons and where they go during their random walk, but how to handle \emph{where} they come from still needs to be defined.  The source terms on the right hand side of the neutron transport equation, other than the scattering source, have not been defined yet.  In this subsection, the other two source terms, the external source and the criticality source, are discussed.

\subsubsection{External Source}

An external, or fixed, source is simply a source which does not depend on the neutron population itself.  Physically, such a source could be from natural radioactive decay, an accelerator source, etc.  The only concern with an external source in a Monte Carlo simulation is in accurately measuring the response of the system, more specifically, the response of the system if it contains fissile material.  If a single neutron is ``shot'' into a multiplying material, all the secondary particles the primary neutron induces also need to be simulated in order to simulate the response of the system.  The number of secondary neutrons produced is fully defined by the multiplication factor since it gives the ratio of subsequent neutron generations.  An expression for the converge infinite series is shown in \eqref{sub_crit_mult}\cite{duderstadt}\cite{jaakko}.

\begin{equation}
\label{sub_crit_mult}
N_\mathrm{total} = N_0 + k N_0 + k^2 N_0 + \dots = N_0 \sum_{i=0}^\infty k^i = \frac{N_0}{1-k}
\end{equation}

This series only converges for $k<1$, which sets a restriction for fixed-source simulations.  If the multiplication factor is greater than one, it will require infinitely many secondary neutrons to be tracked, so this situation must be avoided.

\subsubsection{Fission Source, $k$-Eigenvalue Method}

When a fission source is specified, the simulation will run in a manner different to an external source problem.  The neutron source now depends on the neutron population itself, and a method must be used which directly ties the source to the population.  This is done by using any points where neutrons any secondary-producing reactions as source points for subsequent neutron histories.  When neutrons are born from these induced points, they of course must follow the emissions laws specified by the data.  For fission reactions, there is usually a tabulated energy spectrum, which is sampled and interpolated in the data-specified ways, and then scaled to the incoming energy bins via \eqref{energy_scaling}.  The angle is isotropic in the lab frame, and this is easily sampled via \eqref{iso_samp}.

\begin{equation}
\label{iso_samp}
\begin{split}
\phi &= 2 \pi \xi_1 \\
\mu= \cos \theta &= 2 \xi_2 -1
\end{split}
\end{equation}

In criticality simulations, the parameter of interest is usually the effective multiplication factor, $k_\mathrm{eff}$.  It is defined by \eqref{k_eff_batch}, where $N_{s,n}$ is the number of source neutrons in the current generation, and $N_{s,n+1}$ is the number of neutrons in the next generation, which equals $N_{f,n}$, the yielf of secondary particles from both fission and (n,2/3/4n)) \cite{jaakko}.  Calculating this quantity in a Monte Carlo simulation is straightforward, but generation information must be preserved.  Correct results cannot be obtained if yields from source particles in different generation are used to calculate the number of sources in the next generation.  This is why criticality source simulations are usually run in a batched mode where a preset number of source particles from a single generation are all transported until termination.  The secondary-producing reaction points are then used as the starting points for the next generation.  The generations are not interleaved, and correct values for the multiplication factor can be calculated.

\begin{equation}
\label{k_eff_batch}
k_{\mathrm{eff},n} = \frac{N_{s,n+1}}{N_{s,n}} = \frac{N_{f,n}}{N_{s,n}}
\end{equation}

Modeling the fission chain reaction is simple in Monte Carlo simulations when a system is exactly critical.  In batched criticality mode, the next generation of neutrons have starting points which are determined by the previous generation.  When the system is exactly critical, there is a one-to-one mapping of induced secondary neutrons to preset number of source particles to be transported in the next batch.  But when the system is sub- or super-critical, there is no longer a one-to-one mapping and starting points either have to be reused or discarded, respectively.  This is done by using the \emph{k-eigenvalue method}, which used by the Serpent code \cite{jaakko}.  After $k_\mathrm{eff}$ is calculated via \eqref{k_eff_batch}, it is used to renormalized the fission source of the next generation, i.e. the secondary yield values are divided by $k_\mathrm{eff}$.  This is an analog to dividing the fission source term in \eqref{time_ind_NTE} by $k_\mathrm{eff}$ to enforce the time derivative to be zero.  By dividing the yield values by $k_\mathrm{eff}$, the multiplication factor should be 1, and a one-to-one mapping is recovered.  

Since $k_\mathrm{eff}$ are continuous, calculating an integer value of secondary particles is done stochastically to preserve the aggregate mean.  The renormalized yield, $y_r$, is stochastically rounded based on a random number $\xi$ as shown in  \eqref{stoch_rounding}.  Doing this ensures that over a large batch size, the multiplication factor is renormalized as close to one as possible.  In the case where $y_r$ is slightly less than one, the last few source points from the previous generation are simply reused.  This should not introduce much bias in the results if the source distribution is converged.

\begin{equation}
\label{stoch_rounding}
\begin{split}
&y_r = \frac{\mathrm{yield}}{k_\mathrm{eff}} \\
\mathrm{if}\quad &y_r - \mathrm{floor}(y_r)<\xi \\
\mathrm{then:}\quad &y_r=\mathrm{ceil}(y_r) \\
\mathrm{else:}\quad &y_r=\mathrm{floor}(y_r)
\end{split}
\end{equation}

To start the simulation, a guess must be made for the initial points neutrons are born.  in some codes, one can specify a single initial point where all of the first generation neutrons are born.  As the simulation progresses, the source expands and converges to the true distribution.  The initial batches, or cycles, when the source distribution is far from the correct distribution are therefore discarded.  No quantities are accumulated and the multiplication factors values are not used in calculating the final value.  The discarded cycles only serve to converge the fission source so the accumulated quantities later in the simulation are not biased due to an incorrect source distribution.  In order to accelerated this process, WARP uses a similar method to Serpent to guess the initial source distribution.  The materials in the geometry are all flagged as fissile or non-fissile.  Then and uniform, random distribution of source neutrons are distributed across the geometry.  If the particles lies in a fissile material, it is recorded in a buffer.  This process is repeated until the required amount of source points are accumulated in the starting point buffer.  This method is called the \emph{flat source approximation}, and should require fewer cycles to be discard than the point source method since the initial distribution has spatial extent and fewer cycles will be needed to simply push neutrons into all geometrical regions \cite{jaakko}.

When all the cycles are complete, the individual estimates of $k_\mathrm{eff}$ are averaged to make a final estimate for the system.  This is typically called the \emph{generation estimate} of $k_\mathrm{eff}$ \cite{jaakko}.  As the simulation runs, a recursion relation can be made so the values for every cycle does not need to be stored individually.  Expressions for the generation estimate are shown in \eqref{k_eff_final}.  It may be of interest to keep values for every cycle in order to perform statistical checks to assure convergence, however.

\begin{equation}
\label{k_eff_final}
\begin{split}
\bar{k}_{\mathrm{eff},n} &= \frac{1}{n} \sum_{i=1}^{n} k_{\mathrm{eff},i} \\
\bar{k}_{\mathrm{eff},n} &= \frac{1}{n} \left[ k_{\mathrm{eff},n} + (n-1) \bar{k}_{\mathrm{eff},n-1}  \right]
\end{split}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Standard Monte Carlo Neutron Transport Codes}

The main purpose in developing WARP is to accelerate Monte Carlo nuclear reactor simulations by using the computational power of GPUs.  To know if WARP is successful in doing this, it will compared against Serpent and MCNP, two Monte Carlo neutron transport codes that are used in the nuclear engineering community.  Each code has different features and strengths and weaknesses in different areas.  Comparing against these two codes will certify the accuracy of WARP's results and determine if WARP's GPU implementations are effective in accelerating the calculations.

\subsection{MCNP}

developed when and for what

fortran

gold standard

lots of physics

ray traced

The Monte Carlo method is generally attributed to scientists working on the development of
nuclear weapons in Los Alamos during the 1940s. However, its roots go back much farther.
Perhaps the earliest documented use of random sampling to solve a mathematical problem was that
of Compte de Buffon in 1772. A century later people performed experiments in which they threw
a needle in a haphazard manner onto a board ruled with parallel straight lines and inferred the value
of ? from observations of the number of intersections between needle and lines.5,6 Laplace
suggested in 1786 that ? could be evaluated by random sampling.7 Lord Kelvin appears to have
used random sampling to aid in evaluating some time integrals of the kinetic energy that appear in
the kinetic theory of gasses8 and acknowledged his secretary for performing calculations for more
than 5000 collisions.9

According to Emilio Segr, Enrico Fermi's student and collaborator, Fermi invented a form of the
Monte Carlo method when he was studying the moderation of neutrons in Rome.9,10 Though Fermi
did not publish anything, he amazed his colleagues with his predictions of experimental results.
After indulging himself, he would reveal that his ?guesses? were really derived from the statistical
sampling techniques that he performed in his head when he couldn't fall asleep.
During World War II at Los Alamos, Fermi joined many other eminent scientists to develop the
first atomic bomb. It was here that Stan Ulam became impressed with electromechanical computers
used for implosion studies. Ulam realized that statistical sampling techniques were considered
impractical because they were long and tedious, but with the development of computers they could
become practical. Ulam discussed his ideas with others like John von Neumann and Nicholas
Metropolis. Statistical sampling techniques reminded everyone of games of chance, where
randomness would statistically become resolved in predictable probabilities. It was Nicholas
Metropolis who noted that Stan had an uncle who would borrow money from relatives because he
?just had to go to Monte Carlo? and thus named the mathematical method ?Monte Carlo.?10
Meanwhile, a team of wartime scientists headed by John Mauchly was working to develop the first
electronic computer at the University of Pennsylvania in Philadelphia. Mauchly realized that if
Geiger counters in physics laboratories could count, then they could also do arithmetic and solve
mathematical problems. When he saw a seemingly limitless array of women cranking out firing
tables with desk calculators at the Ballistic Research Laboratory at Aberdeen, he proposed10 that
an electronic computer be built to deal with these calculations. The result was ENIAC (Electronic
Numerical Integrator and Computer), the world?s first computer, built for Aberdeen at the
University of Pennsylvania. It had 18,000 double triode vacuum tubes in a system with 500,000
solder joints.

John von Neumann was a consultant to both Aberdeen and Los Alamos. When he heard about
ENIAC, he convinced the authorities at Aberdeen that he could provide a more exhaustive test of
the computer than mere firing-table computations. In 1945 John von Neumann, Stan Frankel, and
Nicholas Metropolis visited the Moore School of Electrical Engineering at the University of
Pennsylvania to explore using ENIAC for thermonuclear weapon calculations with Edward Teller
at Los Alamos.10 After the successful testing and dropping of the first atomic bombs a few months
later, work began in earnest to calculate a thermonuclear weapon. On March 11, 1947, John von
Neumann sent a letter to Robert Richtmyer, leader of the Theoretical Division at Los Alamos,
proposing use of the statistical method to solve neutron diffusion and multiplication problems in
fission devices.10 His letter was the first formulation of a Monte Carlo computation for an
electronic computing machine. In 1947, while in Los Alamos, Fermi invented a mechanical device
called FERMIAC11 to trace neutron movements through fissionable materials by the Monte Carlo
Method.

By 1948 Stan Ulam was able to report to the Atomic Energy Commission that not only was the
Monte Carlo method being successfully used on problems pertaining to thermonuclear as well as
fission devices, but also it was being applied to cosmic ray showers and the study of partial
differential equations.10 In the late 1940s and early 1950s, there was a surge of papers describing
the Monte Carlo method and how it could solve problems in radiation or particle transport and
other areas.12,13,14 Many of the methods described in these papers are still used in Monte Carlo
today, including the method of generating random numbers15 used in MCNP. Much of the interest
was based on continued development of computers such as the Los Alamos MANIAC (Mechanical
Analyzer, Numerical Integrator, and Computer) in March, 1952.

The Atomic Energy Act of 1946 created the Atomic Energy Commission to succeed the Manhattan
Project. In 1953 the United States embarked upon the ?Atoms for Peace? program with the intent
of developing nuclear energy for peaceful applications such as nuclear power generation.
Meanwhile, computers were advancing rapidly. These factors led to greater interest in the Monte
Carlo method. In 1954 the first comprehensive review of the Monte Carlo method was published
by Herman Kahn16 and the first book was published by Cashwell and Everett17 in 1959.
At Los Alamos, Monte Carlo computer codes developed along with computers. The first Monte
Carlo code was the simple 19-step computing sheet in John von Neumann's letter to Richtmyer.
But as computers became more sophisticated, so did the codes. At first the codes were written in
machine language and each code would solve a specific problem. In the early 1960s, better
computers and the standardization of programming languages such as Fortran made possible more
general codes. The first Los Alamos general-purpose particle transport Monte Carlo code was
MCS,18 written in 1963. Scientists who were not necessarily experts in computers and Monte Carlo
mathematical techniques now could take advantage of the Monte Carlo method for radiation
transport. They could run the MCS code to solve modest problems without having to do either the
programming or the mathematical analysis themselves. MCS was followed by MCN19 in 1965.
MCNcould solve the problem of neutrons interacting with matter in a three?dimensional geometry
and used physics data stored in separate, highly?developed libraries.

In 1973 MCN was merged with MCG,20 a Monte Carlo gamma code that treated higher energy
photons, to form MCNG, a coupled neutron?gamma code. In 1977 MCNG was merged with
MCP,20 a Monte Carlo Photon code with detailed physics treatment down to 1 keV, to accurately
model neutron-photon interactions. The code has been known as MCNP ever since. Though at first
MCNP stood for Monte Carlo Neutron Photon, now it stands for Monte Carlo N?Particle. Other
major advances in the 70s included the present generalized tally structure, automatic calculation of
volumes, and a Monte Carlo eigenvalue algorithm to determine keff for nuclear criticality
(KCODE).

In 1983 MCNP3 was released, entirely rewritten in ANSI standard Fortran 77. MCNP3 was the
first MCNP version internationally distributed through the Radiation Shielding and Information
Center at Oak Ridge, Tennessee. Other 1980s versions of MCNP were MCNP3A (1986) and

MCNP3B (1988), that included tally plotting graphics (MCPLOT), the present generalized source,
surface sources, repeated structures/lattice geometries, and multigroup/adjoint transport.

MCNP4 was released in 1990 and was the first UNIX version of the code. It accommodated
N?particle transport and multitasking on parallel computer architectures. MCNP4 added electron
transport (patterned after the Integrated TIGER Series (ITS) electron physics),21 the pulse height
tally (F8), a thick?target bremsstrahlung approximation for photon transport, enabled detectors and
DXTRAN with the S(?,?) thermal treatment, provided greater random number control, and
allowed plotting of tally results while the code was running.

MCNP4A, released in 1993, featured enhanced statistical analysis, distributed processor
multitasking for running in parallel on a cluster of scientific workstations, new photon libraries, ENDF?6 capabilities, color X?Windows graphics, dynamic memory allocation, expanded
criticality output, periodic boundaries, plotting of particle tracks via SABRINA, improved tallies
in repeated structures, and many smaller improvements.

MCNP4B, released in 1997, featured differential operator perturbations, enhanced photon physics
equivalent to ITS3.0, PVM load balance and fault tolerance, cross-section plotting, postscript file
plotting, 64?bit workstation upgrades, PC X?windows, inclusion of LAHET HMCNP, lattice
universe mapping, enhanced neutron lifetimes, coincident?surface lattice capability, and many
smaller features and improvements.

MCNP4C, released in 2000, featured an unresolved resonance treatment, macrobodies,
superimposed importance mesh, perturbation enhancements, electron physics enhancements,
plotter upgrades, cumulative tallies, parallel enhancements and other small features and
improvements.

MCNP5, released in 2003, is rewritten in ANSI standard Fortran 90. It includes the addition of
photonuclear collision physics, superimposed mesh tallies, time splitting, and plotter upgrades.
MCNP5 also includes parallel computing enhancements with the addition of support for OpenMP
and MPI.

Large production codes such as MCNP have revolutionized science ?? not only in the way it is
done, but also by becoming the repositories for physics knowledge. MCNP represents over 500
person-years of sustained effort. The knowledge and expertise contained in MCNP is formidable.
Current MCNP development is characterized by a strong emphasis on quality control,
documentation, and research. New features continue to be added to the code to reflect new
advances in computer architecture, improvements in Monte Carlo methodology, and better physics
models. MCNP has a proud history and a promising future.

\cite{mcnp}


\subsection{Serpent}

jaakko, vtt, c

only for reactor analysis

woodcock tracking

unionized grid

\cite{jaakko}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{GPUs}

Now that the mathematical theory and simulation methods have been framed, the hardware will be discussed.  As mentioned in the introduction, GPUs are an emerging technology in supercomputing.  Their name, graphics processing units, tells their history.  They started as very specific use coprocessor cards on computers in the early 1990s.  These cards did one job:  process graphics to be displayed on a monitor, a work-intensive job that could be offloaded from the main CPU to the graphics card, freeing up CPU resources and improving the overall performance of the computer.  Most graphics computations require linear algebra operations on large datasets (projections, transforms, shading, etc.), so they were tailored to do these jobs very well and were not able to do much else.  They were not programmable, APIs had to be used to send data to them and operations were performed in standard way.  This was not a problem since the GPU/CPU system was balanced.  The CPU did the complicated jobs and the GPU took care of the large, but simple ones.  Since the turn of the century, CPU speeds have plateaued due to power density.  Power dissipation in a processor goes as $P=fCV^2$, where $f$ is frequency, $C$ is capacitance, and $V$ is voltage.  There is a minimum voltage needed to avoid thermally-induced errors, and the capacitance is related to the process size of the chip.  There is a minimum to both values, and a maximum for power dissipation of the processor (or power density since this is about a single unit).  This relation sets a maximum frequency of the processor (and also why overclockers must use exotic cooling methods to get their frequencies high).  This ceiling was been reached, but CPU manufacturers continued to increase performance by including multiple processor cores on their chips.  Moore's Law is basically in effect, and the number of transistors on the chip are doubling every 18 or so months.  These transistors are in the form of additional cores or in parallel resources, and chips are becoming wider, not faster.  This spreads the heat created in the chips out, and it is possible to dissipate it, i.e. the overall power dissipation increases, but the power density does not.  GPUs had been operating this way for many years, and the similarities between CPU tasks and GPU tasks became blurred.  This lead GPU manufacturers to ask the question, why not make the GPUs programmable?  Supercomputing had especially become massively parallel, and making supercomputers was a lucrative business.  They had been making high performance parallel processors for many years, and it was time to break into the supercomputing realm.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.6\textwidth]{graphics/computational_cap.pdf}
     \caption{The maximum theoretical GigaFLOPs of various NVIDIA GPUs vs. flagship Intel processors \cite{cuda}. \label{computational_cap}}
\end{figure}

To break into the supercomputing market, they crated the first programmable, general purpose GPU which focused on power efficiency and parallelism.  As figure \ref{computational_cap} shows, the maximum theoretical computational capacity of NVIDA GPUs has been increasing faster than that of CPUs.  The single-precision performance is much better than the double-precision, however.  The traditional role of GPUs as graphics accelerators did not require double precision capabilities, which is why double precision performance is emphasized.  Double precision arithmetic is supported on devices with compute capability of 1.3 and above, but gaming cards in the GeForce series have fewer double precision units than the high-end Tesla cards.  The Tesla cards have substantially more double precision units than the GeForce cards, which is why GeForce double precision is not shown \cite{fermi}.  As Figure \ref{computational_cap} also shows, CPUs have almost equivalent single and double precision performance. 

But the GPU executes differently than a CPU and is a separate piece of hardware.  There needed to be an interface between the GPU and CPU and an easy way to program them, so CUDA was created.  The next subsections will talk about the details of GPU execution and how CUDA allows them to be programmed. 

\subsection{Architecture}

A NVIDIA GPU's basic processing unit is called a multiprocessor.  These multiprocessors house many individual computational cores and can process jobs independently.  Figure \ref{fermi_SM} shows the architecture of a Fermi family NVIDIA GPU.  

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.3\textwidth]{graphics/fermi_SM.eps}
     \caption{The architecture of a Fermi multiprocessor \cite{cuda}. \label{fermi_SM}}
\end{figure}

transistor space

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/CUDA_transistors.eps}
     \caption{Relative transistor space use in GPUs and CPUs \cite{cuda}. \label{cuda_transistors}}
\end{figure}

Part of the reason GPUs are able to perform efficiently is do to their reliance on single instruction, multiple data (SIMD) execution.  This execution method uses the same instructions simultaneously carried out over multiple pieces of data.  This reduces the amount of power used in control and therefore more math can be done per watt, which is the main why they are being used in supercomputers \cite{exascale}.  The GPU programming model abstracts SIMD execution by using threads, which can be thought of in the traditional sense. There are some tradeoffs for this power efficiency, however, such as requiring relatively simple tasks due to limited cache and control space and requiring data parallelism for full utilization.  Single-instruction multiple-thread (SIMT) and SIMD are similar in the sense that identical instructions are carried out across different pieces of data, but SIMT allows threads to act independently, albeit with a performance penalty.   When a thread in a multiprocessor executes different instruction than the other resident threads, the multiprocessor masks it from execution, executes the identically-executing threads, then masks the identically-executing threads and executes the \emph{divergent} thread.  This effectively serializes operations if they require different instructions.  This masking and serializing makes many empty spaces in the SIMD lanes of the multiprocessor, and thus can incur severe performance penalties.  The magnitude of the penalty depends on the rate at which data is being delivered to the multiprocessor, of course, since work can only be done if data is present.  If the multiprocessor can completes all jobs, even if they need some serialization, before the next set of data arrives, then the penalty will be invisible \cite{cuda}.

\subsection{CUDA}

CUDA was first released in 2006 as NVIDIA's proprietary GPU programming platform.  It makes minimal additions to C/C++, and any C programming would be very comfortable programming in it.  It was chosen over OpenCL, the open source GPU programming platform, due to CUDA's greater feature support, stability, easy of programming, wider community usage, and ability to use new, cutting-edge features in NVIDIA GPUs that OpenCL is not.

The overarching theme for CUDA is that a host CPU thread directs the GPUs operation.  Data must be transferred from the host to the GPU's global memory over the PCIe bus, \emph{kernels} are launched to performs computations based o the transferred data, then control returns to the host thread.  ``Kernels'' are CUDA's parallel programs that each thread carried out over the data.  That being said, they must be independent tasks in the sense that their execution order does not matter.  Blocks of threads have the same interleaving requirement.  Blocks of threads are executed simultaneously on a multiprocessor.  The group of all blocks is a called a ``grid.''  The grid is analogous to the entire GPU device, blocks are analogous to the multiprocessors, and threads are analogous to the individual cores. Figure \ref{cuda_grid_launch} shows the host-device execution and the organization of threads into blocks and blocks into the grid.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.4\textwidth,trim= 0cm 2.5cm 0cm 0cm]{graphics/CUDA_grid_launch.eps}
     \caption{ Host-device execution in CUDA and the organization of threads into blocks and blocks into the grid \cite{cuda_ptx_isa}. \label{cuda_grid_launch}}
\end{figure}

Grids and block dimensions can be 1D, 2D or 3D arrays, which simply influences how the data is indexed.  Every thread has unique variables, ``threadIdx'' and ``blockIdx,'' that are automatically generated upon kernel launch.  The grid dimensions (the number of blocks in each grid dimension) and the block dimensions (the number of threads in each block dimension) are also broadcast to every thread.  Based on these quantities, a unique thread identification number can be computer and used to access the data.   For example, if the grid is 2x2 and blocks are 4x4, a unique thread coordinate can be computed by doing id$_y$ = blockIdx.y * blockDim.y + threadIdx.y and id$_x$ = blockIdx.x * blockDim.x + threadIdx.x.  If the grid is 1D as well as the blocks, indexing is much simpler, e.g. id = blockIdx.x*blockDim.x+threadIdx.x.

When blocks are executed in a multiprocessor, they are scheduled in smaller units that are 32 threads wide called \emph{warps}.  Warps are what abstract SIMD execution, and everything thread in a warp must execute the same instructions or they split are serialized.

\subsection{Memory}

High bandwidth is necessary to feed the many hungry mouths of the arithmetic units and to maintain high computational throughput.  Most optimization work done of GPUs involves relieving memory bottlenecks.  The bandwidths of recent Intel CPUs and NVIDIA GPUs are shown in Figure \ref{bandwidth}, and it can be seen that GPU's bandwidth far exceeds the CPU's.  It is important to remember that this is maximum \emph{aggregate} bandwidth, however, not the bandwidth available to each individual multiprocessor.   

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.6\textwidth]{graphics/memory_bandwidth.pdf}
     \caption{The memory spaces in CUDA and NVIDIA GPUs \cite{cuda}. \label{bandwidth}}
\end{figure}

In order to maximize bandwidth and take advantage of spatial locality in memory, the memory subsystem on a GPU is also SIMD-like, as are most modern CPU subsystems.  When a thread requests a piece of data, not just the piece that is request is delivered by the subsystem, but rather a chunk of aligned data that contains the requested piece.  If the other values in load are not used, memory bandwidth is lost.  For best performance, CUDA requires adjacent threads to access adjacent pieces of data.  This way, memory transactions are ``coalesced.''   In coalesced transactions, every piece of data in the memory payload is used and bandwidth is maximized \cite{cuda}.  Figure \ref{coalesced} shows is graphically, and helps convey and important subtlety.  If a thread needs to load an entire array, it is better to interleave values in memory so that adjacent memory is accessed at the \emph{same time} by neighboring threads rather than adjacent data is loaded sequentially by a single thread.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/coalesced.eps}
     \caption{Memory transactions of CUDA threads.  Coalesced access on the left and non-coalesced access on the right \cite{programming_massively}. \label{coalesced}}
\end{figure}

GPUs have even higher global memory latency than CPUs, from 200 on newer cards up to 800 clock cycles on older cards, compared to about 50 cycles on a typical CPU.  CPUs, which are for serial execution, want low latency memory access and do this through multilevel cache structures and large control spaces that allow them to do out of order and speculative execution.  GPUs, which are geared for throughput, want high bandwidth and hide latency by pipelining many threads.  More transistor space is allocated for computation rather than for trying to minimize memory latency, which is hidden though parallelism.  Figure \ref{pipeline} shows how GPU threads are pipelined to hide latency whereas CPUs rely on low latency to quickly execute different threads in series.  This is GPUs performance best when as many threads as possible are launched.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=\textwidth]{graphics/pipeline.eps}
     \caption{Pipelining many threads in order to hide high memory latency \cite{cuda_gtc_pres}. \label{pipeline}}
\end{figure}

Even though there are fewer levels of cache in a GPU than a CPU, there are still many different spaces, each of which have different properties.  The global memory space is the largest and has the greatest latency (~200 memory clock cycles).  This is the amount of total RAM specified for each card, which can be as small as 512MB for very low-end cards, up to 12GB for the largest high-end card.  This memory space is accessible by all threads in all blocks.  The next level down is the shared memory space.  This acts as a user-programmable cache, and is not used unless explicitly coded.  It resides on the multiprocessor and has very low latency (on the order of 1 processor clock cycle), and the data can be seen by all threads within a block, hence the ``shared'' name.  It can allow intra-block communication  without incurring the high penalty of global memory transactions and can improve performance by acting as a cache and storing any data that is reused by threads.  The next level down is the local memory, which is thread-specific.  It is not really local in the sense that it is actually stored in global memory, but in Fermi and newer architectures, it is cached by a  L1 cache on the multiprocessor.  It serves to hold any data that the registers cannot hold.  The registers, the lowest level of memory, are what the arithmetic units directly load their data from.  They are fast, but have a limited size, which is why local memory is used to page them.  A Fermi GPU  has a large register file (2MB), but each multiprocessor only has 128k, which is further subdivided by each thread (maximum register size per thread is 63).

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.4\textwidth]{graphics/CUDA_memory.eps}
     \caption{The memory spaces in CUDA and NVIDIA GPUs \cite{cuda}. \label{cuda_mem}}
\end{figure}

The other memory spaces are the constant memory and the texture memory, which are both visible to all threads in all blocks.  They aren't truly separate spaces, but rather global memory that is handled differently.  Storing data in the constant space means that it cannot be written to, hence the ``constant'' name, but it is cached so data reuse by threads can result in a cache hit and the global memory itself does not need to be queried \cite{cuda}.  it is limited to 64kB, however, and is cached by 8kB caches on each multiprocessor, so large datasets cannot be stored in constant memory.  It also performs best when values are broadcast to all threads.  Texture memory memory, on the other hand, is also cached but its cache is optimized for 2D spatial locality in the texture coordinate system instead of actual memory locality, and it also does not have a maximum size.  Another feature of the texture memory is that is can perform low precision linear interpolation in the same transaction as a read.

\section{OptiX}

In order the save time and effort, NVIDIA's OptiX ray tracing framework is used by WARP for the geometry representation, surface detection, and material queries on the GPU.  It is a framework, not a ray tracing library itself.  The programmer must write all intersection programs for the geometry primitives as well as the hit programs.  OptiX simply provides the mathematical and computer science glue between the user-programmed ray tracing elements.  Besides its flexibility, OptiX provides optimizations that would require many months for a single developer to replicate in handwritten code.  It's main optimization is automatically producing high-quality acceleration structures for traversing the primitives in the geometry.  In this section, the general properties and workings of OptiX will be presented.  Details about its implementation in WARP will be discussed in Chapter \ref{chap:imp}.

\subsection{OptiX Programs}

As stated before, the developer must provide all the programs for OptiX.  These various programs are compiled to .ptx files, the paths to which are given the the OptiX API (application programming interface) at compile time and are loaded by the executable at run time \cite{optix}.  They are compiled to ptx since it is a ``virtual assembly language'' and can be easily interpreted by GPU and required no further compilation.  Figure \ref{optix_flow} shows the control flow in OptiX.  The launch is just like a CUDA kernel launch.  The API checks to make sure all the necessary data is present on the card, then launches kernels internally to carry out the trace on the GPU.

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.5\textwidth]{graphics/optix_flow.eps}
     \caption{The memory spaces in CUDA and NVIDIA GPUs \cite{optix_paper}. \label{optix_flow}}
\end{figure}

The ``ray generation'' program is like a main function in the ray tracer. It initializes rays, their starting points and directions, as well as specifies the data that they report.  Once this is done, the trace is executed, which traverses the geometry and find intersection points.  If \emph{any} geometry primitive is intersected, the ``any hit'' program is executed.  This program is typically used for determining areas that are completely occluded from the light source in graphics rendering problems, therefore terminating the trace, but normally this program just returns and allows the trace to continue.  As OptiX queries the primitives along a ray, it tightens an interval on $t$, the path length of the parameterized ray.  The equation for a parameterized ray is shown in \eqref{parameterized_ray}, where $\vec{r}_P$ is the end point of the ray, $\vec{r}_O$ is the origin point of the ray, and $\vec{r}_D$ is the directional unit vector of the ray.  Figure \ref{ray_surface} shows an illustration of a ray-surface intersection.

\begin{equation}
\label{parameterized_ray}
\begin{split}
\vec{r}_P = \vec{r}_O + t \vec{r}_D 
\end{split}
\end{equation}

\begin{figure}[h!] 
  \centering
    \includegraphics[width=0.5\textwidth]{graphics/ray_surface.eps}
     \caption{Ray-surface intersection:  A sphere and a parameterized ray. \label{ray_surface}}
\end{figure}

When the $t$ interval becomes tight enough to determine the closest hit, the ``closest hit'' program is executed.  This program typically returns the closest intersection point to the ray generation program, which then uses this information to shade a pixel in an image, or in WARP's case, determines if a neutron travels past a surface.  Miss programs tell a ray what to do if it does not intersect any primitives.  In rendering, this usually maps to a scene background image.  Since a material must always be defined in WARP, a miss simply throws an error.

\subsection{Geometry and Intersection Programs}

Geometrical primitives in OptiX are represented by their intersection programs.  Intersection programs provide two purposes - to calculate the exact intersection points between a ray and a primitive and to provide axis-aligned bounding boxes around the primitives.  These bounding boxes must completely contain the underlying primitive, but be as small as possible for best performance.  They are queried when the acceleration structure is built over the geometry, which is discussed in the next subsection.  

\begin{equation}
\label{ray_sphere}
\begin{split}
a &= \vec{r}_D \cdot \vec{r}_D = 1\\
b &= 2 \vec{r}_O \cdot \vec{r}_D = 2b_2\\
c &= \vec{r}_O \cdot \vec{r}_O - r^2 \\
t &= \frac{-b \pm \sqrt{b^2-4ac}}{2a} = -b_2 \pm \sqrt{b_2^2-c}
\end{split}
\end{equation}

Their main purpose is to calculate the intersection points, which must be reported back in terms of the $t$ value.  For example, the equation for a sphere is $r^2=x^2+y^2+z^2$.  Substituting \eqref{parameterized_ray} into this equation and solving for $t$ yields \eqref{ray_sphere}.  Since a sphere is a closed surface, there are two intersection points.  The smallest nonnegative value is reported back by the intersection program.

 \begin{figure}[h!] 
  \centering
    \includegraphics[width=0.8\textwidth]{graphics/node_graph.eps}
     \caption{Node graph used for geometry representation in OptiX \cite{optix}. \label{node_graph}}
\end{figure}

The framework by which geometric primitives are arranged into a scene is very flexible in OptiX.  Figure \ref{node_graph} shows the node graph representation of the geometry that OptiX traverses while tracing.   A ``group'' can have children of any kind, whereas a ``geometry group'' can only have ``geometry instance'' children.  A ``geometry instance'' is a object that combines a geometry primitive and a ``material.''  The material is simply the closest and any hit programs for a ray type.  OptiX can handle different ray types, typically one for radiance and one for shadow, but WARP only needs radiance.  This way a geometry can be attached to multiple different materials to handle different ray types.  A root node, or ``entry point,'' must be specified before a trace can begin.  This root node is where all rays begin their graph traversal \cite{optix}.

There are two special node types that can have geometry group children - a selector node and a transform node.  Both do exactly what they appear to.  A selector node will decide which geometry group to send a ray down based on some type of criterion.  This criterion can be based on a  global variable that is assigned before a trace or a piece of data that is determined during the trace itself.  It gives the framework more flexibility for the types of problems it can handle and the ease at which solutions can be implemented.  A transform node transforms the underlying geometry based on a affine transformation matrix.  This node type can be useful for specifying only one geometry instance node which is then instanced throughout the scene with different transform nodes.  In can also be used to move geometry based on time.  Therefore, it can be used to make movies in rendering, but for WARP's purposes it could potentially be used for calculating the effects of geometric perturbations or even introducing time-dependent simulations.  When transform nodes are specified, they must be attached to a geometry group and be given an affine transformation matrix.  In three dimensions, it is a 4x4 matrix which can specify rotation, scale, translation, and shear.  A matrix for simultaneous translation rotation is shown in \eqref{affine} where $dx$, $dy$, and $dz$ are translations in $x$, $y$, and $z$, respectively; and $\theta$ is a rotation around the $z$ axis, i.e. a rotation in the $x$-$y$ plane \cite{affine}.  Of course, other matrices can be specified.

\begin{equation}
\label{affine}
\vec{x} = \left[ \begin{array}{c}
x \\
y\\
z\\
1 \\ \end{array} 
 \right] \qquad
M = \left[ \begin{array}{cccc}
\cos \theta & -\sin \theta & 0 & dx \\
\sin \theta & \cos \theta & 0 &  dy\\
0 & 0 & 1 & dz\\
0 & 0 & 0 & 0\\
\end{array} \right]  \qquad
M \vec{x} = \vec{x}^\prime
\end{equation}

The final, and arguably most important, part of the node graph is the acceleration object.  This node specifies the acceleration data structure that is attached to each group or geometry group and \emph{must} be present for rays to traverse these groups.  Groups and geometry groups can shared acceleration objects, but if any underlying geometry is changed, the structure must be rebuild for both groups sharing the acceleration object, not only the portions that changed.  Simpler acceleration object layouts typically have better performance, however, since the acceleration objects higher up in the graph can only treat groups further down in terms of their coarsest outer limits \cite{optix}.

\subsection{Acceleration Structures}

The basic idea behind acceleration structures is that they provide a way to decompose the scene geometry into a hierarchical graph.  Once this is done, parts of the scene which rays aren't close to intersecting can be pruned from the set of actual geometry primitives a ray must query to find intersection points \cite{optix}.  OptiX provides different types of acceleration structures, namely, bounding volume hierarchy (BVH) trees and k-dimensional (k-d) trees.

BVH is object-centric in the sense that it places boundaries around groups of objects.  Each leaf of the tree can contain one or more objects. K-d trees are space-centric in the sense that they partition the space objects lie in.  They are binary trees which partitions each volume into two smaller volumes until the lowest leaves only contain a portion of a single object.  Figure \ref{bvh_kd} shows how BVH and k-d trees partition the objects and the space, respectively.  BVH trees have the benefit of being shallower, and therefore fast to traverse, but some objects' higher bounding volumes can overlap, leading to situation where both subtrees must be queried.

\begin{figure}[h!] 
\centering
\includegraphics[width=0.8\textwidth, trim= 8cm 0cm 0cm 0cm ]{graphics/bvh_kd.eps}
\caption{Partitioning schemes (top) with the resulting trees (bottom). A BVH tree \cite{wikimedia_bvh} is on the left and a k-d tree is on the right \cite{wikimedia_kd}.   \label{bvh_kd}}
\end{figure}

The benefit of these structures is that they can be queried in $\log(N)$ time rather than in linear time since major portions of the geometry are \cite{something}.  OptiX automatically builds theses trees over the geometry without any user input other than to specify the type.  The different types have tradeoffs with size, build speed, and performance, but since WARP's geometry is static and relatively simple compared to photorealistic rendering jobs, the build speed and size have negligible impact, so the highest performance structure is used is WARP.  Which structure prevodes the best performance is the goal of one of the preliminary studies in Section \ref{sec:prelim}.  

The typical geometry representation in Monte Carlo neutron transport codes is combinatorial geometry, which uses union and intersection operations on the surface sense  of a point in relation to second-order polynomials.  This kind of algorithm is linear with the number of \emph{surfaces} present in the geometry, so using OptiX in complicated geometries should provide provide a performance boost to geometry routines, especially compared to MCNP, which uses ray tracing as well as combinatorial geometry.  Serpent only uses Woodcock tracking for neutrons, and surface intersections are never calculated.  It also uses ``universes'' to provide some level of acceleration structure to primitive replication.  In using them, a neutron is determined to be in a larger cell occupied by another, nonzero universe.  On this is determined, the neutron is transformed into the higher level universe and the geometry there is queried.  This operation is done recursively until the actual material the neutron is traveling in is determined \cite{jaakko}[MUST CHECK THIS FOR TRUTH].  In effect, using ``universes'' in this way is like traversing a k-d tree, and it is unclear how Serpent will compare against WARP in this regard a priori and will be benchmarked in the results section (Chapter \ref{chap:results}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Previous Works}

The impetus for developing WARP was the research done by Martin and Brown in 1984 and by Vuji\'{c} and Martin in 1991.  In the 1984 paper, a method for mapping the Monte Carlo problem onto SIMD vector computers is described.  The essential idea is to bank the neutrons into vectors based on their required operation.  If a neutron is scattering, it's data is placed in the scattering buffer.  If a neutron needs to do a surface crossing calculation, it is put in the crossing buffer, and so on.  Once a buffer becomes full, it is processed in a SIMD fashion by the vector computer.  This was a very different way of performing a Monte Carlo simulation at the time.  Almost all computers were strictly serial, and SIMD lanes were only available in supercomputers.  Therefore the pervasive method was the task-based method, where neutrons are tracked for their entire lifetime in series.  this new approach was named ``event-based'' Monte Carlo, since the neutron events are tracked and processed as a group \cite{vector}.  Since GPUs are massively parallel and rely on SIMD, an event-based algorithm seems to be the appropriate approach to GPU-accelerated neutron transport.  In the 1991 paper, a vectorized approach is applied to the collision probability method (CPM), which is a deterministic way to solve the integral form of the NTE. \cite{vujic_vector}

Vectorizing the Monte Carlo algorithm should allow for GPU execution, but a paper by Zhang also shows that be remapping data references, the thread divergence in GPU warps can be minimized.  This paper provides a simple way to ``vectorize'' the GPU data without actually moving data itself \cite{on_the_fly_remapping}.  Of course this method may lead to sparse data access, but this will happen in a Monte Carlo algorithm regardless, so it may be of use to reduce thread divergence and increase the number of active warps per active cycle on the GPU.  The remapping method is one of the preliminary studies done in the next subsection.

Although there has been much research done on Monte Carlo neutron transport, there is relatively very little about performing it on GPUs due to GPGPUs being a new technology.  There have been two significant developments, however.  

One is by Adam Gregory Nelson, who published work on a GPU accelerated Monte Carlo neutron transport code he wrote for his Master's research at the Pennsylvania State University.  In his thesis, he report on developing a task-based Monte Carlo Neutron transport code, LADON, and an event-based Monte Carlo Neutron transport code, CERBERUS.  Both codes have CPU versions that have the extension ``c'' and GPU versions which have the extension ``g.''  Comparing both the codes, Nelson was able to attain a 24x speedup between LADONc and LADONg, and a 1.13x speedup between CERBERUSc and CERBERUSg \cite{nelson}.  These results are very encouraging and show that significant speedups can be realized by performing Monte Carlo neutron transport on the GPU, but discouraging since the event-based speedups were so poor.  Nelson also states that the poor performance of CERBERUS was due to the sorting of data was done on the CPU and certain sections of his algorithm serialized execution completely.  Due to this, he abandoned it and most of the thesis is about LADON.  

There are also a handful of assumptions in his implementation that require him to make one-to-one comparisons to an identical CPU version.  LANDON and CERBERUS use point-wise cross sections, but they are not read from ACE formatted data files, they do not perform any discrete inelastic scattering,  $\nu$ is fixed at 2.53.  Comparing them against production codes like MCNP and Serpent could not be done, since the results may not be close enough to know the physics is accurate.  The codes seem to also be quite inflexible.  They can only model spheres and cuboids, cannot transform these objects, cells cannot intersect, cell nesting must be explicitly defined and input in-order, and there is a total limit of 100 cells.

The GPU implementation may not be ideal either, since Nelson uses an array of structures data (AOS) layout instead of a structure of arrays layout (SOA), which is not ideal for coalesced loads, as was shown in Figure \ref{coalesced}, as he states himself.  CERBERUS also seems to not employ parallel algorithms other than for transport.  None of the underlying operations necessary for implementing event-based Monte Carlo were parallelized effectively.  The cross section data was also simply left as vectors in device memory and was not reformatted in any way to optimize the access pattern on the GPU.

The second major study done was that by Liu, et al.  In his first publication, a GPU is used to perform an multiplication factor calculation.  Very simple geometry is used, a bare slab and a bare sphere, and on group cross sections are used.  The fission source operations and sorting are also done on the CPU rather than the GPU.  Despite using the CPU to do these tasks, Liu reports a speedup over an identical CPU version of 7x for the bare sphere and 33x for the bare slab using a task-based algorithm for transport \cite{tianyu}.  Further results were presented at the SNA+MC 2013 conference, where Archer, a general purpose Monte Carlo radiation transport code developed at Rensselaer Polytechnic Institute, was modified to run an event-based criticality simulation in a 1D slab.  They conclude that control flow efficiency is increased, but global memory transactions are increased dramatically, the GPU is not kept busy when the number of particles is small, and performance is about 10x less than a task-based implementation \cite{tianyu_snamc}.  No details about the implementation are given other than that data references are remapped similarly to the method I presented at the ANS Winter Meeting in November of 2012 \cite{bergmann_ans_winter}.  These issues will also be addressed by WARP and shown in the preliminary studies.

Henderson has also implemented a GPU-accelerated algorithm for photon transport in Geant4.   He uses interpolated cross sections, which are very smooth and small compared to those used in neutron transport.  The geometry treatment is also a voxelized approach, well-suited for reading in CT scan data, which his project focused on.  He also only needed to consider water as a material since the human body is basically water.  Criticality was also not considered, for obvious reasons. Secondary electrons were transported however, but does not change the transport from being fixed source. He uses a task-based approach, using the shared memory as a stack of to-be-transported photons and electrons, basically transforming the SM into its own small independent processor ( like a CPU).  He also emphasizes the positive effect of using a SOA access pattern, reporting 3-4x speedup over AOS.  Henderson was able to achieve a total speedup of around 50-60x over the standard Geant4 routines \cite{henderson}.

A team from Tsinghua University has also been adding GPU acceleration to their in-house Monte Carlo code RMC. RMC is being developed as a drop-in replacement for MCNP since MCNP is export-controlled and very difficult for Chinese universities to obtain (source code might be impossible to obtain).  They've published some of their results, including an eigenvalue test and geometry routine test.  The eigenvalue test uses algorithms similar to those used by Liu and Henderson.  They use static geometry, single group cross sections, and a pop-stack task-based transport algorithm.  They were able to obtain a 113x speedup over a CPU version without doing a flux tally and 36x speedup doing a flux tally \cite{qixu_ans_winter}   The geometry routine test offloads the geometry processing routines of RMC to a GPU, where they were able to get 5-50x speedup over all-CPU RMC as the number of fuel rods in their geometry went from 25 to 625.  Again, one group cross sections were used \cite{qixu}.

WARP aims to be much more general and accurate than all previous codes as well as employing efficient parallel algorithms in order to have a completely GPU-accelerated code.  It will read ACE-formatted data, perform all reaction types as prescribed by them, use a Serpent-like unionized energy grid to regularize data access, use an event-based transport algorithm with parallelized operations for sorts and sums, use OptiX for general 3D geometry representation (without explicit nesting), use a SOA for neutron history data, and perform \emph{all} operations on the GPU unless strictly forbidden for some reason (the benefit of which will be discussed later).
	
OpenMC is also under development to run on GPUs by NVIDIA themselves.  Progress was presented at the 2014 GPU Technology Conference (GTC) \cite{openmc_gtc}.
